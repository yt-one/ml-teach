{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tensorflow介绍",
   "id": "2a4ed1147e0848c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "当需要额外的控制来编写自定义损失函数、自定义指标、层、模型、初始化程序、正则化函数、权重约束等时，研究Tensorflow的其他底层API很有用。有时候可能需要完全控制训练循环，例如对梯度使用特殊的变换或约束（不仅仅对它们进行裁剪），或者对网络的不同部分使用多个优化器。还将探讨如何使用TensorFlow的自动图形生成功能来增强自定义模型和训练算法\n",
    "\n",
    "Tensorflow：\n",
    "- 它的核心与NumPy非常相似，但支持GPU。\n",
    "- 它支持分布式计算（跨多个设备和服务器）。·\n",
    "- 它包含一种即时(JIT)编译器，可使其针对速度和内存使用情况来优化计算。它的工作方式是从Python函数中提取计算图，然后进行优化（通过修剪未使用的节点），最后有效地运行它（通过自动并行运行相互独立的操作）。\n",
    "- 计算图可以导出为可移植格式，因此可以在一个环境中（例如在Linux上使用Python）训练TensorFlow模型，然后在另一个环境中（例如在Android设备上使用Java）运行TensorFlow模型。\n",
    "- 它实现了反向模式的自动微分(autodiff) 并提供了一些优秀的优化器，例如RMSProp和Nadam，因此可以轻松地最小化各种损失函数。\n",
    "\n",
    "TensorFlow在这些核心功能的基础上提供了更多功能：最重要的当然是Keras， 但它还具有数据加载和预处理操作（tf.data、tf.io等）​，以及图像处理操作(tf.image)和信号处理操作(tf.signal)\n",
    "\n",
    "<img alt=\"Tensorflow的Python API\" height=\"500\" src=\"./images/tensorflow/p1.jpg\" width=\"500\"/>"
   ],
   "id": "cbc25489c63dece1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在底层，每个TensorFlow操作（以下简称op）都是使用高效的C++代码实现的。许多操作都有称为内核的多种实现：每个内核专用于特定的设备类型，例如CPU、GPU甚至TPU（张量处理单元）。GPU可以通过将整个计算分成许多较小的块并在多个GPU线程中并行运行它们来极大地加快计算速度。TPU甚至更快：它们是专门为深度学习操作而构建的定制ASIC芯片\n",
    "\n",
    "Tensforflow大多数时候，使用高层API（尤其是Keras和tf.data), 当需要更大的灵活性时，则可以使用较低层的PythonAPI，直接处理张量"
   ],
   "id": "f8def0939f1cabe7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TensorFlow不仅可以在Windows、Linux和macOS上运行，还可以在移动设备上运行（使用TensorFlow Lite），包括在iOS和Android设备上。\n",
    "\n",
    "如果不想使用Python API，也可以使用其他语言的API，例如C++、Java和Swift API。甚至还有一个名为TensorFlow.js的JavaScript实现，可以直接在浏览器中运行模型。\n",
    "\n",
    "<img alt=\"Tensorflow的架构\" height=\"500\" src=\"./images/tensorflow/p2.jpg\" width=\"500\"/>"
   ],
   "id": "f6ceeced4485a365"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 像NumPy一样使用Tensorflow",
   "id": "32e5d56f88260d5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TensorFlow的API围绕张量展开，张量从一个操作流向另一个操作，因此得名TensorFlow。张量与NumPy ndarray非常相似：\n",
    "\n",
    "它通常是一个多维数组，但也可保存标量（数值）。当创建自定义代价函数、自定义指标、自定义层等时，这些张量将非常重要。"
   ],
   "id": "ad8e87dd2ef5a66c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量和操作",
   "id": "6815f58807848f3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:24.190195Z",
     "start_time": "2025-09-11T05:51:19.912342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t = tf.constant([[1.,2.,3.],\n",
    "                 [4.,5.,6.]])  # tf.constant创建张量\n",
    "t\n",
    "t.shape\n",
    "t.dtype\n",
    "type(t)\n",
    "#\n",
    "tf.constant(100)"
   ],
   "id": "36c3814ca83196f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=100>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:24.334493Z",
     "start_time": "2025-09-11T05:51:24.232386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t[:, 1:]\n",
    "t[..., 1, tf.newaxis]\n",
    "t[0,1]"
   ],
   "id": "44d58fa27dfcf201",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:24.665466Z",
     "start_time": "2025-09-11T05:51:24.365230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 各种张量操作\n",
    "t + 10\n",
    "tf.add(t, 10)\n",
    "tf.square(t)\n",
    "t @ tf.transpose(t)\n",
    "tf.matmul(t, tf.transpose(t))"
   ],
   "id": "4ceb174ebb1c7d0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:24.776880Z",
     "start_time": "2025-09-11T05:51:24.672474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tf.squeeze, tf.reduce_mean, tf.reduce_sum, tf.reduce_max, tf.math.log\n",
    "# 构造一个简单的张量\n",
    "x = tf.constant([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0]])\n",
    "\n",
    "print(\"原始张量 x:\")\n",
    "print(x.numpy())\n",
    "\n",
    "# 1. tf.squeeze：去掉维度为1的维度\n",
    "x_expand = tf.expand_dims(x, axis=0)   # shape (1, 2, 3)\n",
    "print(\"\\n扩展后张量形状:\", x_expand.shape)\n",
    "print(\"squeeze之后:\", tf.squeeze(x_expand).shape)\n",
    "\n",
    "# 2. tf.reduce_mean：求均值\n",
    "mean_all = tf.reduce_mean(x)                 # 所有元素的平均值\n",
    "mean_axis0 = tf.reduce_mean(x, axis=0)       # 按列求均值\n",
    "print(\"\\nreduce_mean 所有元素:\", mean_all.numpy())\n",
    "print(\"reduce_mean 按列:\", mean_axis0.numpy())\n",
    "#\n",
    "# 3. tf.reduce_sum：求和\n",
    "sum_all = tf.reduce_sum(x)\n",
    "sum_axis1 = tf.reduce_sum(x, axis=1)         # 按行求和\n",
    "print(\"\\nreduce_sum 所有元素:\", sum_all.numpy())\n",
    "print(\"reduce_sum 按行:\", sum_axis1.numpy())\n",
    "#\n",
    "# 4. tf.reduce_max：最大值\n",
    "max_all = tf.reduce_max(x)\n",
    "max_axis0 = tf.reduce_max(x, axis=0)\n",
    "print(\"\\nreduce_max 所有元素:\", max_all.numpy())\n",
    "print(\"reduce_max 按列:\", max_axis0.numpy())\n",
    "#\n",
    "# 5. tf.math.log：自然对数\n",
    "log_x = tf.math.log(x)\n",
    "print(\"\\nlog(x):\")\n",
    "print(log_x.numpy())"
   ],
   "id": "7125234a13de621",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量 x:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "扩展后张量形状: (1, 2, 3)\n",
      "squeeze之后: (2, 3)\n",
      "\n",
      "reduce_mean 所有元素: 3.5\n",
      "reduce_mean 按列: [2.5 3.5 4.5]\n",
      "\n",
      "reduce_sum 所有元素: 21.0\n",
      "reduce_sum 按行: [ 6. 15.]\n",
      "\n",
      "reduce_max 所有元素: 6.0\n",
      "reduce_max 按列: [4. 5. 6.]\n",
      "\n",
      "log(x):\n",
      "[[0.        0.6931472 1.0986123]\n",
      " [1.3862944 1.609438  1.7917595]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "许多函数和类都有别名。例如，tf.add()和tf.math.add()是相同的函数。这允许TensorFlow为常见的操作提供简洁的名称\n",
    "\n",
    "tf.transpose()函数的功能与NumPy的T属性完全不相同：在TensorFlow中，使用自己的转置数据副本创建一个新的张量，而在NumPy中，t.T只是相同数据的转置视图。类似地，tf.reduce_sum()之所以这样命名，是因为其GPU内核（即GPU实现）使用的reduce算法不能保证元素添加的顺序：32位浮点数的精度有限，因此每次调用此操作时，结果可能会稍有不同。tf.reduce_mean()也是如此（当然，tf.reduce_max()是确定性的"
   ],
   "id": "9a4a7765d9542c80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量和NumPy",
   "id": "4f980e5e9f8eec55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "张量可以与NumPy配合使用：可以用NumPy数组创建张量，反之亦然。也可以将TensorFlow操作应用于NumPy数组，将NumPy操作应用于张量",
   "id": "3a9115a5c4221f43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:24.835318Z",
     "start_time": "2025-09-11T05:51:24.819333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([2.,4.,5.])\n",
    "tf.constant(a)\n",
    "t.numpy()  # np.array(t)\n",
    "tf.square(a)\n",
    "np.square(t)"
   ],
   "id": "8ca9a6809867507",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 类型转换",
   "id": "c69fac497bccdc96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "类型转换会严重影响性能，并且自动完成的类型转换很容易被忽视。为了避免这种情况，TensorFlow不会自动执行任何类型转换：如果对不兼容类型的张量执行此操作，会引发异常。例如，不能把浮点数张量和整数张量相加，甚至不能将32位浮点数和64位浮点数相加：",
   "id": "d8f30f51743d5806"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:24.903278Z",
     "start_time": "2025-09-11T05:51:24.872323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tf.constant(2.) + tf.constant(40)\n",
    "# tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n",
    "\n",
    "t2 = tf.constant(40, dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32) # 确实需要转换类型时，使用tf.cast()"
   ],
   "id": "da90ef019df05ffe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 变量",
   "id": "5908660fd946a7d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "tf.constant创建的Tensor值是不变的：无法修改它们。这意味着不能使用常规张量在神经网络中表示权重，因为它们需要通过反向传播进行调整。\n",
    "\n",
    "另外，还可能需要随时间改变其他参数（例如动量优化器跟踪过去的梯度）。需要tf.Variable："
   ],
   "id": "480e256875a16e6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:24.990793Z",
     "start_time": "2025-09-11T05:51:24.930729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "v = tf.Variable([[1.,2.,3.], [4.,5.,6.]])\n",
    "v"
   ],
   "id": "2ca12f28eb63922f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "tf.Variable的行为与tf.Tensor的行为非常相似：可以使用它执行相同的操作，它在NumPy中也可以很好地发挥作用，并且对类型也很挑剔。但是，它也可以使用assign()方法（或assign_add()与assign_sub()，给变量增加或减少给定值）进行修改。\n",
    "\n",
    "还可以通过单元（或切片）的assign()方法（直接指定将不起作用）或者使用scatter_update()或scatter_nd_update()方法来修改单个单元（或切片），但直接赋值将不起作用"
   ],
   "id": "fab8efc4aa353c34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:25.125538Z",
     "start_time": "2025-09-11T05:51:25.020540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 变量赋值\n",
    "v.assign(2*v)\n",
    "v\n",
    "v[0,1].assign(42)\n",
    "v[:,2].assign([0., 1.])\n",
    "v.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100., 200.])\n",
    "\n",
    "\n",
    "# v[1] = [7.,8.,9.] # 直接赋值不起作用\n",
    "v[1].assign([7,8,9])\n",
    "\n"
   ],
   "id": "a0c7afbf16d6357b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  7.,   8.,   9.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:25.292466Z",
     "start_time": "2025-09-11T05:51:25.169257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "observation = np.array([111.0, 188.0])\n",
    "codes = np.array([[102.0, 203.0],\n",
    "               [132.0, 193.0],\n",
    "               [45.0, 155.0],\n",
    "               [57.0, 173.0]])\n",
    "\n",
    "np.argmin(np.sqrt(np.sum((observation - codes)**2, axis=1)))\n",
    "# 找到codes里 距离observation最近的codes索引（位置），使用Tensor实现\n",
    "\n",
    "observation = tf.constant(observation)\n",
    "codes = tf.constant(codes)\n",
    "\n",
    "tf.argmin(tf.reduce_sum(tf.square(tf.subtract(observation, codes)), axis=1))"
   ],
   "id": "c1fc7d6b9c58cf86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 自定义模型和训练算法",
   "id": "7134c89b394b8f0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义损失函数",
   "id": "b6b9f98c863d82c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "场景：数据集在删除/修复异常值后仍然有噪声。均方误差可能会对大误差惩罚太多，而导致模型不精确。平均绝对误差不会对异常值惩罚太多，但是训练可能需要一段时间才能收敛，并且训练后的模型可能不太精确。这可能是使用Huber损失而不是MSE的好时机。Huber损失已在Keras中实现（只需使用tf.keras.losses.Huber类的一个实例），但假设它不存在。要实现它，只需创建一个函数，将标签和模型的预测结果作为参数，并使用TensorFlow操作来计算包含所有损失的张量（每个样本一个）",
   "id": "68d69b5a759cf9c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:25.338410Z",
     "start_time": "2025-09-11T05:51:25.327554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    # y_true: (m,), Tensor\n",
    "    # y_pred: (m,), Tensor\n",
    "\n",
    "    # 返回 (m,) 损失值 ，Tensor\n",
    "    error = y_true - y_pred\n",
    "\n",
    "    # 想从Tensorflow框架的优化功能中受益，应仅使用Tensorflow操作\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ],
   "id": "1511632a5f43dfb2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:28.110526Z",
     "start_time": "2025-09-11T05:51:25.342307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "z_center = np.linspace(-1, 1, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z ** 2 / 2, \"r:\", linewidth=1)\n",
    "plt.plot(z_center, z_center ** 2 / 2, \"r\", linewidth=2)\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"k--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"k--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.text(2.1, 3.5, r\"$\\frac{1}{2}z^2$\", color=\"r\", fontsize=15)\n",
    "plt.text(3.0, 2.2, r\"$|z| - \\frac{1}{2}$\", color=\"b\", fontsize=15)\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ],
   "id": "a401afe5341facd0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x350 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqQAAAFkCAYAAAD2RimAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB5/UlEQVR4nO3dd1xV5R/A8c9lg4IbRQVn7hmWYu49Mi3LmaOyX+YoMzNRK83VMHPkzJWaq9w5McW9cKSZmeZWECcoMi5wfn88XRAB5V7GuRe+79eLl5zDPfd+eTzc+z3PeZ7vY9A0TUMIIYQQQgid2OkdgBBCCCGEyNkkIRVCCCGEELqShFQIIYQQQuhKElIhhBBCCKErSUiFEEIIIYSuJCEVQgghhBC6koRUCCGEEELoShJSIYQQQgihK0lIhRBCCCGEriQhFUKIVIwaNQqDwUBgYKDeoSTTqFEjDAaD3mEIIUSGkIRUCGFTLl26hMFgoFWrVqk+5uDBgxgMBnr37p11gQkhhLCYJKRCCCGEEEJXkpAKIYQQQghdSUIqhMgxSpYsScmSJVP82bPGZP74449UrlwZFxcXfHx88Pf3JyoqKsXHnjx5ki5duuDl5YWTkxMlSpRg4MCB3LlzJ8njTMMPevfuzd9//81rr71GwYIFMRgMXLp0yaLfMTY2lu+//57q1avj6upKnjx5aNy4MRs3bkz22Pj4eObOncuLL75I/vz5cXNzo2TJknTo0IHdu3cneeyqVato2LAhnp6euLi44O3tTatWrVi7dq1FcQohxOMc9A5ACCGs3XfffUdgYCCdO3fm5ZdfZtOmTXz11VccP36czZs3J0lk169fT6dOnbC3t+eVV17B29ubv/76ix9++IGtW7dy6NAh8uXLl+T5z58/T506dahcuTK9evXi7t27ODk5mR2npml07tyZ1atXU65cOfr3709ERAQrV67k5ZdfZsqUKXzwwQcJj/f39+ebb76hTJkydOvWDXd3d65fv86ePXvYsWMHDRo0AGDmzJn069cPLy8vXn31VQoUKEBwcDCHDx9m7dq1dOjQwbKGFUKI/0hCKoSwSefPn2fUqFEp/uzatWsZ+lrbt28nKCiIypUrAzBu3DjatGnD1q1bWbJkCT169ADgzp079OjRg0KFCrFv3z58fHwSnmPZsmV069aNzz//nGnTpiV5/n379vHZZ5/x5ZdfpivOJUuWsHr1aho2bMi2bdsSktoRI0bg6+vLkCFDaNeuHaVKlQJg7ty5FCtWjJMnT+Lm5pbwPJqmce/evYTtuXPn4uTkxB9//EGhQoWSvOaTvb5CCGEJSUiFEDbp33//ZfTo0VnyWj169EhIRgEcHBwYP348AQEB/PTTTwkJ6aJFiwgPD2f69OlJklGArl27MnHiRJYvX54sIS1SpAgjR45Md5wLFy4E4JtvvknSw1q8eHE++ugj/P39+fnnn5O8lpOTEw4OST8KDAYD+fPnT7LP0dERR0fHZK9ZoECBdMcthBCSkAohbFLLli3ZsmVLij87ePAgfn5+GfZa9evXT7avVq1auLq6cuLEiSSva/r3/PnzyY6Jiori9u3b3L59m4IFCybsr169ukW36J90/PhxXF1defHFF5P9rFGjRgBJ4u3UqROzZs2iSpUqdO7cmYYNG+Ln50euXLmSHNupUyeGDRtGlSpV6NKlC40aNaJevXrkzZs33TELIQRIQiqEEM/k6emZ6v7r168nbN+9exeA6dOnP/X5IiIikiSkhQsXzoAoITw8HG9v7xR/VqRIEQDCwsIS9k2dOpXSpUuzcOFCxo4dy9ixY3FxcaFTp0589913CTEOHTqUAgUKMGvWLCZNmsR3332Hg4MDbdq0YfLkyQlDAIQQwlIyy14IkWPY2dkRGxub4s8eT9SeFBoamur+PHnyJGx7eHgAcOrUKTRNS/WrRIkSSZ4no1Zc8vDw4ObNmyn+zLTfFCOo2/CffPIJp0+f5vr16yxdupT69euzaNEiunfvniS+Pn36EBQUxK1bt1izZg2vvfYa69evp23btsTFxWVI/EKInEsSUiFEjpEvXz5CQ0OTJaURERGcO3cu1eP27NmTbF9QUBCRkZHUqFEjYV/t2rUBOHDgQMYEbKaaNWsSGRnJ4cOHk/1s165dAEnifVzRokXp2rUrW7Zs4bnnnmP79u1ERkYme1yBAgXo0KEDK1asoEmTJpw5cybF4QlCCGEOSUiFEDlGrVq1MBqN/Pzzzwn7NE3D39+fiIiIVI9bvHgxp0+fTtiOjY1l+PDhAPTq1Sth/1tvvYW7uzsjRoxI8niTR48eJYwzzQymWPz9/TEajQn7r1+/zqRJk3BwcEjo+YyOjmbHjh1ompbkOSIiInjw4AGOjo7Y29sDsHXr1mRJvNFoTBii4Orqmmm/kxAiZ5AxpEKIHGPAgAEsWLCAPn36EBAQQKFChdizZw/379+nevXq/PHHHyke16xZM+rUqUOXLl3Inz8/mzZt4s8//6Rly5a8+eabCY8rVKgQy5Yt44033qB69eq0atWKChUqEBUVxeXLl9m1axd169ZNdTJWevXo0YPVq1ezbt06qlWrxssvv5xQh/TOnTt89913lC5dGoDIyEiaNm1K6dKlqV27Nj4+Pjx8+JDffvuNkJAQPv3004SJVp07d8bNzY169epRokQJjEYjAQEB/PXXX3Tu3DlZRQEhhDCXJKRCiByjatWqbNmyheHDh/Prr7+SO3du2rRpw7fffkvnzp1TPe7jjz+mXbt2TJkyhX///ZdChQoxbNgwPv/882TjP9u2bcvx48f59ttv2b59OwEBAeTKlYvixYvz1ltvJUlgM5rBYODXX39lypQp/PTTT0ybNg0nJyeef/55Bg8ezCuvvJLw2Fy5cvH111/z+++/s2fPHkJDQ8mXLx8VKlTg66+/TtIeEyZMYMuWLRw+fJgNGzaQK1cuypYty+zZs3n77bcz7fcRQuQcBu3J+zVCCCGEEEJkIRlDKoQQQgghdCUJqRBCCCGE0JUkpEIIIYQQQlfpSkgnTJiAwWBg0KBBT33crl278PX1xcXFhdKlSzNr1qz0vKwQQgghhMhGLE5Ijxw5wpw5c6hWrdpTH3fx4kXatGlD/fr1OX78OMOHD+eDDz5g1apVlr60EEIIIYTIRixKSB8+fEj37t358ccfyZcv31MfO2vWLHx8fJg8eTIVK1akT58+vP3220ycONGigIUQQgghRPZiUR3S/v3707ZtW5o1a8bYsWOf+tgDBw7QokWLJPtatmzJvHnzMBqNODo6JjsmOjqa6OjohO34+Hju3r1LgQIFMmzNZyGEEEIIkXE0TePBgwcULVoUOzvz+jzNTkiXL1/OsWPHOHLkSJoeHxISQuHChZPsK1y4MLGxsdy+fRsvL69kx0yYMIHRo0ebG5oQQgghhNDZ1atXKV68uFnHmJWQXr16lQ8//JBt27bh4uKS5uOe7NU01eJPrbfT39+fwYMHJ2yHhYXh4+PDP//8Q/78+c0JOccyGo3s3LmTxo0bp9gLnRq7WbPQPD3RXnstE6OzXpa2W04mbWa+iIgISpQoAcC///5Lnjx5dI7INsi5Zpmc3G6Ggwexb9eO2O3boXr1NB+Xk9ssPe7evUu5cuVwd3c3+1izEtKjR48SGhqKr69vwr64uDh2797NDz/8QHR0NPb29kmOKVKkCCEhIUn2hYaG4uDgQIECBVJ8HWdnZ5ydnZPtz58/f6rHiKSMRiNubm4UKFDAvD+mESMyLygbYHG75WDSZuZ7/II+f/785M2bV79gbIica5bJ0e3WsCH89BM0agRm3ELO0W2WASwZXmlWQtq0aVNOnTqVZN9bb71FhQoV+PTTT5MlowB+fn5s2LAhyb5t27ZRq1Yt+U+2VhcvwubN0K+f3pEIIYQQlsudG3r00DsKkQZmjTh1d3enSpUqSb5y5cpFgQIFqFKlCqBut/fs2TPhmL59+3L58mUGDx7MmTNnmD9/PvPmzWPIkCEZ+5uIjHPwIHz+Ody9q3ckQgghhGVmzoRRo/SOQqRRhq/UFBwczJUrVxK2S5UqxaZNmwgMDKRGjRqMGTOGqVOn0rFjx4x+aZFRXn8drl4FGa8rhBDCVoWHw/37ekch0siisk+PCwwMTLK9cOHCZI9p2LAhx44dS+9Liazi6Ki+IiLAwQFSGM8rhBBCWLVPP9U7AmEGWctepCwsDLy94eef9Y5ECCGEMM+2bfDokd5RCDNIQipSlicPfPcdNGumdyRCCCFE2t28CW3awPLlekcizJDuW/YiG3vrLb0jEEIIIcxTuDD88w8UKaJ3JMIM0kMqnm7lSpmlKIQQwjbEx4OmQenS4OamdzTCDJKQiqe7fh3OnlV/4EIIIYQ1W74cnn9eTcoVNkVu2Yun++gjvSMQQggh0qZUKXj5ZciVS+9IhJkkIRXPpmmwZw+89BKksBqXEEIIYRX8/NSXsDlyy1482x9/qPWAt2/XOxIhhBCZafduaN8eSpQAg8G25hDMmweHDukdhbCQJKTi2WrUUMuJtmihdyRCCCEy08OHUKkSfPNNxs9SX7AAGjeGQoXA3R18fTOu1nV8PMyYAVu3ZszziSwnt+xF2tSurf7VNHXVLIQQIvtp00Z9QcavdPT77/DKKyrZzZcP1qyBHj3UioCdO6fvue3s4MgRiInJmFhFlpOEVKTdu+9C3rzw7bd6RyKEEMLWLFmSdPuTT2DnTlVeMD0JaXw8hIRA0aLg4pK+GIVu5Ja9SLvq1aFqVb2jEEIIkV3cvw8FC6bvObZsUWNez53LkJCEPqSHVKTdgAF6RyCEECK7+OknCAqCH35I3/PUq6fGp5YtmzFxCV1ID6kwz7VrMGmSFMoXQghhuXXr4L33YPZsVcg+PTw84M03ZX6DjZOEVJjnzz/hyy/h6lW9IxFCCKG3119XieDTvp4sxbR8uRozOmsWvPVW+l5/zBiYODF9zyGsgtyyF+Zp0UL1kubOrXckQgghMtrDh3D+vPo+JkZNFjpxApycVDmoJ1WqlPLnwcWLqqapszNUqZK4/8cfYeBAWLgQunRJf7zR0WqGvbB5kpAK89jZqTefyEj1ZpUnj94RCSGEyChBQapWqMns2eqrRAm4dCn547/8Mvm+f/+FRo1UErtqVeIynpMmwdChMH26+nlIiNpvb69qk1pi7FjLjhNWRy4rhPni4qByZVVLTgghRPbRqJGaI/DkV0rJaEpMyWhoKKxeDW3bJv5s6lT1+dG3L3h5JX698IL5cWqaev7oaPOPFVZJekiF+ezt1ZWulIASQghhcuGC6l0NDVU9o48no5D2pDYtTpyAjh1hx46kPbrCZklCKizToYPeEQghhLAWFy6ontGbN1Uy+vLLmft6NWuquqNlymTu64gsI7fsheW2b4fu3aUElBBC5GQXL6peyqxKRo1G9W/ZslLqKRuRhFRYTtMgPBwePNA7EiGEEOn1rPJNj3+ZXLyYOEEpK5JRgM8+UxVfpDMkW5Fb9sJyzZurLyGEELbP3ATv0iXVMxoSAr/+mjXJKECzZupWvfSOZitm9ZDOnDmTatWq4eHhgYeHB35+fmzevDnVxwcGBmIwGJJ9/f333+kOXFgJTYM9e+DGDb0jEUIIkVUuXVI9o8HBKhlt1y7rXrtZM3j33ax7PZElzOohLV68OF999RVl/1sv9qeffqJ9+/YcP36cypUrp3rc2bNn8fDwSNguZGm9MWF9Hj1Sb0TDhqkvIYQQtmvBAli0SK3KFxUF5crB4MFqvsDjevWCy5ehfHl1q37VquTP9eabKnnMKJqm6pi+/TZUrJhxzysyTHpGUZiVkLZ74gpo3LhxzJw5k4MHDz41IfX09CRv3rwWBSisXK5ccPiwGlwuhBDCtv3+O7zyiqoznS8frFkDPXqAg4Na7hMgPh6OHlXfnz2rvlLStWvGxnb9uqo92q6dJKRWKCYG+ve3t/h4i8eQxsXF8csvvxAREYGfn99TH1uzZk2ioqKoVKkSI0eOpPEzaoZFR0cT/Vix2/DwcAC+/lpj7FijDBtJA+N/sxBN/2aqUqVUseOoKHB0zPzXy0RZ2m7ZhLSZ+R5vK6PRKG2XRnKuWcasdluwIOn2oEHY79gBy5cT99prifvv3Uvri6cxyjQoXBhOn1YrBmbyOSDnmnnu34dOnewJDLR8rrxB08zrYD116hR+fn5ERUWRO3duli5dSps2bVJ87NmzZ9m9eze+vr5ER0ezePFiZs2aRWBgIA0aNEj1NUaNGsXo0aNT+EkYjRvfp1+/Ezg6yuw6a1Jm7VqK797Nru++k4HmQjxDVFQUXf5bx3v58uW4uLjoHJEQqav/6aeElyjBH/366RaD8927YDAQnS+fbjGIlN286crYsXW4etUDCAfyEBYWlmSoZlqYnZDGxMRw5coV7t+/z6pVq5g7dy67du2iUqVKaTq+Xbt2GAwG1q9fn+pjUuoh9fb2BsIADxo3jmfFijhkFEDqjEYjAQEBNG/eHMcs6LU0HDiA4fhx4v/3P3Vrx0ZldbtlB9Jm5ouIiCDffx+soaGhMqQpjeRcs0x62s2waBH2779P7N69qhi9Tuw++gi79euJPXdO9ZBmMjnX0uboUQMdOthz86bqiMqfP4y7d/NalJCanTk4OTklTGqqVasWR44cYcqUKcyePTtNx9epU4clS5Y89THOzs44OzunsF8jOhp27rSjcWM7Nm6EEiXM/Q1yFkdHx6z5Y2rQABo0wPLRI9Yly9otG5E2S7vH20nazXzSZpYxu93WrYP+/WH2bBxffDHzAkuLsWOhWzccU8gNMpOca6nbsAG6dFFzm0HNf/v551heeMGy50v3ZYamaUl6M5/l+PHjeHl5WfRaa9fGUbCg+v70aahTB44ds+ipRGZ48ABGjlTLuQkhhLBdy5erSUyzZsFbb+kdDeTNC3Xr6h2F+M/06WoFcVMyWq8e7N+vppRYyqyEdPjw4ezZs4dLly5x6tQpRowYQWBgIN3/Kwfh7+9Pz549Ex4/efJk1q5dy7lz5zh9+jT+/v6sWrWKAQMGWBTsCy9oHDgAzz2ntkNCVMfcb79Z9HQiozk6qjexkyf1jkQIIYSlfvwReveGhQvVv3qKjVW9Txs26BuHAFSBhSFDYMAA9T2oXtKAAChQIH3PbdYt+5s3b9KjRw+Cg4PJkycP1apVY8uWLTT/b7We4OBgrly5kvD4mJgYhgwZwvXr13F1daVy5cps3Lgx1UlQaVG2LBw4AO3bw759EBGhvp82DXQcby0AXFxU+Q/77HLjXgghcphJk1Stz+nTE5cEBfW+rkcN8YcPoUYN8PbO+tcWSURGqgpgj5ecHTYMxo3LmGG9ZiWk8+bNe+rPFy5cmGR76NChDB061OygnqVAAdi+XdXlXblSZen9+6sldb/+OkvGO4vU2NtDdLS6bV+lit7RCCGEMMfUqaqMX9++6sukRAm1OlNWy5tXDRsQurp1S5WnPXhQbdvbw4wZ8L//Zdxr2Gzq5uICy5bBp58m7ps4UQ15iYzULy6Burpu00a9qQkhhLAdly6p5Xae/NIjGT11Sg0Dk88SXf3zD/j5JSajuXOrERQZmYyCDSekoHpCv/pKXTyZekV//RWaNlXZvNDJRx/Btm1y614IIYTlfvsNRozQO4ocbe9elYz++6/aLloU9uyB1q0z/rVsOiE1ee89la3nyqW2DxxQk/FksrdOSpaEChUSr6yFEEIIc/n7q1I60rmhixUroFkzuHtXbVetqnpJa9TInNfLFgkpqDvEe/ao7B3g/HmV1e/bp29cOdbNm/Dii+rySgghhDBHcLD6N08efePIgTRNzcfp0kVNCQFo3lzlWJk5tyzbJKSgFpE4eDBxLs2dO+r2/cqV+saVI3l6qssoWRJRCCGEOR48gIoV4Ycf9I4kx4mNhfffV7PnTd5+GzZuzPxrg2yVkILK3vfuVdk8qOy+c2f45hu5e5ylDAZVy87SJRuEEELkTG5uMH8+vPqq3pHkKA8eqJn0jy+8OWYMzJ2ryoxntmyXkILK4jduTLq4xKefqjqlsbH6xZUjHT0KixbpHYUQQghbYW8Pr70GxYrpHUmOceOGWmho82a17egIixerxRcNhqyJIVsmpKAac948ld2bzJqlsv8HD/SLK8dZv14VWjYt6SCEEEKkZtUqtTqU0ah3JDnGqVNQuzacOKG28+ZVhXLefDNr48i2CSmorH7kSJXlm7qbN2+Ghg3V1YDIAsOGqV5SWa1ACCHEs8TEqA6MrLhHLNi+Xa1Df+2a2i5ZUq1J36hR1seSI7KEN99U2X7evGr7+HF1NXDqlK5h5Qyurur2y40b6o1GCCGESE3XrjLMK4ssWKDqiYaHq+1atdTE8IoV9YknRySkoLL9/ftV9g/qaqBePXV1IDLZzZtQurRaWksIIYRIyfLlcPu23lFke5oGn3+uZs+b5tW0bw+BgVC4sH5x5ZiEFFTWf/Bg4sTv8HB1dTB/vr5xZXuFC6txEx066B2JEEIIa3TnDrzzDqxbp3ck2VpMDPTsmXR+zQcfqKG7psWF9JKjElJQudHOnepqANTVwTvvwGefSVmoTPXGG1LgWAghRMoKFIBLl7J+Jk0Ocu8etGwJS5aobYMBvv8epkyxjsWwclxCCuoqYNUqdVVgMnasumowrUogMsHixWp8kBBCCGHy6JGaVV+oEDg76x1NtnTpErz0krotD2rNmlWrYNAgHYN6Qo5MSEFdDUyZApMnJ9bYWrIEWrVSVxEiE3h4qJllUs5DCCGEybffQrVqEBendyTZUlAQ1KkDZ86o7UKFVGJqbesO5NiE1OTDD9VVgqur2g4MVFcRly7pGVU21b49zJwp5TyEEEIkeu01GD7cOu4bZzPr16tSlzdvqu3y5dVcmtq19Y0rJTk+IQV1lbBzp7pqAHUVUbs2HDmib1zZUmysmm0vGb8QQgiAqlWhRw+9o8h2fvhB5TePHqnt+vVVtaHSpfWNKzWSkP6ndm111VC+vNoODVVXFTLhL4PFxKjBu7/9pnckQggh9BQXp+YVHD6sdyTZSnw8DB4MAwcmLpLYpYuqx54/v76xPY0kpI8pXVpdPTRooLYjI9XVxbRp+saVrbi5wd9/w4ABekcihBBCT6GhcPWq3KrPQI8eqaI233+fuM/fH37+WU1ksmaSkD4hf351FdGtm9rWNNWh99FHMt46wxQooP41DWoRQgiR83h5wd694OurdyTZQmgoNGkCq1erbXt7mDMHxo+3jdW7bSDErOfsrGbcjxiRuG/yZHXVYRqLIdLpu+/UuKHISL0jEUIIkdX++ANOntQ7imzj7Fnw84NDh9R27txqZNy77+oblzkkIU2FwaBqk/74Y+LdhDVroHFjdRUi0um119Slm5OT3pEIIYTIat9+C2+9JSvSZIA9e6BuXbhwQW0XK6Y6nlu10jcuc5mVkM6cOZNq1arh4eGBh4cHfn5+bN68+anH7Nq1C19fX1xcXChdujSzZs1KV8BZrU8f2LQJ3N3V9uHDqp7X2bP6xmXzSpVSS4nK2CEhhMh5FixQvTymQuDCIitWQLNmcPeu2q5WTU3Qrl5d37gsYVZCWrx4cb766iuCgoIICgqiSZMmtG/fntOnT6f4+IsXL9KmTRvq16/P8ePHGT58OB988AGrVq3KkOCzSosW6gqkWDG1ffGi6hrfvVvfuGye0Qi9eqlCsEIIIXKGhw9VPWofH70jsVmaBl9/rWbPx8SofaZcpXhxfWOzlFkJabt27WjTpg3lypWjXLlyjBs3jty5c3Pw4MEUHz9r1ix8fHyYPHkyFStWpE+fPrz99ttMnDgxQ4LPStWrq7EZpquOe/egeXNYulTfuGyao6O6OpaVm4QQImc4fx6KFFGZk7BIbCz07QvDhiXue+cdNWbUw0O/uNLL4jGkcXFxLF++nIiICPz8/FJ8zIEDB2jRokWSfS1btiQoKAijDSYhxYqpvyHTuIyYGOjeXc1gk2EwFlq4UF3iCSGEyP4KFIDPPoNatfSOxCY9eADt2qkpGCbjxqn5Lra+CKKDuQecOnUKPz8/oqKiyJ07N2vWrKFSpUopPjYkJITChQsn2Ve4cGFiY2O5ffs2Xl5eKR4XHR1NdHR0wnZ4eDgARqNR90TWxUXdYf7gA3vmzVP5/IgR8O+/8UybFmc1J4SpnfRurzS5cwfD2rVo77yjdyS21W5WQtrMfI+3lTW8r9kKOdcsY1Xtlju3qtoOVn13zKra7D/Xr0OHDg788Ycad+vkpPHjj3F07aoRG6tzcP9JT3uZnZCWL1+eEydOcP/+fVatWkWvXr3YtWtXqkmp4YkBy9p/XYlP7n/chAkTGD16dLL9O3fuxM3NzdyQM8XLL0NMzHMsXqx+7/nz7Th27DZDhx7Bzc1KzgwgICBA7xCeqcjBg/hOmsROe3siPT31DgewjXazNtJmaRcVFZXw/Y4dO3Cx9orVVkbONcvo3W5l1q4lzsmJS23a6BqHOfRuM5NLlzwYM6YOd+6oXq/cuWMYNuwwefLcYdMmnYN7zKN01MY0aFr6bjY3a9aMMmXKMHv27GQ/a9CgATVr1mTKlCkJ+9asWUOnTp149OgRjql0J6bUQ+rt7U1wcDAFTEXVrcSKFQbeeceemBiVYFetqrFuXazug4qNRiMBAQE0b9481Xa2GvHxaopgwYJ6R2Jb7WYlpM3MFxERQb58+QAIDQ0lb968+gZkI+Rcs4y1tJvdkCHg4kL82LG6xZBW1tJmAAEBBrp0sefBA5VnlCypsX59LBUq6BpWiu7cuYOXlxdhYWF4mDmg1ewe0idpmpYkeXycn58fGzZsSLJv27Zt1KpV66n/wc7Ozjg7Oyfb7+joqPuJ8aQ334QSJVQFo7t34dQpA/XqObJxI9SooXd01tlmKfLyUoNyHzxIXMlJRzbTblZE2iztHm8naTfzSZtZRvd2+69zypaK/endZvPnw3vvkXBL/sUXYf16A4ULW+f5n562MmtS0/Dhw9mzZw+XLl3i1KlTjBgxgsDAQLp37w6Av78/PXv2THh83759uXz5MoMHD+bMmTPMnz+fefPmMWTIEIsDtkb168P+/VC6tNq+cUPt27pV37hsTrNmap1WIYQQ2UdkpFrPMj5e70hshqbByJFq9rwpGW3fHnbuhCem5mQbZiWkN2/epEePHpQvX56mTZty6NAhtmzZQvPmzQEIDg7mypUrCY8vVaoUmzZtIjAwkBo1ajBmzBimTp1Kx44dM/a3sALly8OBA1C7ttp++BDatlUz30Qaff45DB+udxRCCCEy0ubN0KkTXLqkdyQ2IToaevRQs+dNPvhATai2kmk0mcKsW/bz5s176s8XLlyYbF/Dhg05duyYWUHZKk9PdfXy5pvqYjAuDv73P1VIf+xYsJOFWp+uWTO9IxBCCJHRXntNLW9ouo0oUnXvHrz6KuzapbYNBvj+e/jwQ33jygqSImUwV1f45ZfEqhYAEyaoeqWpDLUVj/vnH2jUCIKD9Y5ECCFEeoWFqX/LlNE3Dhtw8aJak96UjLq6qs6tnJCMgiSkmcLODr77DqZNS+wVXb5crex0546+sVm9woXVX6FpYV4hhBC2KS4OXngBxozROxKrd+QI1KkDf/+ttgsVUndcO3TQNawsJQlpJhowANauTRzzsWePuvq5cEHXsKxbnjxqvFHlynpHIoQQIj0MBjVerX17vSOxauvWQcOGEBqqtsuXh4MHE+ek5BSSkGaydu1U97tpVtw//6iroIMH9Y3L6p08qUZwCyGEsE12dmoyU7VqekditaZOVWNGIyPVdoMGSav25CSSkGaBWrVUAlqxotq+dQsaN1ZjQ0Qq5s+H8eNV7QshhBC2ZeNGNas3JkbvSKxSXBx89JEaH2r6mOvWDbZtg/z59Y1NL5KQZpGSJdVVT+PGajsqCl5/Xc2ek5wrBWPGqCz+KUvMCiGEsFLh4XD/Pjg56R2J1Xn0CN54AyZPTtw3YgQsXgwprAmUY0hCmoXy5oUtW1R9MVCJ6ODB6gopLk7X0KyPuzs4OkJICERE6B2NEEIIc3TtCitX6h2F1QkNhSZNYM0atW1vr+qVS2lISUiznJMT/PQTfPFF4r5p09QYEsm7nvDggRrdPWuW3pEIIYRIq59+Ur2jIomzZ9UckkOH1La7uxrZ0KePvnFZC0lIdWAwwKhRsGABOPy3NMGGDar8ZkiInpFZGXd3WLIE3n5b70iEEEKkxaVLavH1nTv1jsSq7NkDfn6q1ihAsWJqX8uW+sZlTSQh1VHv3uoWvoeH2g4KUldPf/2la1jWpV07yJdPBtoKIYQtKFlSJaVS6inBsmVqIcJ799R2tWpqikT16vrGZW0kIdVZ06awbx94e6vty5dVrVK5uHzMli2quHJUlN6RCCGESE1oKMTGQpEiMiAS1Y8yYYKaPW8qNtCypeoZLV5c39iskZwxVqBKFXW19PzzajssTJ20ixfrG5fVKFVKXUrKIFshhLBevXurCRECo1GNXBg+PHFfnz5qeJ7prqhIykHvAIRStKgqoN+lixrkbDRCz55qvMlnn+Xw6kfly8O8eXpHIYQQ4mnGj1eTUXO4Bw9UWaetWxP3jR8Pw4bl8M/yZ5AeUiuSO7daavT99xP3ffGFmtMjtYVRKzetW6d3FEIIIVJSowbUr693FLq6dk01gSkZdXKCn38Gf39JRp9FElIr4+AA06fDt98m7lu4ENq2Vbfyc7Rly9T9DiGEENbjyBG1GPvNm3pHoquTJ9XE5D/+UNv58kFAgBpDKp5NElIrZDDAkCGqprBp1Ybt2+Gll+DKFX1j09WSJTB3rt5RCCGEeFxcHHh5QcGCekeim61boV49uH5dbZcqpVZnbNBA37hsiSSkVuyNN2DHDihQQG2fPq2uvo4d0zcu3bi4qH+DgtRMTiGEEPqrUweWL1fLDuVA8+apu5im4bMvvqgmKleooG9ctkYSUitXt646scuWVdvBweqKa+NGfePSzYUL6q991Sq9IxFCCDFpEvzzj95R6ELTYORINXvetPx3hw6qbKOnp66h2SRJSG1A2bJw4IBKTkFVP3rlFZg5U9+4dFG6tCpH8PrrekcihBA52/37MHmyGkOaw0RHw5tvwrhxifsGDYJffwU3N93CsmmSkNqIggXh99/VbXyA+Hjo1w+GDlXf5yj166tbQ1J6QAgh9JM3L5w7p+oV5iB370KLFrB0qdo2GGDKFPj++xw7aiFDSEJqQ1xc1DCdoUMT9337rXoviIzULy5dzJ8PlStLUiqEEHq4fBlu31Yzb3NQFnbhgrpbuXu32nZ1hTVr4IMP9I0rO5CE1MbY2cHXX6vb9aaV2X75Ra2Te/u2vrFlKT8/+N//cmD3sBBCWIGhQ1U3oabpHUmWOXxYffScPau2PT0hMBDat9c1rGxDVmqyUX37go8PdOqkxpTu36/+UDZtguee0zu6LFCxovoSQgiR9aZPV72kOaTa+9q1qp6o6W5khQrq87ZUKV3DylbM6iGdMGECL7zwAu7u7nh6etKhQwfOmi4VUhEYGIjBYEj29ffff6crcAFt2qjbBl5eavv8eZWU7t+vb1xZ6uuv1coBQgghsobRqCY2+PrqHUmWmDIFXnstMRlt2FB9zkoymrHMSkh37dpF//79OXjwIAEBAcTGxtKiRQsiIiKeeezZs2cJDg5O+HouR3TjZb7nn1dloapUUdt37kCTJvDrrznjqpXz5+HqVb2jEEKInGHPHlXt5NIlvSPJdHFx8PHHdgwalDgyoXt3VQQ/Xz5dQ8uWzLplv2XLliTbCxYswNPTk6NHj9LgGcsReHp6kjdvXrMDFM/m4wN796pKSNu3q3IU3bo50KtXWVq31ju6TDZnTo65ZSSEELorWlRlZT4+ekeSqR49gm++eZFDhxInbI0cCV9+KR85mSVdY0jD/ltcPX/+/M98bM2aNYmKiqJSpUqMHDmSxo0bp/rY6OhooqOjE7bDw8MBMBqNGI3G9IScbbm5wbp18P779ixapDq+f/qpMo6ORqZONeKQnUcLx8ZiWLkSrUOHdBeAM51fcp6lnbSZ+R5vK3lfSzs51yyToe3m4wNjxqjuQ1M1+Gzm5k149VU7goLUeDh7e40ZM+J46y1NFgl8hvScYwZNs2yKnKZptG/fnnv37rFnz55UH3f27Fl2796Nr68v0dHRLF68mFmzZhEYGJhqr+qoUaMYPXp0sv1Lly7FTSrOPpWmwcqV5Vi2LHHCj69vCEOGBOHqmj3fPNxu3qRp//4cGTKEkDp19A5HiGeKioqiy3+1G5cvX46LaVlcIayVplFj+nSuNGnC3UqV9I4m01y7lpsxY+pw82YuAFxdjQwdeoSaNW/pHJltePToEd26dSMsLAwPDw+zjrU4Ie3fvz8bN25k7969FC9e3Kxj27Vrh8FgYP369Sn+PKUeUm9vb4KDgylgWthdPNWiRfH07etIbKzqLa1RQ2Pt2liKFtU5sMxy7RqYeR6mxGg0EhAQQPPmzXF0dMyAwLI/aTPzRUREkO+/QWihoaEynCmN5FyzTIa027172HfsSPzw4WjNmmVsgFZizx4Dr79uz7176p58gQKRbNwIzz+fPW4x7tlj4Pvv7Th50sCVKwZGjozj888ztnTinTt38PLysightaiVBw4cyPr169m9e7fZyShAnTp1WLJkSao/d3Z2xtnZOdl+R0dHeRNKo549jQQH7+e7717i/n0DJ04YqF/fkU2bEidAZSum6Y5XrmTI2CY518wnbZZ2j7eTtJv5pM0sk6528/SE3bvVTOhsOIhy6VJ4663EtVaqVdP48MPdPP98k2xzrkVFqc//N99Uy5za29vj6Jixixqkp63MmmWvaRoDBgxg9erV7Nixg1IW1jw4fvw4XqZaRSLTVK16h8DAWEqUUNtXr8JLL6mJT9nS9OlQtapa100IIUTG2LsXTpxQiWg2S0Y1DcaPV/O0TMloq1awc2csBQpE6RtcBmvTBiZMgM6d1QJb1sashLR///4sWbKEpUuX4u7uTkhICCEhIUQ+tm6lv78/PXv2TNiePHkya9eu5dy5c5w+fRp/f39WrVrFgAEDMu63EKmqVEmVhapVS22Hh0Pr1tm0dGenTrBokdTjEEKIjPTNN/Dpp3pHkeGMRrXg34gRifvefRfWrwd396SPXbhQ5eLp+ezMiOfIzsy6ZT9z5kwAGjVqlGT/ggUL6N27NwDBwcFcuXIl4WcxMTEMGTKE69ev4+rqSuXKldm4cSNt2rRJX+QizYoUUcubdeum/tBiY9WtiYsXYdSobHTBW6hQ4hpumpaNfjEhhNDR6tWqyHU2Eh4Ob7wB27Yl7pswQeXdBoNKVkXWMishTcv8p4VPpP5Dhw5l6NChZgUlMl6uXOo95aOPYNo0te/LL1VSOncuODnpG1+GGjRIZd0//KB3JEIIYbuMRggNhWLFoHBhvaPJMNeuQdu2cPKk2nZygp9+gv8KXwidmHXLXtg2e3uYOhW+/z6x83DxYmjZEu7d0ze2DFWlClSrpncUQghh2xYuhOeeU0lpNnHiBNSunZiM5sun5lVIMqo/SUhzoEGD4NdfwVT6MDBQTXbKNivB9emjBgYJIYSwXOfOqtfC01PvSDLE1q1Qvz7cuKG2S5eGAwfUvvR4/fXE+V6pfR06lP74s7vsUVxLmO2111Qi2q4d3LoFZ85AnTrw22+JE6Bs2qNHajWRbt3UzHshhBBpFx8PHh7QsaPekWSIH3+E999PXFyqdm01pyIjcu1KlSB37uT7L16E3bvVjPYqVdTnrJ4ePoTz59X3MTEQEqJ6jJ2c1O+gN0lIc7DatdUM/DZt4OxZtVxaw4awfLlKVG2aoyNs2qSSUUlIhRAi7e7cUT0UP/4IT0xitjXx8WoN+gkTEve9+iosWZLulaYTfPll8n3//quazskJVq1S8zj0FhQEj6/aPnu2+ipRwjrukMot+xyudGnYvz/xlsWjR9ChQzaYD+ToCMeOqR5SIYQQaRcfD82bW0e3WTpER6si8I8nox99BL/8knHJaEpMyWhoqJpM3LZt5r2WORo1UgVonvyyhmQUpIdUAPnzQ0CAKgW1bJl6Lxo4EC5cgIkTwc5WL1vs7dV9ifXr1W0nKQMlhBDPVqgQzJihdxTpcveu6lzZs0dt29nB5Mnqsy0zXbigeiFDQ1XPaHqS0ddfhz//NO+YRYvgxRctf009SUIqADXGZckStQLn+PFq3/ffw+XLakx7Zl5NZqodO9TA/BMn5Na9EEI8y+jR4OsLL7+sdyQWu3AhcSgagKurGor2yiuZ/7qNGqnhb6tWpb8JL11K/B3S6tGj9L2mniQhFQns7GDcOJWU9u2rBn+vXg3Xr2fc4O8s17Kl+osuW1bvSIQQwrrFxamBhvnz6x2JxQ4dSpysC6p86oYN8MILmfu6Fy+qntGMSkZB/VfkJLZ6M1Zkoj59YOPGxFmDhw6Bn5/5V2pWwWBQyaimwWMriAkhhHiCvb3K3mx0ae81a1RSaEpGK1ZUE3ezIhlt1EjNWs+oZDSjPass1eNfepGEVKSoZUvYu1ct0AHqVoSfX+J4HJvz2Weq2Gp0tN6RCCGE9fnjD9i3T31vg+PtJ09WUwUiI9V2w4bq1ylZMnNf99IllQSHhKj63taYjELKk5lS+9KLJKQiVdWrq6tL06JH9+5Bs2ZqLI7NefttteqIs7PekQghhPWZMQP69dM3I7FAXBx8+KGaPW8K/c03VRH8fPky97UvXVI9o8HBKhm1+XKJOpMxpOKpihdXvaKdOqk/8JgY6NpV/SF++qkNXUiXLq2+QJURsNnSAUIIkQlmzlTdfDbzpg4REdC9O6xbl7hv5EhVFzQrfo1evdTE3/Ll1a36VauSP+bNN1VHjjVYsEDNwv/zT4iKgnLlYPBg1YbWQBJS8UweHmpYUb9+MHeu2ufvr27jT5+uSn7ajN691YD9SZP0jkQIIfQXG6t6GMqWhaJF9Y4mzW7eVD2SR46obQcHVeT97bez5vXj4+HoUfX92bOpz7Ho2jVr4kmL339XlQa++Ub1Hq9ZAz16qLbr3Fnv6CQhFWnk6Ahz5qhOxuHD1b4ff1TzhH75Bdzd9Y0vzerUgTx59I5CCCGsw+LFqqzKxYs2k5CeOaPKOpkKuru7q97J5s2zLgY7O7UUpy1ZsiTp9iefwM6dsHKlJKTCxhgMqme0ZEnV0RgTo27j168Pv/2mbu9bvb599Y5ACCGsR5cuULCgzSSju3apgvf376vt4sUTV4kW5rt/H7y99Y5CkYF0wmxdu8L27YkDxv/4Q3U8njypb1xp9vChKmtiut8ihBA5UUyMqhpvI7Nxfv5Z9YKaktEaNVRZQklGLfPTT6rW6Xvv6R2JIgmpsEj9+nDggCqiD6p4fr16qsfU6rm4wPHjahCsEELkRNeugY+P6nK0cpoGY8eqCUJGo9rXujXs3m0zHbtWZ906lYjOng3PP693NIokpMJi5curslC1a6vtBw/Uur2miU9Wy8FBFVl94w29IxFCCH3kygVvvQU1a+odyVMZjWqxls8+S9z33ntq9UCbmbtgZZYvV2NGZ81Sp4C1kIRUpIunp1ou/tVX1XZcHLz7LowYYeXl7AwGVUF52rTES24hhMgp8uWDCRNUGRUrFR6uOjnmz0/c9/XXqkKVQxbPgKlRA774Qv2r53Ok148/qjkgCxeqf62JTGoS6ebmpmbaf/IJfP+92jd+vJoBOX++Fdei/+cfGDpU9RDUq6d3NEIIkfk0TRXQ7NgR2rfXO5pUXb2qktFTp9S2k5OqoanXbPAaNdKfSGbEc6THpEnqI2/69MSlTkGtGFuokH5xmUgPqcgQ9vbqZJ86NbHm/NKlagD63bv6xpaq6tXVu54ko0KInCIqSi2hbMWLg5w4oSbKmpLR/PlVDU1rKE1ky6ZOVXcx+/YFL6/Erxde0DsyxXrPSGGTBg5UxXZdXdX2nj1Qt64Vzx8qWFD9hR46pHckQgiR+VxdYcUKq51Zv3mzmjR744baLl1aTaCVfoP0u3Qp5bXrTfVc9SYJqchwr7yiJm4WLqy2z55VV7tWm/PNnaveAU33L4QQIhsyrF4N27bpHUaq5sxRebKp4HydOmribLly+sYlsoZZCemECRN44YUXcHd3x9PTkw4dOnA2tfWyHrNr1y58fX1xcXGhdOnSzJo1y+KAhW144QX1RlKxotq+dQsaN1a9p1anZ0/VlVukiN6RCCFEprFbvlwV87Qy8fFq0ZX33lM3rEANcd2xwzrGNoqsYVZCumvXLvr378/BgwcJCAggNjaWFi1aEBERkeoxFy9epE2bNtSvX5/jx48zfPhwPvjgA1atWpXu4IV1K1kS9u1Tg6dBTWrv2BEmT9YxqJS4uqraVZoG9+7pHY0QQmSKuBUr1BR1KxIVBd27w1dfJe4bPFgtZ2ka+iVyBrNm2W/ZsiXJ9oIFC/D09OTo0aM0aNAgxWNmzZqFj48Pk//LQipWrEhQUBATJ06kY8eOlkUtbEa+fLBli6ojt2SJyvk++kgtmzxpkpoMZTU+/RSHLVtg1Ci9IxFCiIxz/jweFy+qcndubnpHk+DOHbUM6N69atvODqZMUQvpiZwnXWWfwsLCAMifP3+qjzlw4AAtWrRIsq9ly5bMmzcPo9GIo6NjsmOio6OJjo5O2A4PDwfAaDRilJqRaWJqJ2toLzs7mDcPfHzsGD9eZaBTp8KFC/EsXhxHrlw6B2jy+uvE+/qCnZ1VtJutsKZzzVY83lbyvpZ2cq5Z6LvveGHzZozvvKN3JAn+/RdeecWBc+cMALi5aSxZEsfLL2tWURpazjXLpKe9DJpmWflyTdNo37499+7dY8+ePak+rly5cvTu3Zvhw4cn7Nu/fz8vvfQSN27cwMvLK9kxo0aNYvTo0cn2L126FDcruroT5vv9dx9mzKhOXJwaLfLcc/cYMeIQefNGP+PILKZpqjdBiEwQFRVFly5dAFi+fDkuLi46RySyM0NsLG63bhGRwuetHs6ezcf48bUJC1NFqvPmjWLkyEOULXtf38BEuj169Ihu3boRFhaGh5mLLljcQzpgwABOnjzJXlNf+1MYnvhgN+XAT+438ff3Z/DgwQnb4eHheHt707hxYwoUKGBpyDmK0WgkICCA5s2bp9gLrZc2baBNm3g6dzYQHm7g3Ll8jBrVknXrYhMmQOnJaDRys2NHileurFYxEc9kreeaNXt83H2TJk3ImzevfsHYEDnXzBQTAyEhGL28rKbd1qwx8MUX9kRFqc//ChU01q+3p2TJurrG9SQ51yxz584di4+1KCEdOHAg69evZ/fu3RQvXvypjy1SpAghT5TTCQ0NxcHBIdXk0tnZGecUlvdxdHSUE8NM1thmrVqpMUNt26q69JcuGWjY0JE1axInQOnpYbFiGMqVw8HK2s3aWeO5Zq0ebydpN/NJm6XRrFkwfDicPw/o226apia0fvxx4rLSjRrB6tUG8uWz3v9LOdfMk562MmuWvaZpDBgwgNWrV7Njxw5KlSr1zGP8/PwICAhIsm/btm3UqlVL/pNzsKpVVVmomjXV9v370KKFmviktwvt2qG99ZbeYQghRPr07q3KPD1lnkdWiIuDDz9Us+dNyWiPHrB1q5r4KgSYmZD279+fJUuWsHTpUtzd3QkJCSEkJITIyMiEx/j7+9OzZ8+E7b59+3L58mUGDx7MmTNnmD9/PvPmzWPIkCEZ91sIm1S0KOzerW7jAxiN6k1qzJjENy3dxMaqwni//aZzIEIIYYGoKHB3VyuV6CgiAl57DaZNS9z3+efw009qfXohTMxKSGfOnElYWBiNGjXCy8sr4WvFihUJjwkODubKlSsJ26VKlWLTpk0EBgZSo0YNxowZw9SpU6XkkwAgd25Yt06trWvy+eeqTJSukxvt7eHvv9WYAiGEsCXHjkGJEvDnn7qGERKibsuvX6+2HRxg/nwYPVrmjIrkzBpDmpYJ+QsXLky2r2HDhhw7dsyclxI5iIMDzJih1iweOlTtmz8frlyBX3+FPHl0CMpggNWr5V1TCGF7ihaFd96BChV0C+HMGWjdGi5fVtseHrBqFTRrpltIwsrJWvbCKhgM8MknsGIFmOazbd8O9erp2ElpMKhu2ilTEt9VhRDC2hUpAuPHq6t9HezcCXXrJr5tenuriaySjIqnkYRUWJVOneD338FUgOHPP9WqnseP6xRQVBRMnKiyYyGEsGb374OfHxw5olsIS5ZAy5YqFFATVw8eVBNZhXgaSUiF1XnpJThwAMqWVdvBwVC/PmzapEMw7u7q3pMVrXAihBApevhQ9Y4WLZrlL61pakJqjx6J4//btFETV3UIR9ggSUiFVXruOZWU+vmp7YgINVl09mwdgsmdW73brl+v80wrIYR4iuLFYc0aKFYsS1/WaFTX7J9/nrjvvffUhNXcubM0FGHDJCEVVqtgQXX7/o031HZcnJqN/+mnEB+fxcGcPQuvvgqbN2fxCwshxDPEx6s3Rx0mD4eFqZ7QBQsS9339NcycqdsQVmGjJCEVVs3VFZYvVxOeTL75Brp2VcM7s0yFCnDqlO41/YQQIpnbt+HwYQgPz9KXvXpVTTw1DbF3dlYTU4cOlQIlwnySkAqrZ2enktDp09X3ACtXqhmbt29nYSCVKql/T53KwhcVQohn8PSEoKAsXXv5+HE14dRU6jR/fpWYduqUZSGIbEYSUmEz+vVTwzhz5VLb+/ap0iL/LdOcNfbuhWrV1IsLIYTeZs2Cc+cSr9azwObN0KCBmnAKUKaMmklfr16WhSCyIUlIhU1p21bN2ixSRG2fOwd16sD+/VkUwEsvqeVE69bNohcUQohUPHoE334LGzdm2UvOng3t2qkJ/aAmnh44oCaiCpEekpAKm/P883DoEFSurLbv3IEmTdSqTpnOYFBZscGQxeMFhBDiCW5uagjRgAGZ/lLx8TBsmJo7FRen9nXsqCaeFiqU6S8vcgBJSIVN8vFRd82bNlXb0dFqNv7EiapCU6bbulUF8c8/WfBiQgjxhIAACA1VSWkmT2ePioJu3dTseZMhQ9RYflfXTH1pkYNIQipsVp48qlh+796J+z75RHUWxMZm8os3bKhmWpUsmckvJIQQT4iNVYU+x43L9Je6c0dNIF2xQm3b2akJpt9+m6XDVkUOIFXChE1zcoL586F06cSizDNmqDWUly/PxKLMLi6Jt8mMRnB0zKQXEkKIJzg4qHFLmfy+8++/0Lq1GqsPqjN2xQp4+eVMfVmRQ8n1jbB5BgN89hksWpT4/rxxo+rENM0CzTQbNkD58nDvXia/kBBCACdPqhlFhQpB3ryZ9jIHDqgJo6ZktEgRNaFUklGRWSQhFdlGjx6wZYu6lQ9q0ZI6deD06Ux8UV9feP116SEVQmS++HhV6HPgwEx9mVWr1ERR07zNSpVUWSdf30x9WZHDSUIqspUmTVQJqBIl1PaVK6pC0++/Z9ILFi2qxpKa1rsXQojMYmenBs6PHp0pT69pMGmSmiBqWgmvcWM1gdT0nipEZpGEVGQ7T17Nh4dDq1bw00+Z+KJbt6pK0Vm6nqkQIse4fl2NVy9dWlX4yGBxcarj9eOPE6+te/ZUd50ycWSAEAkkIRXZUpEisGuXKuAMalJq794walQmdWR6e6suBElIhRAZTdPg1Vfhrbcy5ekjItTTT5+euO+LL2DhQjVxVIisILPsRbaVKxesWQODBsEPP6h9o0fDxYvw448Z/EZbqRIsWZKBTyiEEP8xGNQSoZlQZykkRE1UOnpUbTs4qPfHx8vpCZEVpIdUZGv29jB1qhoXZTCofYsWqVv49+9nwgvu2aMmHZiWMhFCiPQID1c9pM8/DzVqZOhTnz6tJn6aklEPD3WLXpJRoQdJSEW2ZzDARx+ppUVdXNS+nTvVsvSXLmXwizk4qBJQmZLtCiFyFE1TVTzefjvDn9r0Hnj5str29k66+p0QWU0SUpFjvPaaehMuWFBt//WX6h0ICsrAF/HzU0v6FSiQgU8qhMiRDAa1/FzPnhn6tIsWQcuWEBamtp9/Xk0ErVIlQ19GCLOYnZDu3r2bdu3aUbRoUQwGA2vXrn3q4wMDAzEYDMm+/v77b0tjFsJideqoN95y5dT2zZuqgP6GDRn8QidOqMGrUgpKCGGJ6Gj1b/PmqvZSBtA0+PJL6NVLTdgHaNNGTQAtWjRDXkIIi5mdkEZERFC9enV+MM0SSaOzZ88SHByc8PXcc8+Z+9JCZIgyZVSt0nr11PajR9ChQ9IZpukWHKy6Y+/cycAnFULkCJoGHTuqsUYZxGg08O679nzxReK+vn1h3bpMXGJZCDOYPcu+devWtG7d2uwX8vT0JK8UMxNWokABdWf9rbfUmvfx8Wpp+n//taN+/Qx4gdatoUULNatKCCHM1bUr5MuXIU8VFgZjx9bhjz8S+6C++QaGDEmc7CmE3rJsDGnNmjXx8vKiadOm7Ny5M6teVohUubjAzz+Dv3/ivu+/t+fbb18gMjIDXsDeXtWYyrTip0KIbCc+XmWJ3bur++npdOUKNGzowB9/eALg7AwrV6qhqZKMCmuS6XVIvby8mDNnDr6+vkRHR7N48WKaNm1KYGAgDRo0SPGY6Ohook3jZ4Dw8HAAjEYjRtPAF/FUpnaS9nq20aPB29vAwIH2xMUZOHCgKM2bx7FmjZFChdL33IZjx7BfsIDYd95R1fqzITnXzPd4W8n7Wtpl+3NN07Dv0gWtalXiR45M99MdPw4dOjgQHKwyzwIFNFatiqNuXY3s2oQZJdufa5kkPe1l0DTLu24MBgNr1qyhQ4cOZh3Xrl07DAYD69evT/Hno0aNYnQKa/UuXboUNzc3S0IV4pmOHfPkm29eICpKXacVLhzB558foFixiHQ9r11MDPGy3Il4TFRUFF26dAFg+fLluJjqkYmcTdMos349jwoXJrhOnXQ9VVCQJxMnJr6feXk95LPPDlK0aPrez4R4mkePHtGtWzfCwsLw8PAw61hdEtJx48axZMkSzpw5k+LPU+oh9fb2Jjg4mAJSTidNjEYjAQEBNG/eHEdHR73DsRlHj8by8stw544rAPnzqx6Fl15K5y3327exW7CA+Gw4aEvONfNFRESQ77/xgaGhoTK+Po2y9bmmaRn23jBnjh0ffGBHfLx6vtq14+jffxsdOzbMfu2WSbL1uZaJ7ty5g5eXl0UJqS5Lhx4/fhwvL69Uf+7s7Iyzs3Oy/Y6OjnJimEnazDy+vvD11zuYMqUFp04ZuHvXQMuWDixaBJ07p+OJjx+H777DvksXKF06w+K1JnKupd3j7STtZr5s12aapmqN1q6tZldaKD4ehg2Db79N3PfGGzB3bjw7d8Zkv3bLAtJm5klPW5mdkD58+JDz588nbF+8eJETJ06QP39+fHx88Pf35/r16yxatAiAyZMnU7JkSSpXrkxMTAxLlixh1apVrFq1yuKghchMBQtGsXNnLN26ObJtG8TEQJcualWnoUMt7MRo00ZNcDLzilEIkQPEx4OXFxQubPFTREWpnPaXXxL3ffIJfPWVrGQsbIPZCWlQUBCNHyvSO3jwYAB69erFwoULCQ4O5sqVKwk/j4mJYciQIVy/fh1XV1cqV67Mxo0baZMBsweFyCweHvDbb9CvH8ydq/YNGwYXLqh6pQ6W3Fvw8FBFTxcsUE+czW7dCyEsZG+v6jBZ6PZtaN9e1VcGsLODadPU2wxIQipsg9kfq40aNeJpw04XLlyYZHvo0KEMHTrU7MCE0JujI8yZA6VKwYgRat+cOaqMysqV4O5uwZMePKi6LRo0gKpVMzReIYSNiY+HTp3ULZjXX7foKc6fVzdgzp1T27lywYoV0LZtBsYpRBaQteyFeAqDAYYPV/VKTRPlt2yB+vXh+nULnrBJE7h8WZJRIYRaHjRXLsiTx6LD9+9XyyGbktEiRdQyoJKMClskCakQadCtm1rZybRwyh9/qA+CkycteLJChSA2Vo0FiI3N0DiFEDbE1RV++kmtV2+mX39V17em1YkrV1Y3YHx9MzhGIbKIJKRCpFGDBqpHolQptX3tGtSrB9u2WfBkp05B//6wd2+GxiiEsAGxsdChA1iwaqGmwcSJava8qTpikybqraREiYwNU4isJAmpEGaoUEH1Qrz4otp+8ECN35o/38wnqllTzZBq1CijQxRCWLsHD1RSmiuXWYfFxqqqUJ98krivVy/YvBmklK2wdZKQCmEmT0/VsWFaDyIuDt55B0aONHPJ+mLF1AFLlsDDh5kRqhDCGuXLp8p4mK5s0+DhQ/WeM2NG4r5Ro1TRDlkITmQHkpAKYQE3NzWGa9CgxH3jxsGbbybeRkuTGzegb19YsyajQxRCWJuHD6FdO7MHnwcHQ8OGsHGj2nZwgIUL4YsvpHqcyD50WalJiOzA3h6+/16NKR00SHV2Ll2qxpauWQP586fhSYoVg7//huLFMztcIYTe7t2D8HBIYSXC1Jw+rYYFmcp758kDq1ercaNCZCfSQypEOn3wgUpAXV3V9u7d8NJLamGmNDEloxs2qB5TIUT25O2t6jKVL5+mh+/Yod5LTMmojw/s2yfJqMieJCEVIgO0b68+Zzw91fbff6uyUIcPp/EJHj2C996DefMyLUYhhE4uXYKWLdXtkzRatAhatYKwMLX9/PNqQmXlypkTohB6k4RUiAzywgvqA6NCBbUdGqom0a9dm4aD3dzg0CE1M0oIkb3cv6+myOfO/cyHahqMHq1mzxuNal/btuqC18src8MUQk+SkAqRgUqVUrVKGzZU25GR8NprMGVKGg729lYzFPbvh6CgTI1TCJGFatSA339/Zm2mmBh46y01e96kXz91UZuGXFYImyYJqRAZLF8+2LpVzbgH1eMxaBB8+KEqEfVUmgZDh8LkyZkcpRAi0wUGqvE8Dx4886H370Pr1mrhJpOJE+GHH9SseiGyOznNhcgEzs5qDFjJkjB2rNo3daqanPDzz+oOfYoMBjVDyrRGqRDCdsXEqCKhqf7BK5cvq9vyp0+rbWdnVZ749dezIEYhrIT0kAqRSQwGGDNGzVMy9XCsXavGld68+ZQDCxVSB/z9N6xcmQWRCiEylGmFjBYt4JdfVI24VBw9qiZAmpLRggXV7HpJRkVOIwmpEJns7bdh0yZwd1fbR46oD6AzZ55x4Jw5MGGCmgwhhLAdkyfD//73zKXbfvsNGjSAkBC1XbYsHDgAdetmfohCWBtJSIXIAs2bq/qBppKjly6pD51du55y0IQJsHevDCATwtbky6dqwD1lGaUZM9Tw0keP1PZLL6lktGzZLIpRCCsjCakQWaRqVVXZqUYNtX3/vkpUf/45lQOcnSFXLlUs/5tvntnbIoTQmWnWYu/eiYPHnxAfD598Av37q+8B3ngDtm9Xt+uFyKkkIRUiCxUtqlZyat1abRuNajb+uHFPyTf37lW3AE339YQQ1icuDl5+GSZNSvUhkZHQubOaPW8ydCgsXw4uLlkQoxBWTBJSIbKYuzusX68WZjIZORL69EkshJ1Ep05w9qxUxRbCmhkM6r57tWop/vjWLWjaFH79VW3b2cHMmfD11+p7IXI6+TMQQgcODokfRibz56vSL+HhKRzg7q5qGQ4cCHfuZFmcQog0iIpSWeXIkdCsWbIfnzunxowfOKC2c+WCDRugb98sjlMIKyYJqRA6MRjU7boVK9RwUYCAAKhXD65eTeGAe/dg48bE+jBCCP3duAHlyqlSGinYtw/8/OD8ebXt5aWG7bRpk4UxCmEDJCEVQmedOqkJDfnzq+1Tp1RZqBMnnnigj4+6dd+gQVaHKIRITf780L07vPhish+tXKlu05tualSpAgcPwvPPZ3GMQtgASUiFsAL16qkPqjJl1PaNG1C/Pmze/MQDHR1VXdKPPlIDUYUQ+rl1S81GmjAhyRR5TVOFMTp3huhota9ZMzU/0cdHp1iFsHJmJ6S7d++mXbt2FC1aFIPBwNq1a595zK5du/D19cXFxYXSpUsza9YsS2IVIlt77jk1xszPT20/fAjt2qn6+EnY26uM9anLPQkhMtWyZVC+PFy7lmR3bCz06weffpq4r3dvdUc/T56sDVEIW2J2QhoREUH16tX54Ycf0vT4ixcv0qZNG+rXr8/x48cZPnw4H3zwAatWrTI7WCGyu0KF4PffE5cNjItTs/GHDUusWYjBoOrEvPuu2pb6pEJkvdatVb0202oXqIvI9u3h8T6XL79UExYdHXWIUQgbYvYSMK1bt6a1qYhiGsyaNQsfHx8mT54MQMWKFQkKCmLixIl07NjR3JcXIttzdVUTnT79NLFe4ddfq9WdFi78r16haQWYadPUWqQ//fTUVWGEEBkkJETdpShUCN5/P2H3jRuqDOnx42rb0VElom++qVOcQtiYTF+T8MCBA7Ro0SLJvpYtWzJv3jyMRiOOKVw2RkdHE20aeAOE/1cHx2g0YkyxUKN4kqmdpL3MY03tNn48+PjYMWiQHfHxBlasgKtX41m1Ko4CBdRjDAULYvD0JD4mRrdihtbUZrbi8baS97W0s4Zzzf7dd+HuXeJ27ky4CPzzT2jf3oGrV9V2njwav/wSR6NGWsq1hbOYNbSbrZE2s0x62sugaZbf7zMYDKxZs4YOHTqk+phy5crRu3dvhg8fnrBv//79vPTSS9y4cQOvFIp9jxo1itGjRyfbv3TpUtzc3CwNVwibdORIYSZOrEV0tLp+LFr0IZ99dgAvr0dJHmcfHU2cqX6UsGpRUVF06dIFgOXLl+Miy/TYDJdbt3B+8ICw0qUB+OOPQnz99Qs8eqQ6VwoVesTnnx/E2/uBnmEKoYtHjx7RrVs3wsLC8PDwMOvYTO8hBZW4Ps6UAz+538Tf35/BgwcnbIeHh+Pt7U3jxo0pYOoaEk9lNBoJCAigefPmKfZCi5RZY7u1aQPt2ml06KAREmLgxo3cfPZZM1avjqNOnf/+lvbvx75TJ2K3bYNKlbI0PmtsM2sXERGR8H2TJk3ImzevfsHYED3PNcP27Wh168JjnSKLFhkYM8ae2Fj1WebrG8+aNY4UKVI/S2N7FvkbNZ+0mWXupGPhlkxPSIsUKULIE2twh4aG4uDgkGpy6ezsjHMKPT2Ojo5yYphJ2swy1tZutWurslBt26q6+LdvG2jRwoElS6BjR6BWLejTB8fnntNt9oS1tZk1e7ydpN3Ml+VtFhYGXbvCiBHwySdoGowapSYsmbRrB8uW2ZErl/VWU5RzzXzSZuZJT1tl+l+On58fAQEBSfZt27aNWrVqyX+yEGYoUULVMWzSRG1HRcEbb8CkSaC55VIzft3cVBmax8ZgCyHSKU8eVZNt0CBiYqBXr6TJ6IABsGaNWhJUCGEZsxPShw8fcuLECU78t4zMxYsXOXHiBFeuXAHU7faePXsmPL5v375cvnyZwYMHc+bMGebPn8+8efMYMmRIxvwGQuQgefOqYvm9eqltTYOPP1ZL3MfGohLRl16CL77QM0whsofbt1XR+7g4qFiRew8dadkSFi9WPzYY1AXh1Klq4r0QwnJmJ6RBQUHUrFmTmjVrAjB48GBq1qzJ559/DkBwcHBCcgpQqlQpNm3aRGBgIDVq1GDMmDFMnTpVSj4JYSEnJ1iwAB6f9zd9Orz6KkTEOqsiiB9/rF+AQmQXgYEq2wwJ4dIlda0XGKh+5OICv/6qFk2TimtCpJ/ZY0gbNWrE0ybmL1y4MNm+hg0bcuzYMXNfSgiRCoMBPv8cSpaEd95RvaO//QYNG8KGDa3xKgTcvw8nT0KDBjpHK4SNev11aNmSoLPuvPxy4uJoBQvChg1Qp46+4QmRnVjv6GshxDP17AlbtyYuSXj0qPqQPH0aNaa0e3c12FQIkXbjxsGMGQBsCHSnYcPEZLRcOTXBUJJRITKWJKRC2LgmTWDfPvDxUdtXrvx3a7HJl7Bz539LOwkh0kTT4M4duHuXH36ADh3g0X8lf+vVg/37oUwZXSMUIluShFSIbKByZdVr8/zzajssDFq0d2XR/rIQEwP+/mqChhAiddHRYDAQP3ESH98dwcCBEB+vftSlCwQEgJTCFiJzSEIqRDbh5QW7dqn1tAGMRjUb//thN9F+/hmCgvQNUAhrdvYsPPcc0Tv3q3Jq3yfOVBo2DH7+WW42CJGZJCEVIhvJnRvWroX+/RP3Df7em/81/IeYJq3UDstXCxYi+/L25lGb13l5aCVWr1a77O1h9mxV+clOPi2FyFRZsnSoECLr2NvDtGlQujQMGaLyz7lLXLhwA36rOx7XsBCYMkVq1QgBarxoRAT/RPnQOmASFy6o3blzw8qV0Lq1vuEJkVPINZ8Q2ZDBAIMHwy+/JN5m3LEDvplXgPuOhfQNTghrMmAAD5t1wK+OlpCMFi0Ke/ZIMipEVpKEVIhsrGNHlYgWLKi2RwW/R8Wln3H0mAGuX9c3OCGswLqGk2h8cT5376k7BlWrqgmCNWroG5cQOY0kpEJkc35+6gP2uefUdkgIDKx3nLhSZeD33/UNTgg9xMejfTuR70eH0+F9L4JiawDQvDns3Qve3vqGJ0ROJAmpEDlAmTJw4ICqTwpwMKo6/WKnMesvWcVJ5Dyx/1zg0Wfj2T5qT8K+d96BjRvBw0PHwITIwSQhFSKHKFAAtm+Hzp1Bw4452ru8/4Ej3771F/E7d+kdnhBZ4kG4RruPylI8+l820RaAsWPhxx/B0VHn4ITIwSQhFSIHcXGBpUtVXUWTYgvH8s8bw4l8JOWgRPYWNmw8AaX+x5YtGvfJh5MTLFkCI0ZI0Qkh9CZln4TIYezsVF3FUqWgXz94N+5HnO9EU6GZgXXroJBMwhfZ0MmTMH9OcXLfiwcM5M2ravY2bKhzYEIIIBsnpHFxcRiNRr3D0I3RaMTBwYGoqCji4uL0DsdmpNRu9vb2OGbDe3n/+5+avNGpUy7uPczF3wfu8leJ7pT4ZSIl21bWOzwhMsy+H/+i9ceVePCgJwAlS8KmTVCxor5xCSESZbuEVNM0QkJCCAsLQ8vBK9JomkaRIkW4evUqBrkXlWaptZuzszMFCxbEI5vNeGjdWtVbbNsWom/EExMZS49ucXy1KXEClBC27Df/fbT5qj7V2cVe6vPCC7BhAxQurHdkQojHZbuENCwsjPv371OoUCFy5cqVY5Ox+Ph4Hj58SO7cubGTNe/S7Ml20zQNo9FIWFgY1/+r25ndktIaNVRZqLZtC9LiVACEQ4smsayYeJWXB5bSOzwhLKJp8NlnMO6rurRnNXupR/v2agy1m5ve0QkhnpStElJN0wgNDcXDw4OCpkrgOVR8fDwxMTG4uLhIQmqGlNrN1dUVd3d3rl27xu3bt7NdQgrq1v3evfD66xAQAMNiRvP8B/P47v55Bo90kwkfwqZER8PCxj+x90AJoBHr6MAHH8CkSWppXSGE9clWmUpcXBxxcXHZMmEQ+jIYDOTJk4fo6OhsOzbZw0PVYXz7bfiej+jNQoZ87ka/fhAbq3d0QqTNvXvQqkU8JQ8spT3rMBhg8mSYMkWSUSGsWbbqIY3971PTwSFb/VrCSpgmNsXFxWXLSU6g6jDOnQulS+dn5MgWat+sqXx4qiFfba6Ou7vOAQrxFBcvQqeWYQSdy8MB1mHn4szqZdChg96RCSGeJVv1kJrk1HGjInPllPPKYFB1GZcsgdyO0fRkEbn2baVBA7hxQ+/ohEjZ4cMwq9oMfjlXnTzcx6OQCzsDDZKMCmEjpCtRCJGi7t2heHFnXmm/m+AwVzgBLWvdYenWAlStqnd0QiRatw66doX8ka/wiDiKlM/Lpk1QurTekQkh0ipb9pAKITJGw4bw+wE3SpY0UIsj7A8uyUd1DhAQoHdkQihTp2j83mEahsgIrlOckw0Gsn+/JKNC2BpJSIUQT1WxoioL5ehbnVGMYvcjX9q0gfnz9Y5M5GRxcTBoEEwedJGxjKAZ2+nWDbZtg/z59Y5OCGEuixLSGTNmUKpUKVxcXPD19WXPnj2pPjYwMBCDwZDs6++//7Y4aCFE1ipcGLbvduLf9h9jxImysWf4450pfPaZqvcoRFZ69Aje7PCQaVPiuEhpynKeqiPas3gxODvrHZ0QwhJmJ6QrVqxg0KBBjBgxguPHj1O/fn1at27NlStXnnrc2bNnCQ4OTvh67rnnLA5apM50ATB69OhMff5Ro0ZlyvOnVXx8PNWrV6dNmzYWHX/+/HkcHByYMWNGBkeWfbm5wapV8OGH0JrN9GEu3419RO/e9hiNcrNFZI37951o1Uzjo9+aMJ7h2NvD+B89GTsWpOSyELbL7D/fSZMm8c4779CnTx8qVqzI5MmT8fb2ZubMmU89ztPTkyJFiiR82UtBOJEOCxcu5OTJkxYnxmXLlqV79+6MGjWK8PDwjA0uG7O3VzUdS0wejB8HicSN28sCcOqzmHs3IvUOT2RzF7aeo8B7s/kzKJofGMA6t25s3Ah9+ugdmRAivcyaZR8TE8PRo0cZNmxYkv0tWrRg//79Tz22Zs2aREVFUalSJUaOHEnjxo1TfWx0dDTR0dEJ26aEwWg0PrUoudFoRNM04uPjiY+PT8uvlO2Yfm/tv/uopvbIjOfXq43j4uIYPXo0DRs2pFatWhbH8fHHH7No0SKmTJnCiBEjgKe3W3x8fMJSojn9gqpfPyhWzJnhb15iQ3Q7nMOMnKhwhQfbf8HnxSJ6h2f1Hn8fe9b7mlBOTdtN2Y9fpTwPWEZX+hVdy+p1GtWrG5HmezrT+SXnWdpJm1kmPe1lVkJ6+/Zt4uLiKFy4cJL9hQsXJiQkJMVjvLy8mDNnDr6+vkRHR7N48WKaNm1KYGAgDRo0SPGYCRMmpHjLeefOnbg9ZRFiBwcHihQpwsOHD4mJiTHjN8s+Hj16BJDw+z948CBTnj86Olq3nsXNmzdz5coVhgwZkq4YfHx8qFKlCnPmzKF///5JllhNqd1iYmKIjIxk9+7dCYsw5GSOjvBZt78otyAWA/BX1CHu1K/L0kGjyNswZy/d+yxRUVEJ3+/YsQMXFxcdo7F+D6ce4+Ud46hGHADrHM4xYcgqrl934fp1nYOzIQFSHsNs0mbmMeUIlrCoDumTBcI1TUu1aHj58uUpX758wrafnx9Xr15l4sSJqSak/v7+DB48OGE7PDwcb29vGjduTIECBVKNKyoqiqtXr5I7d+4c+wZvStidnJz4448/GDduHAcOHMDOzo7GjRszadIkSpYsmfD4hQsX8s477zBv3jx69+6d5LkCAwNp2rQpn3/+OV988UWS53d2dubEiRN88cUXHD16FCcnJ1q2bMnXX39N8eLFk8W1e/duJk6cyMGDB3nw4AE+Pj506tQJf3//JBcZj79mixYtGD16NIcPHyYsLIy4OPVh9Msvv2AwGOjevXuKy8RWrVqVv/76K9U2mjBhAkOHDgWgS5cujBw5kiNHjtC8eXM0TePBgwe4u7snO6ejoqJwdXWlQYMGOfb8SqZNG+q2bYxL5zdwi7uGm3aNTt8P4LjTUl4Y87Le0VmtiIiIhO+bNGlC3rx59QvGisVGx7G/wQjaH58EwCVgv3sjihxdRqWSqX8WiKSMRiMBAQE0b948264yl9GkzSxz584di481KyEtWLAg9vb2yXpDQ0NDk/WaPk2dOnVYsmRJqj93dnbGOYWpko6Ojk89MeLi4jAYDNjZ2SXp7cpJTL/30aNH+e6772jYsCHvvfcex48fZ926dfz555/8+eefCQmV6fEptZlp29Smj+87dOgQX331FW3btuWDDz7g2LFjLF++nH379nHkyJEk58OsWbPo168f+fLlo127dhQqVIgjR44wfvx4AgMD2blzJ05OTkme/8CBA0yYMIHGjRvzv//9jytXrmBnZ4emaezatYsKFSqQP5XaLl27dk3WgxkdHc3kyZOJjo6mQYMGCa9Tt25dQCXCLVu2TLhN//jv/Hh7GAyGZ56HOY3PK76smzyBKv6TqfDwKK5EUefrjuz762vqrR2CwS5nrHBljsfPHzmfUnbv6kPO+Han6a31Cft2Vh3IvZGNeaFkAWkzC8i5Zj5pM/Okp63MSkidnJzw9fUlICCAV199NWF/QEAA7du3T/PzHD9+HC8vL3NeWphp06ZNCb2epsSqZ8+eLF68mLVr19KlS5d0Pf/WrVuZO3cu77zzTsK+L7/8ki+++ILhw4czb948AP766y8GDhxIjRo12L59e5Ik8quvvsLf359p06bx8ccfJ3n+gIAA5s2bx9tvv51k/5kzZ7h79y6tW7dONbaRI0cm2Y6KiqJDhw7ExMQwb968hCQUoFatWgDPHAMtns7R253iF3awv9Z71L2yHDs06m8Yyu6yf1IraDZu+aVHWaTdv1vOEdv+NerG/AlAHHbs7/YD9Rb2YdOmTTpHJ4TIDGbfsh88eDA9evSgVq1a+Pn5MWfOHK5cuULfvn0Bdbv9+vXrLFq0CIDJkydTsmRJKleuTExMDEuWLGHVqlWsWrUqY3+TZ6hVC1IZ5mo1ihSBoKCMea4GDRrw2muvJdn39ttvs3jxYo4cOZLuhLR8+fLJksVPPvmEH374gWXLljFz5kycnJyYPXs2sbGxTJ06NVmP5tChQ5k0aRLLli1LlpDWrFkz2fMDXLt2DSDNPfKPHj3ilVdeITAwkIULF9KjR48kP3d3d8fFxSXheYV5IiMjqV+/PmFhYRw9ehS/i0v5vXFFmu5WQzwaXFzE6eKnyb1lFSUalNA5WmELgkb9RvnRXXHnIQBhhjxc/vYXavWrh5+fH2FhYTRu3Fh6rYTIZsxOSDt37sydO3f48ssvCQ4OpkqVKmzatIkSJdSHTXBwcJKapDExMQwZMoTr16/j6upK5cqV2bhxo8X1Iy0VEkKOGvxes2bNZPtMYzvv37+f7ud/6aWXko2xdHV1xdfXly1btvDPP/9QpUoVDh48CMCWLVvYvn17sudxdHRMcZGEF198McXXNY1PyZcv3zNjjIiI4OWXX2bPnj0sXryYrl27pvi4/Pnzc/v27Wc+n0guPj6eo0ePJnxvsDPQdNfnHPy4AtUn9cKVKCpHHuVeo5oc/nI5L45soXPEwlrFGePZ2+JLGgYmTmi95Fweh43rqNa0PBEREUnONSFE9mLRpKZ+/frRr1+/FH+2cOHCJNtDhw5NmECipyI2UIkmI2PMkydPsn0ODuq/2zQ5KD08PT1T3G/quQwLCwPg7t27AIwbN86s50+tB9TV1RVQPXNP8+DBA9q0acPBgwdZvnw5r7/+eqqPjYyMfGr1BmG+Ot914kL9Cth3eo0Sxn/Jp92j1met2LFzDI22+mPnkDPHeIuUhf59l0svdafh3S0J+/YXfZ1qQfPJ7eWuY2RCiKxiUUJqizLqVnh2YxpfmlIZI1NSmZLQ0NAU99+8eRNITIhNs+DDw8Nxd0/7B0tqVRsKFSoEJCa6KQkPD6dVq1YEBQXxyy+/0KFDh1QfGx8fT1hYGJUrV05zbCJtSneoRvj5IIJqv0mtkI3YodFkx0iOFN1PqV0LKVixkN4hCitwbNo+Cg/qyovxVwE1XjSw5QQab/wEO3uZECdETiHdFDmc6db39RTGMxw/fjzV4/bt25dQRN4kMjKSo0eP4urqSrly5QCoXbs2QMKt+/SqXLkydnZ2nDt3LsWf379/n+bNm3Ps2DFWr1791GQU4Ny5c8THx1O1atUMiU8k5eGTF99r69ndYizxqOTihVub0KpUIeibHTpHJ/QUZ4xnR7PxVP+gAcX+S0bvGgpweuIWmm4ZKsmoEDmMJKQ53PPPP4/BYGD58uVJinWfO3eOKVOmpHrc2bNnmT9/fpJ93377Lbdu3aJr164JZZz69euHg4MDAwcO5OrVq8me5/79+09NfJ+UN29eqlWrRlBQULKE+O7duzRt2pSTJ0+yZs0aXn752XUwDx06BEDDhg3THIMwj8HejgZbR3BiwhbuGtTEtkLxoTz/aTN+rz2c6IeyEkpOE3oyhBNFWtLk9xHYo8aDnszbgPhjJ6j2cXOdoxNC6CHH3LIXKStWrBidO3dm+fLl+Pr60qpVK0JDQ1mzZg2tWrVKtRpCixYt6NevHxs3bqRChQocO3aMrVu34u3tzfjx4xMeV6VKFWbMmMH7779P+fLladOmDWXKlCE8PJwLFy6wa9cuevfuzaxZs9Icc4cOHRg1ahRHjhxJMvmpa9euHDt2jMaNG3Po0KGEZNOkaNGi/O9//0uyLyAgAHt7+zQlryJ9nh/WgtA2pznapCe+dwKwQ6Pp4Qn86bkd17XLKNOijN4hiiywb8gaKk3qg6+mht3EY2Bvo894actn2DvLR5IQOZX89QvmzZtHoUKFWLlyJdOnT6d8+fLMmTOHokWLppqQ+vn5MWLECEaOHMmUKVNwcnKiS5cufPPNN8kmJL377rvUqFGDSZMmsXv3btavX0+ePHnw8fHho48+olevXmbF26dPH8aMGcOSJUsSEtL4+Hj27t0LqCVmd+7cmey4N954I0lC+ujRI9auXUu7du0oWrSoWTGIRAULFkzzUr2e1YpQMGQLe16diN9vw3EgjiqRR4hoWY09Xb6l3pK+GOzlxk12dO9SGKcaf0CDS4sS9oXYeRH87c80GNw4Tc9hzrkmhLAxmg0ICwvTAO327dtPfVxkZKT2119/aZGRkVkUmfWKi4vT7t27p8XFxekdSqbo2rWrVqBAAe3hw4cWP8e8efM0QNu1a1fCvqe1m5xfKYuJidHWrl2rxcTEmHXc2UUHtcuOpTUNEr6O5WusXdl7OZMitR4PHz7UAA3Q7t27p3c4me7guO1asF3RJP/Xh73aabf/umnW81h6ruV00m7mkzazzO3btzVACwsLM/tY6YoQNmncuHE8fPiQ6dOnW3R8bGws48eP55VXXqFBgwYZHJ1Ii3I9alPw2h/srtQ3YV/NezvxqFeV7V3nERerPeVoYQvCr4axs1I/ao9oRpH4G2ofHhx4byG1rq2jQMWUy8cJIXIeSUiFTSpVqhQ//fQTuXLlsuj4a9eu8eabbzJp0qQMjkyYw80zNw1Oz+TwmK0E2xcDIA/hNFvehz/z1efchuSLJgjrp8VrHBiyiuiS5Wl8ZmbC/j/yNyby8Cn8ZvXCYCez6IUQiWQMqbBZnTt3tvjYkiVLMmrUqIwLJoeKjIykVatW3LlzJ13LOb44sgXhPf/kQMuP8Pt7IQDVH+4j5pVq7GroT511/jjnccnAyEVmubTnKsEdB+B3a33Cvofk4mSXCfgt6W/xGOGMOteEENZJekiFEBaLj49n9+7dnD59Ot3LOXr45MXvzAJOTdzCNYeSADhhpOGuLwktWImDX25Dk7v4VisqLJrtzb+mUIMKSZLRIwVbc3fPX9RdNjBdE9Yy8lwTQlgfSUiFEFal6sctKXjzNIF+/hj/u4njHXuROl+05JBnO85t/EfnCMXjtHiNo6PWc7NQZZptH0YuHgEQaleYQx+vpNbNjfjU89E5SiGEtZOEVAhhdVzyu9Fo/3gu/HqcP939EvbXuf0bpV6uxM6ag7l38b5+AQoATv/yF8cLNsd3dHtKGP8F1NKf+6v3xe3SGWpPfEPGigoh0kQSUiGE1SrfsQqV7+3l8MDFhNirWrEOxNH4xPfElynLrvaTiLwbqXOUOc/lvVcILP02FTpV5fl7vyfsP+7RkAu/HKPuiZnk9s6nY4RCCFuTLRNSTQaaiUwg55U+DPZ2vDj1TfKE/MPuRp8TiZrcVEC7Q8P1H3Ov0HPs6DKbqAeyBGlmu/1XKIE1B1G0fhkaXVyQsOxniH1R9n6wkup3d/Lc69V1jlIIYYuyVULq4KDGm8XGxuociciOjEaV8Njb2+scSc7kWjAXDXaOJuzQWfaV7E486lZw0fjrNFnRlzt5y7Cj50KiH0pimtFC/rjJ7y8Ow7lyGRqdmIIj6j32viEv+16eQL5b56g35Q3s7OX2vBDCMtkqIbW3t8fe3p7w8HC9QxHZjKZphIWF4ezsLOVmnuDm5oazs3OWvV6RF3146eISzv/6B4e92ifsLxZ/lSaL3+JW3rLseWMqEaERWRZTdnUx8DI7Kg8gT42SND3yNe48BOARruyuOwy7ixd4acMwnPO5ZUk8WX2uCSGyTraqQ2owGPD09CQ4OBhnZ2dy5cqFwZAzr9jj4+OJiYkhKioKO7tsdd2RqZ5sN03TMBqNhIWF8fDhQ4oVK6Z3iFYlV65c3L9/n02bNlm8SIGlynWsCh3XcnbxYSIHD6fGbTWWsXjcFYr/+iH3V40ioO6HVPqhH8VqFMrS2Gzd30uPETr8e+peXkYp4hL2x+DIwcp9KL/kMxrU8MrSmPQ814QQmS9bJaQAefLkITIyktu3b3Pr1i29w9GNpmlERkbi6uqaY5NyS6TWbs7OzhQrVgwPDw8doxMpKd/jReixnb/n7iVsxNfUDv0NgLzaPZrvG0V0zfHs9XmdvCMHUKVPHZC/hxRFhsVwZNgq8v08jaoPDlDhsZ89JBfHXuhLhTmDaVCjqG4xCiGyr2yXkBoMBry8vPD09EwY85cTGY1Gdu/eTYMGDeQWsxlSajd7e3tpQxtQoU896FOPv1ac4p7/N9S+uAwH4nAmhnpXlsL/lnL2wxoEd3ifauM6k79UHr1DtgrnN5/jwhc/US1oHg20kCQ/Czd48Efjj6j240AalC6gU4RCiJwg2yWkJqbxpDmVvb09sbGxuLi4SDJlBmk380RFRfHaa68RGhpKkyZNrKLNKnWuCp0Xc+vIGP75cDpVDv5IHi0MgPKRJyi/7D0il33IvmKvYOjVk1rDW+CUS/+4s9Lt8/c5M2oF+dcvoPKDQ5R94uf/OFfhxmsDqT21O/ULWsftcWs814QQGSfbJqRCiMwXFxfH5s2bE763JoVeKEmh/d8Sde9L9nyykrzLZlD10WEAXInipesrYfxK7k3Ix/7K3cjdvQNVBzTEOXf2THRu/3OXUxM24PrbL9S8HUB9YpL8PBZ7jvl0wPWTgVTp14ByVlbQ3prPNSFE+klCKoTI1lzyuVJ/bi+Y24t/lh0l9JuFVPxjOQW02wDk0+7R6M/p4D+dh/65OFK8HbEvd6DSh83xrJBf5+gtp8VrnNv4DzcWbiPvjtVUub+HxiRP5M45VSK49dtUndCdFysW0SFSIYSwsOzTjBkzKFWqFC4uLvj6+rJnz56nPn7Xrl34+vri4uJC6dKlmTVrlkXBCiFEepTr6ku949PIE3GDY6PWc8j7daJILCOUmwjqXVtOo1ldKFixIP+4VWfXi59w4PPN3Lr4UMfI00DTuLr/KoHv/syu0r256Viccq9UoNHqD6hxPxCHx5LRO3aF2Ov7AaeXHKNs5J80WPsx+SQZFULoyOwe0hUrVjBo0CBmzJjBSy+9xOzZs2ndujV//fUXPj4+yR5/8eJF2rRpw7vvvsuSJUvYt28f/fr1o1ChQnTs2DFDfgkhhDCHg6sjz3/RDr5oR/j1Bxz/bgva2rVUvbQBd+0BAHZolIs8SbkjJ+HIROLHGPjHqRIhPi9iePEFCrSoRfHWVfHwdMny+DUNbvxxi8u/neJR4GFy/3mQsrf24x1/C+9Ujrnu4MP55ztRuO9rlO9Zm3r2Ug5OCGE9zE5IJ02axDvvvEOfPn0AmDx5Mlu3bmXmzJlMmDAh2eNnzZqFj48PkydPBqBixYoEBQUxceJESUiFELrzKOaO36Q3YNIbxEUZOf3jbu4t2UiRP7ZSNvqvhMfZoVEu5jTlzp+G8wtgKcRj4LJ9SW55lOWBdyUMFcqTq4I3bmWLkqeCFwUreeKSy7LJlcZHRkKO3eDW8Ws8OHON6PNXcbp8jvw3/6JY+BmKaXd4WlXcaJw4XbARD+s0o0jPFjzXsRrFrGxcqBBCmJiVkMbExHD06FGGDRuWZH+LFi3Yv39/isccOHCAFi1aJNnXsmVL5s2bh9FoNGumZEREBC4uWd8bYYuMRiNRUVFERETIbFQzSLuZJyIiIsn32aHNSr5dh5Jv1wHGcPHMba4s2UPs77vw/PcAPpFncPhv/XZFo2DcRQreuwj3AuBk0ueKwUAo+Yiyz02EUx6i7N0xOrjwwJDYO3mobEec4+1wjnlI7ui7uMU9wEWLwINw8gOpjWJ9ch2qCNw4n8eXh1XqkLdDQ8q9WYfy7onvl48iH6WjVfSXHc+1rCLva+aTNrPM43+n5jIrIb19+zZxcXEULlw4yf7ChQsTEhKS4jEhISEpPj42Npbbt2/j5ZV8tY/o6Giio6MTtsPCVMmWEiVKmBOuECILFS9eXO8QrJAG3IW4uxCZ8iNa3dmRQa/1CML2wL49sO9b+CSDntYKybkmhHXTNM3sYywaRPTkyj+apj11NaCUHp/SfpMJEyaQJ0+ehK+UxqYKIYQQQgjrc+fOHbOPMauHtGDBgtjb2yfrDQ0NDU3WC2pSpEiRFB/v4OBAgQIpr/zh7+/P4MGDE7bv379PiRIluHLlCnnyyOoqaREeHo63tzdXr16V5S7NIO1mPmkzy0i7mU/azDLSbuaTNrNMWFgYPj4+5M9vfsk8sxJSJycnfH19CQgI4NVXX03YHxAQQPv27VM8xs/Pjw0bNiTZt23bNmrVqpXquAxnZ2ecnZ2T7c+TJ4+cGGby8PCQNrOAtJv5pM0sI+1mPmkzy0i7mU/azDJ2dubfgDf7iMGDBzN37lzmz5/PmTNn+Oijj7hy5Qp9+/YFVO9mz549Ex7ft29fLl++zODBgzlz5gzz589n3rx5DBkyxOxghRBCCCFE9mN22afOnTtz584dvvzyS4KDg6lSpQqbNm1KmHAUHBzMlStXEh5fqlQpNm3axEcffcT06dMpWrQoU6dOlZJPQgghhBACsHDp0H79+tGvX78Uf7Zw4cJk+xo2bMixY8cseSlA3cL/4osvUryNL1ImbWYZaTfzSZtZRtrNfNJmlpF2M5+0mWXS024GzZK5+UIIIYQQQmQQWTtOCCGEEELoShJSIYQQQgihK0lIhRBCCCGEriQhFUIIIYQQurLZhDQ6OpoaNWpgMBg4ceKE3uFYvVdeeQUfHx9cXFzw8vKiR48e3LhxQ++wrNalS5d45513KFWqFK6urpQpU4YvvviCmJgYvUOzeuPGjaNu3bq4ubmRN29evcOxSjNmzKBUqVK4uLjg6+vLnj179A7Jqu3evZt27dpRtGhRDAYDa9eu1TskqzdhwgReeOEF3N3d8fT0pEOHDpw9e1bvsKzezJkzqVatWkJBfD8/PzZv3qx3WDZlwoQJGAwGBg0aZNZxNpuQDh06lKJFi+odhs1o3LgxK1eu5OzZs6xatYp///2X119/Xe+wrNbff/9NfHw8s2fP5vTp03z//ffMmjWL4cOH6x2a1YuJieGNN97g/fff1zsUq7RixQoGDRrEiBEjOH78OPXr16d169ZJ6jeLpCIiIqhevTo//PCD3qHYjF27dtG/f38OHjxIQEAAsbGxtGjRgoiICL1Ds2rFixfnq6++IigoiKCgIJo0aUL79u05ffq03qHZhCNHjjBnzhyqVatm/sGaDdq0aZNWoUIF7fTp0xqgHT9+XO+QbM66des0g8GgxcTE6B2Kzfjmm2+0UqVK6R2GzViwYIGWJ08evcOwOi+++KLWt2/fJPsqVKigDRs2TKeIbAugrVmzRu8wbE5oaKgGaLt27dI7FJuTL18+be7cuXqHYfUePHigPffcc1pAQIDWsGFD7cMPPzTreJvrIb158ybvvvsuixcvxs3NTe9wbNLdu3f5+eefqVu3Lo6OjnqHYzPCwsLInz+/3mEIGxYTE8PRo0dp0aJFkv0tWrRg//79OkUlcoKwsDAAeQ8zQ1xcHMuXLyciIgI/Pz+9w7F6/fv3p23btjRr1syi420qIdU0jd69e9O3b19q1aqldzg259NPPyVXrlwUKFCAK1eusG7dOr1Dshn//vsv06ZNo2/fvnqHImzY7du3iYuLo3Dhwkn2Fy5cmJCQEJ2iEtmdpmkMHjyYevXqUaVKFb3DsXqnTp0id+7cODs707dvX9asWUOlSpX0DsuqLV++nGPHjjFhwgSLn8MqEtJRo0ZhMBie+hUUFMS0adMIDw/H399f75CtQlrbzeSTTz7h+PHjbNu2DXt7e3r27ImWwxbqMrfNAG7cuEGrVq1444036NOnj06R68uSdhOpMxgMSbY1TUu2T4iMMmDAAE6ePMmyZcv0DsUmlC9fnhMnTnDw4EHef/99evXqxV9//aV3WFbr6tWrfPjhhyxZsgQXFxeLn8cqlg69ffs2t2/ffupjSpYsSZcuXdiwYUOSN+64uDjs7e3p3r07P/30U2aHalXS2m4pnSDXrl3D29ub/fv356hbEea22Y0bN2jcuDG1a9dm4cKF2NlZxTVclrPkXFu4cCGDBg3i/v37mRyd7YiJicHNzY1ffvmFV199NWH/hx9+yIkTJ9i1a5eO0dkGg8HAmjVr6NChg96h2ISBAweydu1adu/eTalSpfQOxyY1a9aMMmXKMHv2bL1DsUpr167l1Vdfxd7ePmFfXFwcBoMBOzs7oqOjk/wsNQ6ZGWRaFSxYkIIFCz7zcVOnTmXs2LEJ2zdu3KBly5asWLGC2rVrZ2aIVimt7ZYS03VIdHR0RoZk9cxps+vXr9O4cWN8fX1ZsGBBjk1GIX3nmkjk5OSEr68vAQEBSRLSgIAA2rdvr2NkIrvRNI2BAweyZs0aAgMDJRlNB03TctxnpTmaNm3KqVOnkux76623qFChAp9++mmaklGwkoQ0rXx8fJJs586dG4AyZcpQvHhxPUKyCYcPH+bw4cPUq1ePfPnyceHCBT7//HPKlCmTo3pHzXHjxg0aNWqEj48PEydO5NatWwk/K1KkiI6RWb8rV65w9+5drly5QlxcXEKd4LJlyyb8zeZkgwcPpkePHtSqVQs/Pz/mzJnDlStXZHzyUzx8+JDz588nbF+8eJETJ06QP3/+ZJ8LQunfvz9Lly5l3bp1uLu7J4xRzpMnD66urjpHZ72GDx9O69at8fb25sGDByxfvpzAwEC2bNmid2hWy93dPdnYZNN8FbPGLGfonP8sdvHiRSn7lAYnT57UGjdurOXPn19zdnbWSpYsqfXt21e7du2a3qFZrQULFmhAil/i6Xr16pViu+3cuVPv0KzG9OnTtRIlSmhOTk7a888/L6V4nmHnzp0pnlO9evXSOzSrldr714IFC/QOzaq9/fbbCX+bhQoV0po2bapt27ZN77BsjiVln6xiDKkQQgghhMi5cu6gOCGEEEIIYRUkIRVCCCGEELqShFQIIYQQQuhKElIhhBBCCKErSUiFEEIIIYSuJCEVQgghhBC6koRUCCGEEELoShJSIYQQQgihK0lIhRBCCCGEriQhFUIIIYQQupKEVAghstD48eMxGAzJviZNmqR3aEIIoRtZy14IIbLQgwcPiIiISNj+8ssv2bRpE3v37qV48eI6RiaEEPpx0DsAIYTISdzd3XF3dwdg9OjRbNq0iV27dkkyKoTI0eSWvRBC6GD06NEsWLCAXbt2UaJECb3DEUIIXUlCKoQQWUySUSGESEoSUiGEyEKSjAohRHIyhlQIIbLI2LFj+eGHH/jtt99wdnYmJCQEgHz58uHs7KxzdEIIoR+ZZS+EEFlA0zTy5s1LeHh4sp8dPHiQ2rVr6xCVEEJYB0lIhRBCCCGErmQMqRBCCCGE0JUkpEIIIYQQQleSkAohhBBCCF1JQiqEEEIIIXQlCakQQgghhNCVJKRCCCGEEEJXkpAKIYQQQghdSUIqhBBCCCF0JQmpEEIIIYTQlSSkQgghhBBCV5KQCiGEEEIIXUlCKoQQQgghdPV/5WIvF6V5bCwAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:35.261938Z",
     "start_time": "2025-09-11T05:51:28.122629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ],
   "id": "e9b5b3d9bcc27c9f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:35.292397Z",
     "start_time": "2025-09-11T05:51:35.276233Z"
    }
   },
   "cell_type": "code",
   "source": "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])",
   "id": "35cac58689fa6c23",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:39.651203Z",
     "start_time": "2025-09-11T05:51:35.312383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ],
   "id": "28041b3d7b7e8525",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.4858 - mae: 0.8357 - val_loss: 0.3479 - val_mae: 0.6527\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2415 - mae: 0.5419 - val_loss: 0.2630 - val_mae: 0.5473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e302191b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 保存和加载包含自定义组件的模型",
   "id": "8ab2c41598f219a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "保存包含自定义损失函数的模型很方便，直接保存；但加载它时，需要将名称映射到对象",
   "id": "613d1dd2337aefe8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:39.889695Z",
     "start_time": "2025-09-11T05:51:39.698545Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"./models/my_model_with_a_custom_loss.keras\")",
   "id": "4bd9ee5a0ba20bb9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:39.979240Z",
     "start_time": "2025-09-11T05:51:39.909438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.load_model(\"./models/my_model_with_a_custom_loss.keras\", custom_objects={\"huber_fn\": huber_fn})\n",
    "\n",
    "# 如果用@keras.utils.register_keras_serializable()修饰 huber_fn()函数，它将自动可供load_model()函数使用，无须将其包含在custom_objects字典中"
   ],
   "id": "688a9de587a11a8f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:42.047488Z",
     "start_time": "2025-09-11T05:51:39.992328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ],
   "id": "cc5732236934871b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2065 - mae: 0.4930 - val_loss: 0.2173 - val_mae: 0.4919\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.1906 - mae: 0.4709 - val_loss: 0.1978 - val_mae: 0.4725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e30556fb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:42.082514Z",
     "start_time": "2025-09-11T05:51:42.068246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 想要不同的误差阈值，创建函数，让该函数创建已配置的损失函数\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ],
   "id": "a1475d5b56c1abe3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:42.120879Z",
     "start_time": "2025-09-11T05:51:42.104560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存模型时，上述方式定义的阈值不会保存\n",
    "# 创建tf.keras.losses.Loss类的子类， 实现get_config() 自定义要保存的配置\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}  # base_config | {\"threshold\":self.threshold}"
   ],
   "id": "46ec52ef43b1da2f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:42.136553Z",
     "start_time": "2025-09-11T05:51:42.122403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# {**{\"one\":1, \"three\":3}, \"two\":2}\n",
    "{\"one\":1, \"three\":3}  | {\"two\": 2}"
   ],
   "id": "3ed3de0ab98bce13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 1, 'three': 3, 'two': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "构造函数接受**kwargs并将它们传递给父类构造函数，该父类构造函数处理标准超参数：损失的名称和用于聚合单个实例损失的归约算法。\n",
    "\n",
    "call()方法获取标签和预测结果，计算所有实例损失，然后将其返回。\n",
    "\n",
    "get_config()方法返回一个字典，将每个超参数名称映射到其值。它首先调用父类的get_config()方法，然后将新的超参数添加到此字典中"
   ],
   "id": "8bd9143d930c21cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:44.265313Z",
     "start_time": "2025-09-11T05:51:42.200572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")  # 编译使用损失类的实例\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ],
   "id": "afb12d32696a0b3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2065 - val_loss: 0.2170\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1989 - val_loss: 0.1984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e304f1a50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:44.336490Z",
     "start_time": "2025-09-11T05:51:44.305099Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"./models/my_model_with_a_custom_class.keras\") # 当保存模型时，Keras会调用损失实例的get_config()方法，并将配置以SavedModel格式保存",
   "id": "17d6685f558cce52",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:46.019078Z",
     "start_time": "2025-09-11T05:51:44.349113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.load_model(\"./models/my_model_with_a_custom_class.keras\", custom_objects={\"HuberLoss\": HuberLoss}) # 加载模型时，它在HuberLoss类上调用from_config()类方法：此方法由基类(Loss)实现，并创建该类的实例，将**config传递给构造函数。\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ],
   "id": "7841a4a56a4e00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1933 - val_loss: 0.1788\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.1900 - val_loss: 0.2234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e330ff4c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:46.068500Z",
     "start_time": "2025-09-11T05:51:46.052706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.loss\n",
    "model.loss.threshold"
   ],
   "id": "7fe14e99675d2bd7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义激活函数，初始化，正则化和约束",
   "id": "cb1425ebf782f60f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "多数Keras功能，例如损失、正则化、约束、初始化、指标、激活函数、层甚至完整模型，都可以以几乎相同的方式进行自定义。在大多数情况下，只需要编写带有适当输入和输出的简单函数即可。以下是自定义激活函数（等同于tf.keras.activations.softplus()或tf.nn.softplus()）、自定义Glorot初始化（等同于tf.keras.initializers.glorot_normal()）、自定义l1正则化（等同于tf.keras.regularizers.l1(0.01)），以及确保权重均为正的自定义约束（相当于tf.keras.constraints.nonneg()或tf.nn.relu()）：",
   "id": "9091c3aeaa8e104"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:46.124619Z",
     "start_time": "2025-09-11T05:51:46.118292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01*weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0, tf.zeros_like(weights), weights)  # tf.nn.relu(weights)\n",
    "\n",
    "# 这些函数接收什么参数，取决于自定义函数的类型"
   ],
   "id": "b5dd35f9938121f9",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:46.170836Z",
     "start_time": "2025-09-11T05:51:46.152696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer = tf.keras.layers.Dense(1,  kernel_initializer=my_glorot_initializer,\n",
    "                              kernel_regularizer=my_l1_regularizer, kernel_constraint=my_positive_weights)  # 只要参数和返回值能对上，可以正常使用这些自定义函数"
   ],
   "id": "399be8e2c5b1236b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 自定义出 he初始化 （normal/uniform)\n",
    "2. 自定义出 l2正则化\n",
    "3. 自定义 swish / relu\n",
    "\n",
    "用自定义这些组件 去构建Dense层， 然后训练一下数据试试"
   ],
   "id": "c4da1ae6c6cdc49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:48.242283Z",
     "start_time": "2025-09-11T05:51:46.172423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_l2_regularizer(weights):\n",
    "    return tf.reduce_sum(0.01*tf.square(weights))\n",
    "\n",
    "def my_relu(z):\n",
    "    return tf.nn.relu(z)\n",
    "\n",
    "def my_he_normal(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / shape[0])\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=my_relu, kernel_initializer=my_he_normal, kernel_regularizer=my_l2_regularizer,input_shape=input_shape),\n",
    "    layer\n",
    "])\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ],
   "id": "e22a3097acc79657",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9486 - mae: 0.8319 - val_loss: 0.6649 - val_mae: 0.6471\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4955 - mae: 0.5545 - val_loss: 0.4211 - val_mae: 0.5295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e341af190>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "激活函数将应用于此密集层的输出，其结果将传递到下一层。层的权重将使用初始化程序返回的值进行初始化。在每个训练步骤中，将权重传递给正则化函数以计算正则化损失，并将其添加到主损失中以得到用于训练的最终损失。最后，在每个训练步骤之后，将调用约束函数，并将层的权重替换为约束权重。\n",
    "\n",
    "如果函数具有需要与模型一起保存的超参数，那么需要继承适当的类，例如tf.keras.regularizers.Regularizer、tf.keras.constraints.Constraint、tf.keras.initializers.Initializer或tf.keras.layers.Layer（适用于任何层，包括激活函数）。就像自定义损失时所做的一样，这是一个用于l1正则化的简单类，它保存了其factor超参数"
   ],
   "id": "59429c06ddbcb6ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:48.294494Z",
     "start_time": "2025-09-11T05:51:48.286623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        # 不需要调用父类构造函数或get_config()方法，因为它们不是由父类定义的\n",
    "        return {\"factor\": self.factor}\n",
    "\n",
    "# 注意：必须为损失，层（包括激活函数）和模型实现call方法，或者为正则化，初始化和约束实现__call__()方法。"
   ],
   "id": "6aa741caf10e9b8a",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义指标",
   "id": "cd4ba4921d26dc8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "损失和指标在概念上不是一回事：损失（例如交叉熵）被梯度下降用来训练模型，因此它们必须是可微的（至少在评估它们的点上），并且梯度除了局部最小值不应为0。另外，即使人类不容易解释它们也没有问题。\n",
    "\n",
    "相反，指标（例如精度）用于评估模型：它们必须更容易被解释，并且可以是不可微的，在各处也可以具有0梯度。\n",
    "\n",
    "在大多数情况下，定义自定义指标函数与定义自定义损失函数完全相同。实际上，甚至可以将之前创建的Huber损失函数用作指标。"
   ],
   "id": "19cbaad5292d44b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.110212Z",
     "start_time": "2025-09-11T05:51:48.345393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ],
   "id": "55f2b7619071a498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.3637 - huber_fn: 0.7583\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.2146 - huber_fn: 0.3773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e343b9f60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "对于训练期间的每一个批次，Keras都会计算该指标并跟踪自轮次0以来的均值，但有的时候不想要这样的效果\n",
    "\n",
    "例如，考虑一个二元分类器的准确率。精确率是真阳性的数量除以阳性预测结果（包括真阳性和假阳性）的数量。假设该模型在第一个批次中预测了5个阳性结果，其中4个是正确的：即80%的精确率。假设该模型在第二个批次中预测了3个阳性结果，但它们都是不正确的：准确率为0%。如果仅计算这两个准确率的均值，则可以得到40%。但是，这并不是模型在这两个批次上的准确率！实际上，在8个(5+3)阳性预测结果中，总共有4个(4+0)真阳性，因此总体准确率为50%，而不是40%。\n",
    "\n",
    "需要的是一个对象，它应该可以跟踪真阳性的数量和假阳性的数量，并可以在请求时根据这些数据计算准确率。这正是tf.keras.metrics.Precision类所做的"
   ],
   "id": "ed4045e986776be4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.286052Z",
     "start_time": "2025-09-11T05:51:50.123698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 流式评价\n",
    "precision = tf.keras.metrics.Precision()  # 创建了一个Precision对象，然后将其用作函数\n",
    "\n",
    "# 将第一个批次的标签和预测结果以及第二个批次的标签和预测结果传递给它\n",
    "precision([0,1,1,1,0,1,0,1], [1,1,0,1,0,1,0,1])\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
    "\n",
    "#\n",
    "#  调用result()方法来获取指标的当前值。使用variables属性查看其变量（跟踪真阳性和假阳性的数量），并使用reset_states()方法重置这些变量：\n",
    "precision.result()\n",
    "precision.variables\n",
    "precision.reset_state()"
   ],
   "id": "4ac235b23470e028",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.320542Z",
     "start_time": "2025-09-11T05:51:50.296670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "precision.result()\n",
    "precision.variables"
   ],
   "id": "51b495b804227e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.368376Z",
     "start_time": "2025-09-11T05:51:50.352454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 自定义流式指标，创建tf.keras.metrics.Metric类的子类\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "\n",
    "\n",
    "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\") # tf.Variable(0, dtype=tf.float32)\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        sample_metrics = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(sample_metrics))  # self.total = self.total + tf.reduce_sum(sample_metrics)\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32)) # self.count = self.count + len(y_true)\n",
    "\n",
    "    def result(self):\n",
    "        # return tf.divide(self.total, self.count)\n",
    "        return self.total / self.count\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n"
   ],
   "id": "cf97c73263ec4eb3",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.384496Z",
     "start_time": "2025-09-11T05:51:50.372385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyPrecision(tf.keras.metrics.Metric):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.true_positives = self.add_weight(name=\"true_positives\", initializer=\"zeros\")\n",
    "        self.false_positives = self.add_weight(name=\"false_positives\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        total_pred_positives = tf.cast(tf.reduce_sum(y_pred), tf.float32)\n",
    "        true_positives = tf.reduce_sum(tf.multiply(tf.cast(y_true, tf.float32), tf.cast(y_pred, tf.float32)))\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.false_positives.assign_add(total_pred_positives - true_positives)\n",
    "\n",
    "    def result(self):\n",
    "        if tf.equal(self.true_positives + self.false_positives, 0):\n",
    "            return self.true_positives\n",
    "        return self.true_positives / (self.true_positives + self.false_positives)"
   ],
   "id": "bbf7dacf70ad94a6",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.565697Z",
     "start_time": "2025-09-11T05:51:50.392504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = MyPrecision()\n",
    "\n",
    "m(tf.constant([0,1,1,1,0,1,0,1]), tf.constant([1,1,0,1,0,1,0,1]))\n",
    "m([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ],
   "id": "ab34cda83aaa5d8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 构造函数使用add_weight()方法创建用于跟踪多个批次的指标状态所需的变量，在本例中，这些变量包括所有Huber损失的总和(total)以及到目前为止看到的实例数(count)。\n",
    "- 也可以手动创建变量。Keras会跟踪任何设置为属性的tf.Variable（一般而言，指任何“可跟踪”的对象，例如层或模型）。\n",
    "- 当使用此类的实例作为函数时（像对Precision对象所做的那样），将调用update_state()方法。给定一个批次的标签和预测结果，它会更新变量。\n",
    "- result()方法计算并返回最终结果，在本例中为所有实例的平均Huber损失。当使用指标作为函数时，首先调用update_state()方法，然后调用result()方法，并返回其输出。\n",
    "- 还实现了get_config()方法来确保阈值与模型一起被保存。\n",
    "- reset_states()方法的默认实现将所有变量重置为0.0（但是可以根据需要覆盖它）。\n",
    "\n",
    "当使用简单的函数定义指标时，Keras会自动为每个批次调用该指标，它会跟踪每个轮次的均值，因此HuberMetric类的唯一好处是保存阈值。\n",
    "\n",
    "但是，某些指标（如精确率）不能简单地按批次平均：在这些情况下，除了实现流式指标之外，别无选择。"
   ],
   "id": "60384c15dfd26920"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Keras的可跟踪机制：keras会自动收集自定义类（比如层/模型）的可跟踪对象（比如变量），放进父类的 变量列表（layer.trainable_variables / layer.non_trainable_variables）。\n",
    "\n",
    "这样的作用是：\n",
    "\n",
    "1. 参数能被自动训练\n",
    "\n",
    "* 当你调用 `model.compile(..., optimizer=...)` 并训练时，优化器会去找 `model.trainable_variables`。\n",
    "* 因为 Keras 已经把子层的变量挂到父层上，所以优化器也能“看到”子层的权重并更新它们。\n",
    "  否则，子层的参数就不会被训练。\n",
    "\n",
    "2. 保存 / 加载模型时参数不会丢\n",
    "\n",
    "* 当调用 `model.save()` 或 `model.get_weights()` 时，Keras 会把所有层（包括嵌套子层）的参数序列化。  序列化：内存的对象 转成 可被被保存到磁盘/数据库/网络传输的二进制形式\n",
    "* 这依赖于刚才那个“变量列表”，否则模型恢复时，子层权重可能不在里面。\n",
    "\n",
    "3. 方便层级管理\n",
    "\n",
    "* 可以直接访问父层的 `.variables` 或 `.summary()`，会自动列出子层的参数。\n",
    "* 这对调试和可视化很方便。\n",
    "\n",
    "**总结**：\n",
    "“添加到该层的变量列表里”就是保证 优化器能更新它们，模型能保存/恢复它们，用户能统一管理它们 —— 这是 Keras 的“可跟踪机制”。\n"
   ],
   "id": "6b9320ed4940b693"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.677307Z",
     "start_time": "2025-09-11T05:51:50.572539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = HuberMetric(2.)\n",
    "#  squared_loss = tf.square(error) / 2\n",
    "#  linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "\n",
    "# total = 2 * |10 - 2| - 2²/2 = 14\n",
    "# count = 1\n",
    "# result = 14 / 1 = 14\n",
    "m(tf.constant([[2.]]), tf.constant([[10.]]))"
   ],
   "id": "ca6d4c0979d0a1fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=14.0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.724145Z",
     "start_time": "2025-09-11T05:51:50.702493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# total = total + (|1 - 0|² / 2) + (2 * |9.25 - 5| - 2² / 2) = 14 + 7 = 21\n",
    "# count = count + 2 = 3\n",
    "# result = total / count = 21 / 3 = 7\n",
    "m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))"
   ],
   "id": "4d59d0f87c3f6aa5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.763587Z",
     "start_time": "2025-09-11T05:51:50.755767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m.result()\n",
    "m.variables\n",
    "#\n",
    "m.reset_state()\n",
    "m.variables"
   ],
   "id": "56fbb9a69adcce75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:50.852790Z",
     "start_time": "2025-09-11T05:51:50.812272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查自定义指标正常运行\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ],
   "id": "17d3570f9c2198aa",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:52.712223Z",
     "start_time": "2025-09-11T05:51:50.872197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\",\n",
    "              metrics=[HuberMetric(2.0)])\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ],
   "id": "5ee71b69f34319cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0514 - huber_metric_1: 1.0514\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3152 - huber_metric_1: 0.3152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e354a1930>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:52.815415Z",
     "start_time": "2025-09-11T05:51:52.736562Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"./models/my_model_with_a_custom_metric.keras\")",
   "id": "870293e58af91036",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:52.893666Z",
     "start_time": "2025-09-11T05:51:52.837437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    \"./models/my_model_with_a_custom_metric.keras\",\n",
    "    custom_objects={\n",
    "        \"huber_fn\": create_huber(2.0),\n",
    "        \"HuberMetric\": HuberMetric\n",
    "    }\n",
    ")"
   ],
   "id": "14c231375ead1978",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:54.316422Z",
     "start_time": "2025-09-11T05:51:52.972450Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(X_train_scaled, y_train, epochs=2)",
   "id": "f26df72d1a08da8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2527 - huber_metric_1: 0.2527\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2270 - huber_metric_1: 0.2270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e304dd810>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:54.348495Z",
     "start_time": "2025-09-11T05:51:54.324637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 也可以这样自定义指标，这个类对形状处理更好，支持样本权重\n",
    "class HuberMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ],
   "id": "7ce62bf023b7d753",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:54.411711Z",
     "start_time": "2025-09-11T05:51:54.372414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ],
   "id": "35adf382e79fb823",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:56.413362Z",
     "start_time": "2025-09-11T05:51:54.475378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss=tf.keras.losses.Huber(2.0), optimizer=\"nadam\",\n",
    "              weighted_metrics=[HuberMetric(2.0)])\n",
    "\n",
    "sample_weight = np.random.rand(len(y_train))\n",
    "print(sample_weight)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2,\n",
    "                    sample_weight=sample_weight)"
   ],
   "id": "b647134ba9a167a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37454012 0.95071431 0.73199394 ... 0.16710009 0.0333932  0.38571671]\n",
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2695 - HuberMetric: 0.5431\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1474 - HuberMetric: 0.2970\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:56.452382Z",
     "start_time": "2025-09-11T05:51:56.420673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(history.history[\"loss\"][0],\n",
    " history.history[\"HuberMetric\"][0] * sample_weight.mean())  # todo: 为什么这两个值接近\n",
    "\n",
    "model.save(\"./models/my_model_with_a_custom_metric_v2.keras\")"
   ],
   "id": "3945fd2b3ade5b59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26950663328170776 0.2695066773642387\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:56.546449Z",
     "start_time": "2025-09-11T05:51:56.492335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.load_model(\"./models/my_model_with_a_custom_metric_v2.keras\",\n",
    "                                   custom_objects={\"HuberMetric\": HuberMetric})"
   ],
   "id": "44d0517c7dbdbc80",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:57.964990Z",
     "start_time": "2025-09-11T05:51:56.553730Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(X_train_scaled, y_train, epochs=2)",
   "id": "9f792677a7b0334c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2418 - HuberMetric: 0.2418\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2143 - HuberMetric: 0.2143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e388d7100>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义层",
   "id": "fd31f23c07e8c762"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "想构建一个架构，其中包含TensorFlow未提供默认实现的奇异层。或者可能只是想构建一个具有重复结构的架构（其中特定的层块重复多次），将每个块视为一个层会很方便。\n",
    "\n",
    "对于这种情况需要构建一个自定义层。\n",
    "\n",
    "有些层没有权重，例如tf.keras.layers.Flatten或tf.keras.layers.ReLU。如果想创建一个没有任何权重的自定义层，最简单的方法是编写一个函数并将其包装在tf.keras.layers.Lambda层中。例如，以下的层将对其输入应用指数函数："
   ],
   "id": "7fda343742f4be03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:57.996109Z",
     "start_time": "2025-09-11T05:51:57.987602Z"
    }
   },
   "cell_type": "code",
   "source": "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x)) # 可以通过顺序API，函数式API或子类化API 像其他层一样使用此自定义层",
   "id": "6d7b2e35d659180c",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:51:58.072162Z",
     "start_time": "2025-09-11T05:51:58.024714Z"
    }
   },
   "cell_type": "code",
   "source": "exponential_layer(tf.constant([-1., 0., 1.])) #  # 像其他层对象那样，可以当成函数使用",
   "id": "b4fce2fe19ff5069",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:01.300232Z",
     "start_time": "2025-09-11T05:51:58.090368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 如果要预测的值为正数且尺度差异很大（例如 0.001、10、10000），则在回归模型的输出处添加指数层会很有用。\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    exponential_layer\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# 事实上，指数函数是Keras中的标准激活函数之一，可以用activation=\"exponential\"\n"
   ],
   "id": "e5898ee8d42a1707",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5863 - val_loss: 0.3901\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7664 - val_loss: 0.3872\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4246 - val_loss: 0.3581\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6361 - val_loss: 0.4369\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4408 - val_loss: 0.3836\n",
      "162/162 [==============================] - 0s 971us/step - loss: 0.4030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4030041992664337"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:01.332270Z",
     "start_time": "2025-09-11T05:52:01.319283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 自定义有状态的层（具有权重的层），需要创建tf.keras.layers.Layer类的子类\n",
    "\n",
    "# Dense层的简化自定义实现  # 形状 输出的长度，激活函数， W， b\n",
    "\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units  # units 输出的数量 （这个时候还不知道输入有几个神经元）\n",
    "        self.activation = tf.keras.activations.get(activation)  # 字符串 获取到 字符串对应的激活函数\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\"\n",
    "        )\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        # 执行前向传播\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units, \"activation\": tf.keras.activations.serialize(self.activation)}"
   ],
   "id": "d259d87ea5239f19",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 构造函数将所有超参数用作参数（在此示例中，指输入的 activation），重要的是，它还接受 **kwargs 参数。它调用父类构造函数，并将其传递给 kwargs。这负责处理标准参数，例如 input_shape、trainable 和 name，然后，它将超参数保存为属性，使得 tf.keras.activations.get() 函数将 activation 参数转换为适当的激活函数（它接受函数、标准符号，如 \"relu\" 或 \"swish\"，或者 None）。\n",
    "\n",
    "- build() 方法的作用是通过为每个权重调用 add_weight() 方法来创建层的变量。首次使用该层时，将调用 build() 方法。在这一点上，Keras 知道该层的输入形状，并将其传给 build() 方法，这对于创建某些层而言通常是必需的。例如，我们需要知道上一层中神经元的数量，以便创建连接权重矩阵（即 \"kernel\"）；这对应于输入的最后一个维度的大小。在 build() 方法调用的最后（并且仅在最后），必须调用父类的 build() 方法：这告诉 Keras 这一层已被构建（它设置了 self.built=True）。\n",
    "\n",
    "- call() 方法执行前向传播。在本示例中，我们计算输入 x 与层的核的矩阵积，加上偏置向量，并对结果应用激活函数，从而获得层的输出。\n",
    "\n",
    "- get_config() 方法就像在以前的自定义类中一样。请注意，通过调用 tf.keras.activations.serialize() 保存激活函数的完整配置。\n",
    "\n",
    "\n",
    "现在，可以像使用其他层一样使用 MyDense 层、\n"
   ],
   "id": "7f74ff6258e9cdd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:03.576410Z",
     "start_time": "2025-09-11T05:52:01.347390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "model.save(\"./models/my_model_with_a_custom_layer.keras\")"
   ],
   "id": "65e181d38fb80d77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0923 - val_loss: 0.6167\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5295 - val_loss: 0.5367\n",
      "162/162 [==============================] - 0s 930us/step - loss: 0.4655\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:05.162589Z",
     "start_time": "2025-09-11T05:52:03.592216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.load_model(\"./models/my_model_with_a_custom_layer.keras\",\n",
    "                                   custom_objects={\"MyDense\": MyDense})\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ],
   "id": "80d9b311ecdcb995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4440 - val_loss: 0.5043\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4080 - val_loss: 0.5742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18e39acf5e0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:05.210358Z",
     "start_time": "2025-09-11T05:52:05.194516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 要创建具有多个输入的层（例如Concatenate），call()方法的参数应该是一个包含所有输入的元组。\n",
    "# 要创建有多个输出的层，call()方法应该返回输出列表\n",
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        # 两个输入，三个输出\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape ,\" X2.shape: \", X2.shape)  # 额外代码\n",
    "        return X1 + X2, X1 * X2, X1 / X2"
   ],
   "id": "5cae79895f4993a",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:05.305889Z",
     "start_time": "2025-09-11T05:52:05.235328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这种层不能用不能使用Sequential API（仅接受具有一共输入和一个输出的层），但可以用函数式API和子类化API\n",
    "# 用 符号化的输入来测试\n",
    "inputs1 = tf.keras.layers.Input(shape=[2])\n",
    "inputs2 = tf.keras.layers.Input(shape=[2])\n",
    "MyMultiLayer()((inputs1, inputs2))"
   ],
   "id": "b1ddfb6f980edc08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (None, 2)  X2.shape:  (None, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer')>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:05.368985Z",
     "start_time": "2025-09-11T05:52:05.342581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 具体数值测试\n",
    "X1, X2 = np.array([[3., 6.], [2., 7.]]), np.array([[6., 12.], [4., 3.]])\n",
    "MyMultiLayer()((X1, X2))"
   ],
   "id": "6b87b790be7b4e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (2, 2)  X2.shape:  (2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[ 9., 18.],\n",
       "        [ 6., 10.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[18., 72.],\n",
       "        [ 8., 21.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.5      , 0.5      ],\n",
       "        [0.5      , 2.3333333]], dtype=float32)>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:05.454560Z",
     "start_time": "2025-09-11T05:52:05.442657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomConcat(tf.keras.layers.Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, X):\n",
    "        return tf.concat(X, axis=self.axis)"
   ],
   "id": "651662615d9d78fd",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:08.499503Z",
     "start_time": "2025-09-11T05:52:05.524456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# X1, X2 = np.array([[3., 6.], [2., 7.]]), np.array([[6., 12.], [4., 3.]])\n",
    "# CustomConcat()((X1, X2))\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=input_shape)\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = CustomConcat()((input_, hidden2))\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_], outputs=[output])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=3,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ],
   "id": "4ff5fff91169c44c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3812 - val_loss: 0.5179\n",
      "Epoch 2/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4734 - val_loss: 0.4650\n",
      "Epoch 3/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4004 - val_loss: 0.5310\n",
      "162/162 [==============================] - 0s 950us/step - loss: 0.3798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3797661066055298"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "如果层在训练和测试期间需要不同的行为（例如如果使用Dropout 或 BatchNormalization层），则必须将training参数添加到call()方法并使用此参数，来决定 训练 和 不训练分别做什么\n",
    "\n",
    "创建一个在训练期间添加高斯噪声（用于正则化）但在测试期间不执行任何操作的层（Keras具有相同功能的层是tf.keras.layers.GaussianNoise）："
   ],
   "id": "998562c689b19d13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:08.532376Z",
     "start_time": "2025-09-11T05:52:08.515395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X"
   ],
   "id": "44d8ff580211148a",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:11.253410Z",
     "start_time": "2025-09-11T05:52:08.562463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    MyGaussianNoise(stddev=1.0, input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=3,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ],
   "id": "cf0ef11f19810f06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3.8037 - val_loss: 19.2246\n",
      "Epoch 2/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3423 - val_loss: 11.5664\n",
      "Epoch 3/3\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.1923 - val_loss: 5.9433\n",
      "162/162 [==============================] - 0s 933us/step - loss: 1.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0018396377563477"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:11.518293Z",
     "start_time": "2025-09-11T05:52:11.285274Z"
    }
   },
   "cell_type": "code",
   "source": "model.evaluate(X_test_scaled, y_test)",
   "id": "745a0834c7a51e90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 953us/step - loss: 1.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0018396377563477"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义模型",
   "id": "3a7dae6aaae6f817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "之前讨论过子类化API，直接继承tf.keras.Model类，在构造函数中创建层和变量，并实现call()方法来自定义模型执行的操作，现在构建如下模型：\n",
    "\n",
    "![自定义ResidualBlock层的模型](./images/tensorflow/p3.png)\n",
    "\n",
    "输入经过第一个密集层，之后经过由两个密集层和加法运算（残差块将其输入与输出相加）组成的残差块（ResidualBlock层），然后经过相同的残差块3次或者更多次，再然后通过第二个残差块，最终结果通过密集输出层。\n",
    "\n",
    "该模型没有多大意义：这只是一个示例，它说明可以轻松构建所需的任何模型，即使是包含循环和跳过连接的模型\n",
    "\n",
    "注意：循环 1 个 ResidualBlock 3 次，就是把同一个 ResidualBlock 层在前向传播时重复调用 3 次。等价于用 相同的一组参数（权重共享） 对输入数据依次做 3 次残差映射。参数只有 1 个残差块的参数；放置了 3 个独立的 ResidualBlock 层，它们各自有 不同的参数。"
   ],
   "id": "400bea6ff88b98a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:11.589025Z",
     "start_time": "2025-09-11T05:52:11.573471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 要实现此模型，首先创建一个ResidualBlock层\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "\n",
    "# Keras会自动检测到hidden属性，该属性包含可跟踪对象（在这个示例中是层），因此它们的变量会自动添加到该层的变量列表中"
   ],
   "id": "6811ea1386eba6df",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:11.609479Z",
     "start_time": "2025-09-11T05:52:11.600926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ],
   "id": "765f840d24ca3be9",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "构造函数中创建层，并在call()方法中使用它们。然后，可以像使用其他模型一样使用此模型（对其进行编译、拟合、评估并使用它进行预测）\n",
    "\n",
    "如果还希望能够使用save()方法保存模型并使用tf.keras.models.load_model()函数加载模型，则必须在ResidualBlock类和ResidualRegressor类中都实现get_config()方法。\n",
    "\n",
    "另外，可以使用save_weights()和load_weights()方法保存并加载权重。Model类是Layer类的子类，因此可以像定义和使用层一样定义并使用模型。\n",
    "\n",
    "但是模型具有一些额外的功能，包括compile()、fit()、evaluate()和predict()方法（以及一些变体）以及get_layer()方法（可以按名称或按索引返回模型的任何层）和save()方法（支持tf.keras.models.load_model()和tf.keras.models.clone_model()）。\n",
    "\n",
    "既然模型提供的功能远不止层，为什么不将每个层都定义为模型？从技术上讲，这是可以的，但是通常可以轻松地将模型的内部组件（即层或可重复使用的层块）与模型本身（即要训练的对象）区分开来。前者应继承Layer类，而后者应继承Model类。"
   ],
   "id": "6cae5828fa1f71f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:15.015289Z",
     "start_time": "2025-09-11T05:52:11.627550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "score = model.evaluate(X_test_scaled, y_test)"
   ],
   "id": "8d04dc4ec4e12a37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 3.8465\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0294\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.5711\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:15.047271Z",
     "start_time": "2025-09-11T05:52:15.022295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.summary()\n",
    "\n",
    "# tf.keras.utils.plot_model(model) 画不出 Model子类的结构，因为是靠代码决定的模型结构"
   ],
   "id": "1f2854af57abeea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"residual_regressor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            multiple                  270       \n",
      "                                                                 \n",
      " residual_block (ResidualBl  multiple                  1860      \n",
      " ock)                                                            \n",
      "                                                                 \n",
      " residual_block_1 (Residual  multiple                  1860      \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " dense_22 (Dense)            multiple                  31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4021 (15.71 KB)\n",
      "Trainable params: 4021 (15.71 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:15.171194Z",
     "start_time": "2025-09-11T05:52:15.122873Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"./models/my_custom_model.keras\")",
   "id": "3a6c7ebd1432735e",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:17.750802Z",
     "start_time": "2025-09-11T05:52:15.193409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    \"./models/my_custom_model.keras\",\n",
    "    custom_objects={\"ResidualRegressor\": ResidualRegressor}\n",
    ")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "model.predict(X_test_scaled[:3])"
   ],
   "id": "78f8fb14a685c76d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.7947\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.1504\n",
      "1/1 [==============================] - 0s 170ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8245145],\n",
       "       [1.5282155],\n",
       "       [3.0846748]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:17.815322Z",
     "start_time": "2025-09-11T05:52:17.783266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这个模型也可以用顺序API实现\n",
    "block1 = ResidualBlock(2, 30)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ],
   "id": "b308e640b9298e5b",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:20.805543Z",
     "start_time": "2025-09-11T05:52:17.822396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)"
   ],
   "id": "eec22b0a0e4c3fb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 1ms/step - loss: 1.0279\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6225\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:20.852178Z",
     "start_time": "2025-09-11T05:52:20.820238Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "171b3ea9dc65d7aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_31 (Dense)            (None, 30)                270       \n",
      "                                                                 \n",
      " residual_block_4 (Residual  (None, 30)                1860      \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_5 (Residual  (None, 30)                1860      \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4021 (15.71 KB)\n",
      "Trainable params: 4021 (15.71 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基于模型内部数据的损失和指标",
   "id": "6d2582b66d0a8e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可以在模型里面自己定义一些额外的损失函数，而不仅仅是依赖训练目标（比如预测的值和真实值的误差）。Keras 提供了 `add_loss()`，让你把“自定义损失”加进去。\n",
    "\n",
    "例子：\n",
    "\n",
    "* 搭了一个多层感知机（MLP），有 5 层隐藏层和 1 个输出层。\n",
    "* 在比较高的一层隐藏层上，你再接一个“辅助输出”，这个输出尝试把输入数据**重建**回来。\n",
    "* 然后计算“输入”和“重建结果”的均方误差（MSE）。这个就是所谓的“重建损失”。\n",
    "\n",
    "这样一来，训练时的总损失 = 主任务的损失（比如回归任务的 MSE） + 重建损失。\n",
    "\n",
    "这么做的意义：\n",
    "\n",
    "* 让模型在学习预测任务的同时，还要“保留”输入的信息。\n",
    "* 虽然这些信息可能和预测目标没直接关系，但保留下来往往能提升模型的泛化能力（算是一种正则化方式）。\n",
    "\n",
    "另外，Keras 还提供了 `add_metric()` 方法，可以顺便把自己定义的指标（比如重建误差）加进来，这样在训练日志里就能直接看到。\n",
    "\n",
    "总结：**就是在训练时，强行让模型一边做主任务，一边顺手做一个“输入重建”副任务，把它的损失加到总损失里，以此帮助模型学得更稳健。**"
   ],
   "id": "36714a97ec4aecdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:52:24.045465Z",
     "start_time": "2025-09-11T05:52:24.032374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(name=\"reconstruction_error\")   # 评价指标\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        # build会告诉模型，输入的形状是什么\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)    # 隐藏层 之后 跟一个重建层，重建层神经元数量 肯定要和 模型的 输入数量的一致的\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)  # 训练的时候 额外加一个指标，这里的指标就是重建损失的平均值\n",
    "\n",
    "        return self.out(Z)"
   ],
   "id": "617e9d791b16e890",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 构造函数创建具有 5 个密集隐藏层和一个密集输出层的 DNN。还创建了一个 `Mean` 指标来跟踪训练期间的重建误差。\n",
    "\n",
    "- `build()` 方法创建一个额外的密集层，该层用于重建模型的输入。必须在此处创建它，因为它的单元数必须等于输入数，并且在调用 `build()` 方法之前，此数是未知的。\n",
    "\n",
    "- `call()` 方法处理所有 5 个隐藏层的输入，然后将结果传递到重建层，从而产生重建结果。\n",
    "\n",
    "- 然后 `call()` 方法计算重建损失（重建结果与输入之间的均方差），并使用 `add_loss()` 方法将其添加到模型的损失列表中。\n",
    "  注意：通过将其乘以 0.05（这是可以调整的超参数）来比例缩小了重建损失。这可以确保重建损失不会在主损失中占大部分。\n",
    "\n",
    "- 接下来，仅在训练期间，`call()` 方法更新重建指标并将其添加到模型中以便显示。这个代码示例实际上可以通过调用 `self.add_metric(recon_loss)` 来简化：Keras 将自动跟踪均值。\n",
    "\n",
    "- add_loss(recon_loss) → 把这个误差加到总损失里，影响反向传播。\n",
    "- add_metric(recon_loss) → 只是把这个误差记录下来，方便在训练日志里看到它的变化，但不影响训练。"
   ],
   "id": "7daa13d65c1b3bc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:53:03.684291Z",
     "start_time": "2025-09-11T05:52:57.432013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "y_pred = model.predict(X_test_scaled)"
   ],
   "id": "d15cb43c16acbd57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 3s 2ms/step - loss: 0.9542 - reconstruction_error: 1.0593\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4962 - reconstruction_error: 0.5945\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4461 - reconstruction_error: 0.4918\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3862 - reconstruction_error: 0.4049\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3757 - reconstruction_error: 0.3713\n",
      "162/162 [==============================] - 0s 987us/step\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用自动微分计算梯度",
   "id": "c803771c1c2f625e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "自定义训练循环本身之前，了解一下TensorFlow中自动计算梯度",
   "id": "8021c6f4f5a8f2ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:05.072363Z",
     "start_time": "2025-09-11T05:57:05.056700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(w1, w2):\n",
    "    return 3*w1**2 + 2*w1*w2"
   ],
   "id": "3574f7d34a8aabf",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果了解微积分，则可以通过分析发现该函数关于w1的偏导数为6*w1+2*w2，关于w2的偏导数是2*w1。例如，在点(w1，w2)=(5，3)处，这些偏导数分别等于36和10，因此此点的梯度向量为(36，10)。但是，如果这是一个神经网络，则该函数将更加复杂，通常具有数以万计的参数，并且通过手动分析找到偏导数几乎是不可能完成的任务。一种解决方案可能是，通过在调整相应参数时测量函数输出的变化来计算每个偏导数的近似值：",
   "id": "e70ebbb0e7638745"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps\n",
    "\n",
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ],
   "id": "2e1c264f66c13bd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " 这只是一个近似值，更重要的是，每个参数至少要调用一次f()（不是两次，因为只计算一次f(w1，w2)）。每个参数至少需要调用f()一次，这种方法对于大型神经网络来说很棘手。因此，应该使用反向模式自动微分, 上面函数可以构造出如下计算图：\n",
    "\n",
    "![函数f的计算图](./images/tensorflow/p4.png)\n",
    "\n",
    "按 **反向自动微分（reverse-mode AD** 写出这张图的求导公式 (可以不理解细节，重点是感受有了计算图可以用程序化 + 高效运行的方式完成任何函数的导数计算过程）。记号\n",
    "$\\bar{x}\\equiv \\partial f/\\partial x$（adjoint）。初始化 $\\bar f=1$，其余为 0，并按计算图反向拓扑次序累计 “+=”。\n",
    "\n",
    "**前向定义**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_1 &= w_1\\cdot w_1 \\\\\n",
    "v_2 &= w_1\\cdot w_2 \\\\\n",
    "v_3 &= 3\\cdot v_1 \\\\\n",
    "v_4 &= 2\\cdot v_2 \\\\\n",
    "f   &= v_3+v_4\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**局部梯度规则**\n",
    "\n",
    "* 加法 $z=x+y$: $\\bar x += \\bar z,\\ \\bar y += \\bar z$\n",
    "* 标量乘 $z=cx$: $\\bar x += c\\,\\bar z$\n",
    "* 乘法 $z=xy$: $\\bar x += y\\,\\bar z,\\ \\bar y += x\\,\\bar z$\n",
    "* 平方 $z=x^2$（也可看作乘法的特例）: $\\bar x += 2x\\,\\bar z$\n",
    "\n",
    "**反向传播**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\bar v_3 += \\bar f \\qquad\\qquad\\ \\ (\\text{from } f=v_3+v_4)\\\\\n",
    "&\\bar v_4 += \\bar f \\\\[2mm]\n",
    "&\\bar v_1 += 3\\,\\bar v_3 \\qquad\\ (\\text{from } v_3=3v_1)\\\\\n",
    "&\\bar v_2 += 2\\,\\bar v_4 \\qquad\\ (\\text{from } v_4=2v_2)\\\\[2mm]\n",
    "&\\bar w_1 += 2w_1\\,\\bar v_1 \\qquad(\\text{from } v_1=w_1^2)\\\\\n",
    "&\\bar w_1 += w_2\\,\\bar v_2 \\qquad\\ (\\text{from } v_2=w_1w_2)\\\\\n",
    "&\\bar w_2 += w_1\\,\\bar v_2 \\qquad\\ (\\text{from } v_2=w_1w_2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "代入 $\\bar f=1$ 可得最终梯度：\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial f}{\\partial w_1}=6w_1+2w_2},\\qquad\n",
    "\\boxed{\\frac{\\partial f}{\\partial w_2}=2w_1}.\n",
    "$$\n",
    "\n",
    "（若也对常数求梯度：$\\bar 3 = v_1,\\ \\bar 2 = v_2$，通常优化时不需要。）\n"
   ],
   "id": "ab4cdbe4cc49e0f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T06:24:02.417515Z",
     "start_time": "2025-09-11T06:24:02.401410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tensorflow可以调用以下代码，可以直接使用反向模式自动微分\n",
    "w1, w2= tf.Variable(5.), tf.Variable(3.)\n",
    "# def f(w1, w2):\n",
    "#    return 3*w1**2 + 2*w1*w2\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # GradientTape 会开始“录制”接下来的运算轨迹：\n",
    "    # 哪些张量被用来计算\n",
    "    # 做了哪些操作（加减乘除、平方，根号，矩阵乘法、激活函数……）\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2]) # 反向自动微分得到梯度\n",
    "gradients"
   ],
   "id": "549b1d1470783039",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T06:57:07.717515Z",
     "start_time": "2025-09-11T06:57:07.685798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# f关于w1的导数 = 1/w1 + w2\n",
    "# f关于w2的导数 = w1-cos(w2)\n",
    "\n",
    "w1, w2 = tf.Variable(2.), tf.Variable(5.)\n",
    "1/w1 + w2, w1 - tf.math.cos(w2)\n",
    "\n",
    "with tf.GradientTape() as  tape:\n",
    "    z = tf.math.log(w1) + w1 * w2 - tf.math.sin(w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ],
   "id": "607ef7b9652acf6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=5.5>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.7163378>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "tape.gradient(...)不仅结果是正确的（准确率仅受浮点误差限制，符合数学公式）， 而且无论输入有多少变量，只要输出只有一个，这个方法只需要反向遍历一次，就能计算出所有梯度，非常高效\n",
    "\n",
    "- with ... as ...怎么实现的\n",
    "with ... as ... 是 上下文管理器语法糖。\n",
    "它让你把“获取资源 → 使用 → 无论是否出错都正确释放”这件事写得简洁安全。\n",
    "\n",
    "```python\n",
    "# with EXPR as VAR:\n",
    "#     BODY\n",
    "\n",
    "mgr = EXPR                         # 生成上下文管理器对象\n",
    "value = mgr.__enter__()            # 进入上下文\n",
    "try:\n",
    "    VAR = value\n",
    "    BODY                           # 这里可能抛异常\n",
    "except BaseException as e:\n",
    "    # __exit__ 返回 True 表示把异常吞掉；False/None 表示继续抛出\n",
    "    if not mgr.__exit__(type(e), e, e.__traceback__):\n",
    "        raise\n",
    "else:\n",
    "    mgr.__exit__(None, None, None) # 正常结束\n",
    "```\n",
    "\n",
    "把 with 块看作一个受控的环境：进入时建立不变式（打开文件、加锁、开始记录…），离开时无条件恢复（关闭、解锁、停止记录…）。"
   ],
   "id": "b07df9bc8455432"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:29:45.996707Z",
     "start_time": "2025-09-11T07:29:42.961839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) 内置例子：文件\n",
    "# with open(\"data.txt\", \"w\") as f:\n",
    "#     f.write(\"hello\")   # 出错也会自动关闭文件\n",
    "\n",
    "# 2) TensorFlow：GradientTape（开始/停止“记录”）\n",
    "# import tensorflow as tf\n",
    "# x = tf.Variable(2.0)\n",
    "# with tf.GradientTape() as tape:   # __enter__ 返回 tape\n",
    "#     y = 3 * x**2\n",
    "# dy_dx = tape.gradient(y, x)\n",
    "\n",
    "import time\n",
    "\n",
    "class MyTimer:\n",
    "    def __enter__(self):\n",
    "        self.t0 = time.perf_counter()  # 记录一下当前时间\n",
    "        return self               # as t -> t 就是这个对象\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        self.dt = time.perf_counter() - self.t0\n",
    "        print(f\"耗时 {self.dt:.3f}s\")\n",
    "        return False              # 不吞异常\n",
    "\n",
    "\n",
    "with MyTimer() as t:  # t = MyTimer().__enter__()\n",
    "    # 做些事...\n",
    "    time.sleep(3)"
   ],
   "id": "59db41a091daa175",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时 3.015s\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了节省内存，tf.GradientTape() 里面只放那些计算损失并对可训练参数求梯度所必需的运算，其它不需要反向微分的东西（指标统计、打印、argmax、数据记录等）要么放到 with 外面，要么在 with 里面用 tape.stop_recording() 暂停录制。",
   "id": "6d32c04770b5755c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:32:14.887340Z",
     "start_time": "2025-09-11T07:32:14.866283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 调用tape的gradient()方法后，tape会立即被自动擦除，因此如果尝试两次调用gradient，会报错\n",
    "w1, w2= tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw1\n",
    "# dz_dw2 = tape.gradient(z, w2) # 报错"
   ],
   "id": "d57dfc4276ce90dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=36.0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:33:42.005076Z",
     "start_time": "2025-09-11T07:33:41.984398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 如果需要多次调用gradient()，则必须使该tape具有持久性，并在每次使用完该tape后将其删除，释放资源\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "\n",
    "print(dz_dw1, dz_dw2)\n",
    "del tape"
   ],
   "id": "9ee46bf694337dde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(36.0, shape=(), dtype=float32) tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:35:07.014746Z",
     "start_time": "2025-09-11T07:35:07.003649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 默认情况下，tape仅跟踪涉及变量的操作 针对变量以外的任何其他对象计算z梯度，结果为None\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ],
   "id": "f81f85eaa2b22d0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:36:03.329943Z",
     "start_time": "2025-09-11T07:36:03.319461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 可以强制tape观察任何张量，以记录涉及它们的所有操作\n",
    "# 然后针对这些张量计算梯度，就好像它们是变量一样\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ],
   "id": "826067068a6d2040",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在某些情况下，watch可能有用，例如如果要实现正则化损失，从而在输入变化不大时惩罚那些变化很大的激活函数：损失将基于激活函数相对于输入的梯度而定。\n",
    "\n",
    "由于输入不是变量，因此需要告诉tape观察它们： 比如传进网络的输入 x_batch = tf.constant(...)。 它不是“可训练参数”，只是数据，不会被 GradientTape 自动跟踪。 如果想对输入本身算梯度，需要利用tape.watch\n",
    "\n",
    "在大多数情况下，梯度tape用来计算单个值（通常是损失）相对于一组值（通常是模型参数）的梯度。这就是反向模式自动微分有用的地方，因为它只需执行一次前向传播和一次反向传播即可一次获得所有梯度。如果尝试计算向量（例如包含多个损失的向量）的梯度，那么TensorFlow将计算向量和的梯度。因此，如果需要获取单独的梯度（例如每种损失相对于模型参数的梯度），则必须调用tape的jacobian()方法：它对向量中的每个损失执行一次反向模式自动微分（默认情况下全部并行）。它甚至可以计算二阶偏导数（即偏导数的偏导数），但实际上很少用到"
   ],
   "id": "5645faadc4136f03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:43:51.912363Z",
     "start_time": "2025-09-11T07:43:51.886348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if given a vector, tape.gradient() will compute the gradient of the vector's sum.\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2])"
   ],
   "id": "bd4fc920b18d07ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:44:29.742765Z",
     "start_time": "2025-09-11T07:44:29.717043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 和上个格子的计算结果一样\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "    z = z1 + z2 + z3\n",
    "\n",
    "tape.gradient(z, [w1, w2])\n"
   ],
   "id": "6f048aa290f27552",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:50:32.467836Z",
     "start_time": "2025-09-11T07:50:31.022295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "    y = tf.stack((z1,z2,z3))  # [z1, z2,z3]\n",
    "\n",
    "tape.jacobian(y, [w1, w2])"
   ],
   "id": "260e31294a17aa03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3,), dtype=float32, numpy=array([40., 46., 50.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 10., 10.], dtype=float32)>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:54:01.827820Z",
     "start_time": "2025-09-11T07:54:01.805167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 阻止梯度在神经网络的某些部分反向传播，使用tf.stop_gradient()函数\n",
    "# 该函数在前向传播过程中返回其输入（比如tf.identity()), 但在反向传播期间不让梯度通过（类似tf.constant)\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ],
   "id": "89f62d26ea34603c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:59:13.268797Z",
     "start_time": "2025-09-11T07:59:13.256844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 计算梯度遇到的数值问题\n",
    "x = tf.Variable(1e-50)  # 根号x 在x=10**(-50)的梯度，结果是无穷大，超过了32位浮点数可以处理的范围\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sqrt(x)\n",
    "\n",
    "tape.gradient(z, [x])\n",
    "\n",
    "# 解决方式：计算平方根时给x加一个小值（10**(-6))\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sqrt(x + 1e-6)\n",
    "\n",
    "tape.gradient(z, [x])"
   ],
   "id": "f2bd9a1a3e128692",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=499.99997>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T07:56:12.728838Z",
     "start_time": "2025-09-11T07:56:12.718845Z"
    }
   },
   "cell_type": "code",
   "source": "1 / (2 * np.sqrt(1e-50))",
   "id": "34352f7870b5bc92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.999999999999999e+24"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T08:01:35.231785Z",
     "start_time": "2025-09-11T08:01:35.224509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tf.math.log(1.0+tf.exp(100.0)))  # inf 无穷大\n",
    "# f(z) = log(1 + exp(z))\n",
    "\n",
    "# 数值稳定版的 softplus\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
    "\n",
    "# softplus(z) = softplus(–|z|) + max(0, z)"
   ],
   "id": "4933e877930baca1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(inf, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "以下是该等式等于 log(1 + exp(z)) 的证明：\n",
    "\n",
    "- softplus(z) = log(1 + exp(z))\n",
    "- softplus(z) = log(1 + exp(z)) - log(exp(z)) + log(exp(z)) ；只是加减同一个值\n",
    "- softplus(z) = log[(1 + exp(z)) / exp(z)] + log(exp(z)) ；因为 log(a) - log(b) = log(a / b)\n",
    "- softplus(z) = log[(1 + exp(z)) / exp(z)] + z ；因为 log(exp(z)) = z\n",
    "- softplus(z) = log[1 / exp(z) + exp(z) / exp(z)] + z ；因为 (1 + a) / b = 1 / b + a / b\n",
    "- softplus(z) = log[exp(-z) + 1] + z ；因为 1 / exp(z) = exp(–z)，且 exp(z) / exp(z) = 1\n",
    "- softplus(z) = softplus(–z) + z  ；log[exp(-z) + 1]就是softplus(-z)\n",
    "- softplus(z) = softplus(–|z|) + max(0, z) ；如果你考虑两种情况，即 z < 0 或 z ≥ 0，会发现这是成立的"
   ],
   "id": "23ad51774ba305f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在极少数情况下，数值稳定的函数可能仍然具有数值不稳定的梯度。在这种情况下，必须告诉TensorFlow使用哪个方程计算梯度，而不是让它使用自动微分。为此，必须在定义函数时使用@tf.custom_gradient装饰器，并返回函数通常的结果和计算梯度的函数。例如，更新my_softplus()函数以返回一个数值稳定的梯度函数：",
   "id": "1feaf8a5f0799176"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T12:27:39.377173Z",
     "start_time": "2025-09-10T12:27:39.364639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@tf.custom_gradient\n",
    "def my_softplus(z):\n",
    "    def my_softplus_gradients(grads): # 这里的grads是反向自动微分的上一层的累积梯度\n",
    "        return grads * (1 - 1/(1 + tf.exp(z)))  # softplus的稳定梯度\n",
    "    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
    "    return result, my_softplus_gradients"
   ],
   "id": "22cf6a7a3c524fe0",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T08:12:12.881745Z",
     "start_time": "2025-09-11T08:12:12.781689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ],
   "id": "9e787edb27676d40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "log(1+exp(z)) 的导数是 exp(z)/(1+exp(z))。但这种形式并不稳定：对于较大的 z 值，它最终计算会导致无穷大除以无穷大，返回 NaN。但是，通过一些代数运算，你可以证明它等于 1-1/(1+exp(z))，这是稳定的。\n",
    "my_softplus_gradients() 函数使用这个方案来计算梯度。请注意，此函数将接收到目前为止反向传播的梯度作为输入，向下传播到 my_soft_plus() 函数，并且根据链式法则，我们必须将它们与该函数的梯度相乘。\n",
    "\n",
    "此外，当计算 my_softplus() 函数的梯度时，得到了正确的结果，即使对于大输入值也是如此。\n",
    "\n",
    "现在可以计算任何函数的梯度（前提是它在计算点的点上是可微的），甚至在需要时阻止反向传播，并编写自己的梯度函数了  接下来，来看如何自定义训练循环。"
   ],
   "id": "2f22c61b01163fa4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义训练循环",
   "id": "2e0b8c4d1b302b1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在少数情况下，fit()方法可能不够灵活而无法满足你的需要。例如, 宽深神经网络的论文使用了两种不同的优化器：一种用于宽路径，另一种用于深路径。由于fit()方法只使用一个优化器（在编译模型时指定的优化器），因此实现该论文需要编写自己的自定义循环。\n",
    "\n",
    "有时想编写自定义训练循环，只是为了让自己更有信心，确信它们按照自己的意图进行操作，但是编写自定义训练循环会使代码更长，更容易出错并且更难以维护。\n",
    "\n",
    "除非正在学习或者真的需要额外的灵活性，否则应该更倾向使用fit()方法，而不是实现自己的训练循环"
   ],
   "id": "51ed328abe82f843"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T08:52:16.260625Z",
     "start_time": "2025-09-11T08:52:16.234559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建一个模型，无需编译，因为手动处理训练循环意味着，计算损失，优化器都在循环里自己调用\n",
    "l2_reg = tf.keras.regularizers.l2(0.05)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=l2_reg),\n",
    "    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg),\n",
    "])"
   ],
   "id": "a7bab2f540cdc0d2",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T09:09:58.903174Z",
     "start_time": "2025-09-11T09:09:58.889778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 从训练集中随机采样一批实例\n",
    "# for i in range(总轮次）\n",
    "#    循环这个轮次下每个批次：需要从样本 提取批次的步骤 （\n",
    "#        .\n",
    "\n",
    "\n",
    "def random_batch(X,y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# 定义一个函数，以显示训练状态，包括步数，步总数，从轮次开始以来的平均损失\n",
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\" for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "\n",
    "    print(f\"\\r{step}/{total} - \" + metrics, end=end)   # \\r和end=\"\"配合确保状态栏始终打印在同一行上"
   ],
   "id": "2da1b7f28f4d2785",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T08:55:12.945569Z",
     "start_time": "2025-09-11T08:55:12.934071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = 10000\n",
    "np.random.randint(m, size=32)"
   ],
   "id": "9a8a9b70a3ca7b32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7765, 6193, 3046,  304, 6339, 8043, 4908, 2804, 3936,  712, 6848,\n",
       "       9873, 9525, 8366, 3205, 5185, 9608, 6506, 1772, 1074, 5447, 8783,\n",
       "       5264, 7322, 6651, 2410, 9039, 6441, 8685, 8737, 9732, 9075])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T09:11:25.043816Z",
     "start_time": "2025-09-11T09:11:25.018163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "y_pred = model(X_batch, training=True)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "loss_fn(y_batch,y_pred)"
   ],
   "id": "7badf7802dfd220",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.3505983>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T09:11:54.004515Z",
     "start_time": "2025-09-11T09:11:53.994055Z"
    }
   },
   "cell_type": "code",
   "source": "tf.reduce_mean(loss_fn(y_batch,y_pred))",
   "id": "d1fdd00e41efff15",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.3505983>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T09:29:53.842938Z",
     "start_time": "2025-09-11T09:29:29.544167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义一些超参数，选择优化器，损失函数和指标（MAE）\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# 构建自定义循环\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True) #  relu(X@W1+b1) @ W2 + b2\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))  # tf.reduce_mean可以不需要\n",
    "\n",
    "            # tf.add_n([main_loss, 其余的损失（正则化，layer自己加的）])  ->  main_loss + 其余的损失  -> 最终的损失\n",
    "            loss = tf.add_n([main_loss] + model.losses)  # model.losses 可以把所有的损失 汇总一个python列表里\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)  # 计算 损失 关于 所有可训练的参数的梯度 （内部用反向微分）\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  #   [(损失关于可训练参数1的梯度,可训练的参数1）, ... (损失关于可训练参数n的梯度,可训练的参数n)]\n",
    "\n",
    "        # 如果想给模型添加权重约束（kernel_constraint/bias_constraint) 在apply_gradients()之后立即应用这些约束\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "\n",
    "        mean_loss(loss)  # 这个批次的损失传给mean_loss, 返回这个轮次的平均损失\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()  # reset_states(): 下个轮次，评价指标从头统计"
   ],
   "id": "e1f39a6257138dcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "362/362 - mean_loss: 3.1911 - mean_absolute_error: 0.6482\n",
      "Epoch 2/5\n",
      "362/362 - mean_loss: 1.7012 - mean_absolute_error: 0.5130\n",
      "Epoch 3/5\n",
      "362/362 - mean_loss: 1.1168 - mean_absolute_error: 0.4925\n",
      "Epoch 4/5\n",
      "362/362 - mean_loss: 0.8541 - mean_absolute_error: 0.4983\n",
      "Epoch 5/5\n",
      "362/362 - mean_loss: 0.7359 - mean_absolute_error: 0.5005\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 创建了两个嵌套循环：一个用于轮次，另一个用于轮次内的批处理。\n",
    "\n",
    "- 然后从训练集中抽取一个随机批次。\n",
    "\n",
    "- 在 tf.GradientTape() 块中，我们对一个批次进行了预测（使用模型作为函数），并计算了损失：它等于主损失加其他损失（在此模型中，每层都有一个正则化损失）。由于 mean_squared_error() 函数每个实例返回一个损失，因此我们使用 tf.reduce_mean() 计算批次的平均值（如果要对每个实例应用不同的权重，则可以在这里进行操作）。正则化损失已经归约到单个标量，因此我们只需要对它们进行求和（使用 tf.add_n() 即可对具有相同形状和数据类型的多个张量求和）。\n",
    "\n",
    "- 接下来，要 tape 针对每个可训练变量（不是所有变量！）计算损失的梯度，并用优化器来执行“梯度下降”步骤。\n",
    "\n",
    "- 然后，更新平均损失和指标（在当前轮次内），并显示状态栏。\n",
    "\n",
    "- 在每个轮次结束时，我们重置平均损失和指标的状态。\n",
    "\n",
    "如果想用梯度裁剪，则设置优化器的clipnorm或clipvalue超参数；如果想对梯度做任何其他变换，只需要调用apply_gradients方法之前做就行"
   ],
   "id": "a25b2817dea514bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TensorFlow函数和图",
   "id": "5067320d2ce75ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T13:43:37.026179Z",
     "start_time": "2025-09-10T13:43:37.013177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ],
   "id": "5442ac2f5d660b47",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T13:43:48.940746Z",
     "start_time": "2025-09-10T13:43:48.925748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cube(2)\n",
    "cube(tf.constant(2.0))"
   ],
   "id": "d182d3ed514e16a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T13:44:41.601040Z",
     "start_time": "2025-09-10T13:44:41.586911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ],
   "id": "e8c739af545c44c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x22666531510>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tf.function修饰函数后，TensorFlow 在第一次调用时不会真的跑计算，而是跟踪（Tracing）每一步运算，最终把这些操作记录成一张 计算图（Graph）。",
   "id": "b939e421f6834ed8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@tf.function  # 更常用的写法，直接换成装饰器\n",
    "def tf_cube(x):\n",
    "    print(f\"x = {x}\")\n",
    "    return x ** 3"
   ],
   "id": "1d15d595053b1ad9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T13:56:37.542283Z",
     "start_time": "2025-09-10T13:56:37.532283Z"
    }
   },
   "cell_type": "code",
   "source": "tf_cube.python_function(2) # 使用原始的python函数",
   "id": "e5ed8dc053eae433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TensorFlow可以优化计算图，修剪未使用的节点，简化表达式等。准备好优化的图后，TF函数会以适当的顺序有效地执行图中的操作。因此，TF函数通常比原始的Python函数运行得更快，尤其是在执行复杂计算的情况下。\n",
    "\n",
    "大多数时候，想利用tensorflow加速Python函数时，不需了解更多，只需将其转换为TF函数即可。\n",
    "\n",
    "此外，如果在调用tf.function()时设置jit_compile=True，则TensorFlow将使用加速线性代数(XLA)为图编译专用内核，通常会融合多个操作。例如，如果TF函数调用tf.reduce_sum(a * b+c)，那么在没有XLA的情况下，该函数首先需要计算a * b并将结果存储在一个临时变量中，然后将c添加到该变量，最后对结果调用tf.reduce_sum()。使用XLA，整个计算被编译到一个内核中，它将一次完成tf.reduce_sum(a*b+c)计算，不使用任何临时变量。这不仅速度会更快，而且使用的RAM更少。\n",
    "\n",
    "当编写自定义损失函数、自定义指标、自定义层或任何其他自定义函数并在Keras模型中使用它时，Keras会自动将函数转换为TF函数——无须使用tf.function()。而如果想让Keras使用XLA，只需要在调用compile()方法时设置jit_compile=True即可。"
   ],
   "id": "31441b5156cbaa83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "认情况下，TF函数会为每个不同的输入形状和数据类型集生成一个新图，并将其缓存以供后续调用。例如，如果调用tf_cube(tf.constant(10))，将为形状为[ ]的int32张量生成图。如果调用tf_cube(tf.constant(20))，则会重用相同的图。但是，如果随后调用tf_cube(tf.constant([10，20]))，则会为形状为[2]的int32张量生成一个新图。这就是TF函数处理多态（即变化的参数类型和形状）的方式。但是，这仅适用于张量参数：如果将Python数值传递给TF函数，则将为每个不同的值生成一个新图，例如，调用tf_cube(10)和tf_cube(20)将生成两个图。\n",
    "\n",
    "如果用不同的Python数值多次调用TF函数，则会生成许多图，这会降低程序运行速度并消耗大量RAM（必须删除TF函数才能释放它）"
   ],
   "id": "24c5f0ba6936d10d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T13:56:39.402444Z",
     "start_time": "2025-09-10T13:56:39.330770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = tf_cube(tf.constant(2.0))\n",
    "result = tf_cube(2)\n",
    "result = tf_cube(3)\n",
    "result = tf_cube(tf.constant([[1., 2.]]))\n",
    "result = tf_cube(tf.constant([[3., 4.], [5., 6.]]))"
   ],
   "id": "720eca19e8442098",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Tensor(\"x:0\", shape=(), dtype=float32)\n",
      "x = 2\n",
      "x = 3\n",
      "x = Tensor(\"x:0\", shape=(1, 2), dtype=float32)\n",
      "x = Tensor(\"x:0\", shape=(2, 2), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function tf_cube at 0x0000022666CC9AB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T13:54:48.140348Z",
     "start_time": "2025-09-10T13:54:48.125266Z"
    }
   },
   "cell_type": "code",
   "source": "result = tf_cube(tf.constant([[7., 8.], [9., 10.]]))  # 一样的形状，没有生成图",
   "id": "1f9280d0a214ff58",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T14:02:24.853620Z",
     "start_time": "2025-09-10T14:02:24.838616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 指定 特定的input_signature\n",
    "@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\n",
    "def shrink(images):\n",
    "    print(\"Tracing\", images)\n",
    "    return images[:, ::2, ::2]"
   ],
   "id": "36fac522b39027dc",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T14:18:06.227841Z",
     "start_time": "2025-09-10T14:18:06.202836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_batch_1 = tf.random.uniform(shape=[100, 28, 28])\n",
    "img_batch_2 = tf.random.uniform(shape=[50, 28, 28])\n",
    "preprocessed_images = shrink(img_batch_1)   # 跟踪了函数\n",
    "preprocessed_images = shrink(img_batch_2)   # 没有跟踪\n",
    "\n",
    "img_batch_3 = tf.random.uniform(shape=[2, 2, 2])\n",
    "# preprocessed_images = shrink(img_batch_3)   # 和input_signature不兼容， 会报错"
   ],
   "id": "53d086281a5e8c4",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AutoGraph和跟踪",
   "id": "53f5649773501f65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上一部分属于跟踪运算，生成计算图；但是Python 的控制流（if/for/while）不是运算符，没法像 + * 那样被“跟踪”（跟踪__add__, __mul__), AutoGraph 直接把你ython 源码翻译一遍，把 if / for / while 换成等价的 tf.cond() / tf.while_loop()。\n",
    "\n",
    "所以 AutoGraph 在 编译阶段 做代码转换，让这些控制流能进图。\n",
    "\n",
    "接下来，TensorFlow将此函数称为“升级”函数，但不传递参数，而是传递符号张量——没有任何实际值的张量，仅包含名称、数据类型和形状。例如，如果调用 sum_even_linear_graph(tf.constant(3.0), tf.constant(10))，TensorFlow 实际上传入的是两个符号张量：x 的 dtype 为 float32、shape 为 []；n 的 dtype 为 int32、shape 为 []。该函数将在图模式下运行，这意味着每个TensorFlow操作都会在图中添加一个节点来表示自身及其输出张量（与常规模式相对，称为eager执行或eager模式）。在图模式下，TF操作不执行任何计算,正的数值运算发生在执行阶段。\n",
    "\n",
    "<img alt=\"Tensorflow如何使用Autograph和跟踪生成图\" height=\"500\" src=\"./images/tensorflow/p5.png\" width=\"500\"/>"
   ],
   "id": "3ddfd9babb95dc88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T14:42:56.501820Z",
     "start_time": "2025-09-10T14:42:56.351787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) 同一段逻辑：对 0..n-1 中的偶数 i，计算 s += i * x\n",
    "def sum_even_linear_eager(x, n):\n",
    "    s = tf.constant(0., dtype=tf.float32)\n",
    "    for i in range(n):            # 纯 Python for（Eager 下逐句执行）\n",
    "        if i % 2 == 0:            # 纯 Python if\n",
    "            s = s + float(i) * x\n",
    "    return s\n",
    "\n",
    "@tf.function  # 2) 编译成图：会触发 AutoGraph + Tracing\n",
    "def sum_even_linear_graph(x, n):\n",
    "    s = tf.constant(0., dtype=tf.float32)\n",
    "    # 用 tf.range 有助于避免频繁 retrace（也能让逻辑更“张量化”）\n",
    "    for i in tf.range(n):         # Python for → AutoGraph → tf.while_loop\n",
    "        if (i % 2) == 0:          # Python if  → AutoGraph → tf.cond\n",
    "            s = s + tf.cast(i, tf.float32) * x\n",
    "    return s\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "n = tf.constant(10)\n",
    "\n",
    "print(\"\\n[Eager] 直接执行：\")\n",
    "print(sum_even_linear_eager(x, int(n.numpy())))  # -> 3*(0+2+4+6+8) = 3*20 = 60\n",
    "\n",
    "print(\"\\n[Graph] 第一次调用会 tracing（建图），随后直接执行已编译图：\")\n",
    "print(sum_even_linear_graph(x, n))               # 同样应为 60\n",
    "\n",
    "# 3) 看 AutoGraph 把 Python 源码转换成了什么\n",
    "print(\"\\n[AutoGraph 转换后的函数源码片段]:\")\n",
    "converted_src = tf.autograph.to_code(sum_even_linear_graph.python_function)\n",
    "print(\"\\n\".join(converted_src.splitlines()[:40]))  # 只看前 40 行"
   ],
   "id": "5b8ccb931c39f6e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eager] 直接执行：\n",
      "tf.Tensor(60.0, shape=(), dtype=float32)\n",
      "\n",
      "[Graph] 第一次调用会 tracing（建图），随后直接执行已编译图：\n",
      "tf.Tensor(60.0, shape=(), dtype=float32)\n",
      "\n",
      "[AutoGraph 转换后的函数源码片段（截断展示）]:\n",
      "def tf__sum_even_linear_graph(x, n):\n",
      "    with ag__.FunctionScope('sum_even_linear_graph', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        s = ag__.converted_call(ag__.ld(tf).constant, (0.0,), dict(dtype=ag__.ld(tf).float32), fscope)\n",
      "\n",
      "        def get_state_1():\n",
      "            return (s,)\n",
      "\n",
      "        def set_state_1(vars_):\n",
      "            nonlocal s\n",
      "            (s,) = vars_\n",
      "\n",
      "        def loop_body(itr):\n",
      "            nonlocal s\n",
      "            i = itr\n",
      "\n",
      "            def get_state():\n",
      "                return (s,)\n",
      "\n",
      "            def set_state(vars_):\n",
      "                nonlocal s\n",
      "                (s,) = vars_\n",
      "\n",
      "            def if_body():\n",
      "                nonlocal s\n",
      "                s = ag__.ld(s) + ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(i), ag__.ld(tf).float32), None, fscope) * ag__.ld(x)\n",
      "\n",
      "            def else_body():\n",
      "                nonlocal s\n",
      "                pass\n",
      "            ag__.if_stmt(ag__.ld(i) % 2 == 0, if_body, else_body, get_state, set_state, ('s',), 1)\n",
      "        i = ag__.Undefined('i')\n",
      "        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, (ag__.ld(n),), None, fscope), None, loop_body, get_state_1, set_state_1, ('s',), {'iterate_names': 'i'})\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(s)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**为什么要把函数变成计算图**：\n",
    "\n",
    "1. **算梯度（自动微分）**\n",
    "\n",
    "* 深度学习训练的核心是 **反向传播**。\n",
    "* 反向传播需要知道前向运算的“依赖关系”。\n",
    "* 计算图正好就是一张依赖图：每个节点有输入、有输出。\n",
    "* TensorFlow 就可以在这张图上“从输出往输入”走，自动套链式法则。\n",
    "   所以 **图 = 梯度计算的路线图**。\n",
    "\n",
    "\n",
    "2. **硬件优化（加速）**\n",
    "\n",
    "* 图是静态的，TF 能在图级别做很多 **全局优化**：\n",
    "\n",
    "  * **算子融合**（多个小操作合并成一个大核，减少 GPU/TPU 调用开销）。\n",
    "  * **常量折叠**（提前把常量算掉，节省运行时开销）。\n",
    "  * **内存复用**（释放/重用张量内存，避免爆显存）。\n",
    "  * **跨设备调度**（哪些节点跑 GPU，哪些跑 CPU/TPU）。\n",
    "* Eager 模式下，操作是一步步立即执行，没机会做这些整体优化。\n",
    "    所以 **图 = 优化器能发挥的舞台**。\n",
    "\n",
    "\n",
    "\n",
    "3. **部署 & 可移植性**\n",
    "\n",
    "* 图是语言无关的（一个 JSON/ProtoBuf 格式）。\n",
    "* 你训练好模型后，可以把图导出：\n",
    "\n",
    "  * **TF Serving**（服务器上推理）。\n",
    "  * **TF Lite**（移动端/嵌入式）。\n",
    "  * **TF.js**（浏览器）。\n",
    "* 没有图，只能在 Python 环境里跑，没法跨平台部署。\n",
    "    所以 **图 = 通用模型格式**。\n",
    "\n",
    "\n",
    "4. **调试 & 可视化**\n",
    "\n",
    "* 图能在 **TensorBoard** 里显示：层的结构、数据流、梯度规模、耗时瓶颈。\n",
    "* 这对理解模型和性能优化非常直观。\n",
    "\n",
    "\n",
    "**变成计算图 = 为了能自动算梯度 + 方便全局优化 + 跨平台部署 + 可视化调试。**\n",
    "\n"
   ],
   "id": "23c38472cb2974c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TF函数规则",
   "id": "e247e7ccc2519932"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "在大多数情况下，将执行 TensorFlow 操作的 Python 函数转换为 TF 函数很简单：用 `tf.function` 装饰它或让 Keras 处理。\n",
    "但是，有一些规则需要遵守：\n",
    "\n",
    "* **避免调用外部库**\n",
    "  如果调用任何外部库，包括 NumPy 基础标准库，此调用将在跟踪过程中执行，它不会成为图的一部分。\n",
    "  实际上，TensorFlow 图只能包含 TensorFlow 结构（张量、运算、变量、数据集等）。\n",
    "  因此，请使用 `tf.reduce_sum()` 代替 `np.sum()`，使用 `tf.sort()` 代替内置的 `sorted()` 函数。\n",
    "  （除非确实希望这些代码只在跟踪过程中运行。）\n",
    "\n",
    "* **随机数生成必须使用 TF 提供的方法**\n",
    "  如果你写了一个返回 `np.random.rand()` 的 TF 函数 `f(x)`，那么随机数只会在**跟踪函数时**生成一次。\n",
    "\n",
    "  * `tf.constant(2.)` 和 `tf.constant(np.random.rand())` 返回相同的随机数。\n",
    "  * 但 `tf.constant([2., 3.])` 将返回不同的随机值。\n",
    "  * 要生成真正的随机数，请使用 `tf.random.uniform([])`。这样每次操作都会生成新的随机数，因为它属于图的一部分。\n",
    "\n",
    "* **避免副作用**\n",
    "  如果非 TensorFlow 代码具有副作用（例如写日志、更新 Python 计数器），不要期望每次调用 TF 函数都会发生这些副作用，因为它们只会在**跟踪**函数时执行。\n",
    "\n",
    "* **慎用 `py_function`**\n",
    "  可以在 `tf.py_function()` 操作中包装任何 Python 代码，但这样会降低性能，因为 TensorFlow 无法对其中代码做任何图优化。\n",
    "  这会降低可移植性，因为该代码只能在安装了 Python 的平台上运行。\n",
    "\n",
    "* **Python 函数要符合规则**\n",
    "  可以调用其他 Python 函数，它们也必须遵循相同的规则，因为 TensorFlow 会在计算图中捕获它们的操作。\n",
    "  注意：这些函数本身需要用 `@tf.function` 修饰。\n",
    "\n",
    "* **变量必须只创建一次**\n",
    "  如果在 TF 函数内部创建了变量（例如数据集或张量列表），必须确保只创建一次，否则会引发错误。\n",
    "  常见做法：在 `__init__` 或 `build()` 方法中创建变量，更新时用 `assign()`，而不是重新赋值。\n",
    "\n",
    "* **源码可用性**\n",
    "  Python 源码必须可用才能用于 TensorFlow。\n",
    "  如果函数定义在交互式 Python shell 中，或者没有源码（比如 `.pyc` 编译文件），生成图可能失败。\n",
    "\n",
    "* **避免使用 Python 原生循环遍历 Dataset**\n",
    "  TensorFlow 图会捕获张量级别的循环（`tf.range()`），而不会捕获 Python 原生 `for` 循环。\n",
    "  如果用 Python 的 `for` 循环迭代 Dataset，则无法在图模式下被捕获，也无法在分布式环境下正常运行。（不在图里的计算，无法重现到其他机器）\n",
    "\n",
    "* 出于性能原因，应尽可能使用向量化实现，而不是使用循环。"
   ],
   "id": "9359f29090307116"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 其他Tensor类型",
   "id": "dd118d6efaa61e4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 字符串",
   "id": "82fec30a9520ddf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:08:33.383906Z",
     "start_time": "2025-09-11T23:08:33.368277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.constant(\"hello world\")\n",
    "tf.constant(b\"hello world\")"
   ],
   "id": "f65febb792636018",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hello world'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:10:10.438354Z",
     "start_time": "2025-09-11T23:10:10.422337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "u = tf.constant([ord(c) for c in \"诞生于1996,梦想做说唱领袖\"])\n",
    "u"
   ],
   "id": "7fe95102d4dbc00d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15,), dtype=int32, numpy=\n",
       "array([35806, 29983, 20110,    49,    57,    57,    54,    44, 26790,\n",
       "       24819, 20570, 35828, 21809, 39046, 34966])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:10:31.375139Z",
     "start_time": "2025-09-11T23:10:31.343849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
    "b"
   ],
   "id": "b1cb7c7e4cc95145",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xaf\\x9e\\xe7\\x94\\x9f\\xe4\\xba\\x8e1996,\\xe6\\xa2\\xa6\\xe6\\x83\\xb3\\xe5\\x81\\x9a\\xe8\\xaf\\xb4\\xe5\\x94\\xb1\\xe9\\xa2\\x86\\xe8\\xa2\\x96'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:10:59.030764Z",
     "start_time": "2025-09-11T23:10:59.015176Z"
    }
   },
   "cell_type": "code",
   "source": "tf.strings.length(b, unit=\"UTF8_CHAR\")",
   "id": "c46e027c49b78ecc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:12:19.798909Z",
     "start_time": "2025-09-11T23:12:19.783291Z"
    }
   },
   "cell_type": "code",
   "source": "tf.strings.unicode_decode(b, \"UTF-8\")",
   "id": "b273b99ed459157a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15,), dtype=int32, numpy=\n",
       "array([35806, 29983, 20110,    49,    57,    57,    54,    44, 26790,\n",
       "       24819, 20570, 35828, 21809, 39046, 34966])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:17:28.787876Z",
     "start_time": "2025-09-11T23:17:28.772233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = tf.constant([\"全民制作人\", \"你们好\", \"cxk\", \"wzy\"])\n",
    "p\n",
    "\n",
    "tf.strings.length(p, unit=\"UTF8_CHAR\")\n",
    "r = tf.strings.unicode_decode(p, \"UTF8\")\n",
    "r"
   ],
   "id": "4ea9bb19085eb058",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[20840, 27665, 21046, 20316, 20154], [20320, 20204, 22909],\n",
       " [99, 120, 107], [119, 122, 121]]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 张量数组（tf.TensorArray)",
   "id": "f7efc67e364c83db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "张量的列表，默认固定长度，可以动态增长；包含的张量必须具有相同的形状和数据类型",
   "id": "521cc615f5606487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:19:21.786208Z",
     "start_time": "2025-09-11T23:19:21.770564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "array = tf.TensorArray(dtype=tf.float32, size=3)\n",
    "array = array.write(0, tf.constant([1., 2.]))\n",
    "array = array.write(1, tf.constant([3., 10.]))\n",
    "array = array.write(2, tf.constant([5., 7.]))\n",
    "tensor1 = array.read(1)  #  tf.constant([3., 10.]) , 输出位置清0\n",
    "tensor1"
   ],
   "id": "705eabf6503e8962",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 3., 10.], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:27:36.360448Z",
     "start_time": "2025-09-11T23:27:36.329126Z"
    }
   },
   "cell_type": "code",
   "source": "array.stack()",
   "id": "8f8b0f3dbf434551",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [0., 0.],\n",
       "       [5., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:28:21.634887Z",
     "start_time": "2025-09-11T23:28:21.619259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "array2 = tf.TensorArray(dtype=tf.float32, size=3, clear_after_read=False)\n",
    "array2 = array2.write(0, tf.constant([1., 2.]))\n",
    "array2 = array2.write(1, tf.constant([3., 10.]))\n",
    "array2 = array2.write(2, tf.constant([5., 7.]))\n",
    "tensor2 = array2.read(1)  # return tf.constant([3., 10.])\n",
    "array2.stack()"
   ],
   "id": "aad1b55c221c1c93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 1.,  2.],\n",
       "       [ 3., 10.],\n",
       "       [ 5.,  7.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:28:54.512923Z",
     "start_time": "2025-09-11T23:28:54.481681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 长度动态增长\n",
    "array3 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "array3 = array3.write(0, tf.constant([1., 2.]))\n",
    "array3 = array3.write(1, tf.constant([3., 10.]))\n",
    "array3 = array3.write(2, tf.constant([5., 7.]))\n",
    "tensor3 = array3.read(1)\n",
    "array3.stack()"
   ],
   "id": "762f1e49de501eb1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [0., 0.],\n",
       "       [5., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a57a6d66ad8eb15a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
