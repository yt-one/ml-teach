{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用字符RNN生成莎士比亚文本",
   "id": "b6905ec0806fdce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在2015年的一篇著名博客文章 \"https://karpathy.github.io/2015/05/21/rnn-effectiveness/\" 中，Andrej Karpathy展示了如何训练循环神经网络来预测句子中的下一个字符。这个char-RNN可以用来生成小说文本，每次一个字符。",
   "id": "bdf100365ed041e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-22T02:49:21.704737Z",
     "start_time": "2026-01-22T02:49:19.435088Z"
    }
   },
   "source": [
    "# tf.keras.utils.get_file()函数下载莎士比亚的所有作品\n",
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T02:21:25.377590Z",
     "start_time": "2026-01-22T02:21:25.362903Z"
    }
   },
   "cell_type": "code",
   "source": "print(shakespeare_text[:80])",
   "id": "5a0427af95a99073",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:27.569769Z",
     "start_time": "2025-10-13T06:24:26.907345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用tf.keras.layers.TextVectorization层对此文本进行编码。我们将设置split=\"character\"，基于字符进行编码，而不是默认的基于单词进行编码，并使用standardize=\"lower\"将文本转换为小写（这将简化任务）：\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "\n",
    "\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "encoded\n"
   ],
   "id": "a051c0bf83bb06c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:28.765374Z",
     "start_time": "2025-10-13T06:24:28.754856Z"
    }
   },
   "cell_type": "code",
   "source": "text_vec_layer.get_vocabulary()",
   "id": "fc62a9636f6ee7f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " ' ',\n",
       " 'e',\n",
       " 't',\n",
       " 'o',\n",
       " 'a',\n",
       " 'i',\n",
       " 'h',\n",
       " 's',\n",
       " 'r',\n",
       " 'n',\n",
       " '\\n',\n",
       " 'l',\n",
       " 'd',\n",
       " 'u',\n",
       " 'm',\n",
       " 'y',\n",
       " 'w',\n",
       " ',',\n",
       " 'c',\n",
       " 'f',\n",
       " 'g',\n",
       " 'b',\n",
       " 'p',\n",
       " ':',\n",
       " 'k',\n",
       " 'v',\n",
       " '.',\n",
       " \"'\",\n",
       " ';',\n",
       " '?',\n",
       " '!',\n",
       " '-',\n",
       " 'j',\n",
       " 'q',\n",
       " 'x',\n",
       " 'z',\n",
       " '3',\n",
       " '&',\n",
       " '$']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:32.555436Z",
     "start_time": "2025-10-13T06:24:32.545488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 每个字符现在都映射到一个整数，从2开始。TextVectorization层将值0保留为填充词元(token)，并将值1保留为未知字符。\n",
    "# 现在不需要这两个词元，因此从字符ID数中减去2，并计算不同字符的总数和总字符数：\n",
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(n_tokens)"
   ],
   "id": "8e7d446d727dc90e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将这个非常长的序列转换成一个窗口数据集，然后使用它来训练一个序列到序列的循环神经网络。目标序列将类似于输入序列，但是会向“未来”移动一个时间步。例如，数据集中的样本可能是一个由表示文本“to be or not to b”（不包括最后一个“e”）的字符ID组成的序列，相应的目标序列是一个由表示文本“o be or not to be”（包括最后一个“e”，但不包括开头的“t”）的字符ID组成的序列",
   "id": "6f9c13d7635eb999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:23:47.007242Z",
     "start_time": "2025-10-13T06:23:46.995673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将字符ID序列转换成输入/目标窗口对的数据集\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ],
   "id": "70ed9e8982001bf0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 它将序列作为输入（即编码文本），并创建一个包含所需长度的所有窗口的数据集。\n",
    "- 它将长度增加1，因为我们需要将下一个字符放在目标序列中。\n",
    "- 然后，它对窗口进行乱序处理（可选），将它们分批，将它们拆分为输入/输出对，并激活预取功能。\n",
    "\n",
    "长度为11的窗口和批量大小3。每个窗口的起始索引都显示在旁边：\n",
    "\n",
    "![准备乱序窗口的数据集](./images/RNN/p7.png)"
   ],
   "id": "205152dc0a2c913e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:39.614518Z",
     "start_time": "2025-10-13T06:24:39.509151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 大约90%的文本进行训练，5%用于验证，5%用于测试\n",
    "length = 100  # length决定循环神经网络能学习的最长模式\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000: 1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ],
   "id": "414ec5b323fe4c3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:40.485396Z",
     "start_time": "2025-10-13T06:24:40.365750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for data in valid_set.take(1):\n",
    "    print(data)"
   ],
   "id": "2198babd695e6b80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 5,  7,  0, ...,  6,  1,  0],\n",
      "       [ 7,  0,  8, ...,  1,  0, 18],\n",
      "       [ 0,  8,  1, ...,  0, 18,  6],\n",
      "       ...,\n",
      "       [ 4,  2,  0, ...,  3, 26, 10],\n",
      "       [ 2,  0,  7, ..., 26, 10, 10],\n",
      "       [ 0,  7,  6, ..., 10, 10,  2]], dtype=int64)>, <tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 7,  0,  8, ...,  1,  0, 18],\n",
      "       [ 0,  8,  1, ...,  0, 18,  6],\n",
      "       [ 8,  1,  4, ..., 18,  6,  3],\n",
      "       ...,\n",
      "       [ 2,  0,  7, ..., 26, 10, 10],\n",
      "       [ 0,  7,  6, ..., 10, 10,  2],\n",
      "       [ 7,  6,  1, ..., 10,  2,  8]], dtype=int64)>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 构建和训练char-RNN模型",
   "id": "ce01571b7082bc92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 使用Embedding层作为第一层，以编码字符ID。Embedding层的输入维数是不同字符ID的数量，输出维数是可以调整的超参数——现在将其设置为16。Embedding层的输入将是形状为［批量大小，窗口长度］的二维张量，Embedding层的输出将是形状为［批量大小，窗口长度，嵌入大小］的三维张量。\n",
    "- 为输出层使用Dense层：它必须具有39个单元(n_tokens)，因为文本中有39个不同的字符，希望在每个时间步输出每个可能字符的概率。每个时间步39个输出概率的总和应该为1，因此对Dense层的输出应用softmax激活函数。\n",
    "- 最后，使用\"sparse_categorical_crossentropy\"损失和Nadam优化器编译此模型，并使用ModelCheckpoint回调函数在训练过程中保存最佳模型（以验证精度为标准）进行多个轮次的训练。"
   ],
   "id": "bbeb76c3ffa2e1ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:11:43.648220Z",
     "start_time": "2025-10-13T07:11:43.068519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 不用简单RNN,用带有长短记忆的GRU\n",
    "#\n",
    "# [[ 5,  7,  0, ...,  6,  1,  0]]  1 * 100\n",
    "# [[ v5  v7  v0  .... v6  v1  v0]] 1 * 100 * 16\n",
    "\n",
    "# n_tokens:39\n",
    "# embedding： 39 * n 列的矩阵：  5行：  [w0, w1, ... wn-1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
   ],
   "id": "ddb8c8daf6234f32",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:14:49.242137Z",
     "start_time": "2025-10-13T07:14:48.979220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练后搭建最终模型\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X-2), # 不使用填充词元(0)  和 未知词元 (1)\n",
    "    model\n",
    "])"
   ],
   "id": "4a9cd3d1b9707e25",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:19:35.453464Z",
     "start_time": "2025-10-13T07:19:35.333035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to be\"])[0, -1]\n",
    "y_proba\n",
    "y_pred = tf.argmax(y_proba)  # 选择概率最高的字母ID\n",
    "\n",
    "print(text_vec_layer.get_vocabulary())\n",
    "\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ],
   "id": "660fc80a64c43c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "['', '[UNK]', ' ', 'e', 't', 'o', 'a', 'i', 'h', 's', 'r', 'n', '\\n', 'l', 'd', 'u', 'm', 'y', 'w', ',', 'c', 'f', 'g', 'b', 'p', ':', 'k', 'v', '.', \"'\", ';', '?', '!', '-', 'j', 'q', 'x', 'z', '3', '&', '$']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 生成莎士比亚文本",
   "id": "97a339785e97f31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "要使用char-RNN模型生成新文本，可以将一些文本输入模型，让模型预测最有可能的下一个字母，将其添加到文本的末尾，然后将扩展后的文本输入模型来猜测下一个字母，以此类推。这叫作贪婪解码。但是在实践中，这往往导致相同的单词一遍又一遍地重复。\n",
    "\n",
    "相反，可以使用TensorFlow的tf.random.categorical()函数随机采样下一个字符，采样概率等于估计概率。这将生成更多样化和有趣的文本。categorical()函数在给定类别对数概率(logits)的情况下，对随机类别指数进行采样。"
   ],
   "id": "c5d1bcc028d8d3e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:36:14.884959Z",
     "start_time": "2025-10-13T07:36:14.873932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n = 2000\n",
    "tf.random.categorical(log_probas, num_samples=n)\n",
    "\n"
   ],
   "id": "89b01494486ce3e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2000), dtype=int64, numpy=array([[0, 1, 0, ..., 1, 0, 2]], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了更好地控制生成文本的多样性，可以将logits（对数概率）除以一个称为“温度”的数字，这个数字可以根据需求进行调整。“温度”接近零将更偏向于高概率字符，而较高“温度”则会使所有字符获得相同的概率。通常在生成相对严谨和精确的文本（例如数学公式）时，较低的“温度”更为适用，而在生成更多样化且有创意的文本时，则适合用较高的“温度”。",
   "id": "aefc5c6f8577915f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:36:16.894325Z",
     "start_time": "2025-10-13T07:36:16.882005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "b358a7b630203dce",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:13:26.642973Z",
     "start_time": "2026-01-22T03:13:26.627334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ],
   "id": "fbd43a4da8c0dd45",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:36:23.437635Z",
     "start_time": "2025-10-13T07:36:19.758035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"To be or not to be\", temperature=1))\n"
   ],
   "id": "786e75ebe35d87e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "To be or not to bedf3zwvcik :ua!&q. :phgr&;ubltcpzhp:'rv:cq3z!$ pau:\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:47:50.974144Z",
     "start_time": "2025-10-10T13:47:48.297565Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=1))",
   "id": "ccb5b2f788da0fc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "To be or not to be good and wrong; but come. though which the fine.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:48:15.816518Z",
     "start_time": "2025-10-10T13:48:13.225736Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=100))",
   "id": "d36fa2f1a2ba4d56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "To be or not to bef ,mt'&ozfpady-$\n",
      "wh!nse?pws3ert--vgerdjw?c-y-ewxnj\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模拟生成莎士比亚文本的流程 生成名字",
   "id": "c69b7c36365a237d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:38:55.207827Z",
     "start_time": "2025-10-13T07:38:55.195726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"datasets/dino.txt\", \"r\") as f:\n",
    "    dino_names = f.read()\n",
    "    dino_names = dino_names.lower()"
   ],
   "id": "8c9db58ee8770353",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:39:27.978605Z",
     "start_time": "2025-10-13T07:39:27.812047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([dino_names])\n",
    "encoded = text_vec_layer([dino_names])[0]\n",
    "encoded"
   ],
   "id": "82a72b78a106c110",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19909,), dtype=int64, numpy=array([ 2,  2, 15, ...,  4,  4, 12], dtype=int64)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:39:41.911362Z",
     "start_time": "2025-10-13T07:39:41.900342Z"
    }
   },
   "cell_type": "code",
   "source": "text_vec_layer.get_vocabulary()",
   "id": "1febe554f24a9100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'a',\n",
       " 's',\n",
       " 'u',\n",
       " 'o',\n",
       " 'r',\n",
       " '\\n',\n",
       " 'n',\n",
       " 'i',\n",
       " 'e',\n",
       " 't',\n",
       " 'l',\n",
       " 'p',\n",
       " 'h',\n",
       " 'c',\n",
       " 'g',\n",
       " 'd',\n",
       " 'm',\n",
       " 'y',\n",
       " 'b',\n",
       " 'k',\n",
       " 'v',\n",
       " 'x',\n",
       " 'z',\n",
       " 'j',\n",
       " 'w',\n",
       " 'f',\n",
       " 'q']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:42:01.180282Z",
     "start_time": "2025-10-13T07:42:01.162135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "n_tokens"
   ],
   "id": "340430d9880913e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:43:02.164675Z",
     "start_time": "2025-10-13T07:43:02.153141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "end_of_name_encode = text_vec_layer([\"\\n\"])[0,0].numpy() - 2\n",
    "end_of_name_encode"
   ],
   "id": "8019902ca41e0a37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:02:38.365281Z",
     "start_time": "2025-10-13T08:02:38.259717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把 Tensor 转成 numpy 数组\n",
    "encoded_np = encoded.numpy()\n",
    "\n",
    "# 存放 (X, Y) 对\n",
    "names_X = []  # [第一个名字不包括换行符， 第二个名字不包括换行符， ...  最后一个名字不包括换行符]\n",
    "names_Y = []  # [第一个名字有换行符，但无第一个字符， .....    , ...  最后一个名字有换行符，但无第一个字符]\n",
    "\n",
    "# 临时缓冲区\n",
    "current_name = []\n",
    "\n",
    "for token in encoded_np:\n",
    "    current_name.append(token)\n",
    "    if token == end_of_name_encode:\n",
    "        if len(current_name) > 1:  # 至少两个字符才能形成 X/Y\n",
    "            X = current_name[:-1]\n",
    "            Y = current_name[1:]\n",
    "            names_X.append(X)\n",
    "            names_Y.append(Y)\n",
    "        current_name = []\n",
    "\n",
    "\n",
    "current_name.append(end_of_name_encode)\n",
    "if len(current_name) > 1:\n",
    "    X = current_name[:-1]\n",
    "    Y = current_name[1:]\n",
    "    names_X.append(X)\n",
    "    names_Y.append(Y)\n",
    "    current_name = []\n",
    "\n",
    "names_X[:5], names_Y[:5]\n",
    "# 转成 TensorFlow 张量\n",
    "X_dataset = tf.ragged.constant(names_X)\n",
    "X_dataset\n",
    "Y_dataset = tf.ragged.constant(names_Y)\n",
    "Y_dataset\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_dataset, Y_dataset)).shuffle(1000).batch(8)\n",
    "print(\"共提取名字数:\", len(names_X))\n"
   ],
   "id": "f5b11ea33082d4a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共提取名字数: 1536\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:02:57.620113Z",
     "start_time": "2025-10-13T08:02:38.883547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# history = model.fit(X_dataset, Y_dataset, batch_size=1, epochs=10)\n",
    "model.fit(train_set, epochs=20)"
   ],
   "id": "e96aa6e901b201d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "192/192 [==============================] - 3s 4ms/step - loss: 2.4416 - accuracy: 0.3078\n",
      "Epoch 2/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.9805 - accuracy: 0.4220\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.8669 - accuracy: 0.4487\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.7963 - accuracy: 0.4662\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.7508 - accuracy: 0.4783\n",
      "Epoch 6/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.7099 - accuracy: 0.4866\n",
      "Epoch 7/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.6760 - accuracy: 0.4983\n",
      "Epoch 8/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.6460 - accuracy: 0.5024\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.6203 - accuracy: 0.5163\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5969 - accuracy: 0.5203\n",
      "Epoch 11/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5747 - accuracy: 0.5281\n",
      "Epoch 12/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5509 - accuracy: 0.5349\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5320 - accuracy: 0.5420\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5098 - accuracy: 0.5459\n",
      "Epoch 15/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4904 - accuracy: 0.5522\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4695 - accuracy: 0.5584\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4525 - accuracy: 0.5642\n",
      "Epoch 18/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4326 - accuracy: 0.5684\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4157 - accuracy: 0.5749\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.3951 - accuracy: 0.5816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20321972560>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:03:37.075421Z",
     "start_time": "2025-10-13T08:03:36.969948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dino_name_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X -2),\n",
    "    model\n",
    "])"
   ],
   "id": "c4e238724c5a6c7",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:03:46.082733Z",
     "start_time": "2025-10-13T08:03:46.070616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_next_char(model, text, temperature=1):\n",
    "    y_proba = model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "521b1157784271da",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:05:21.427051Z",
     "start_time": "2025-10-13T08:05:21.415881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        char_gen_next = my_next_char(dino_name_model, text, temperature)\n",
    "        if char_gen_next == \"\\n\":\n",
    "            break\n",
    "        text += my_next_char(dino_name_model, text, temperature)\n",
    "    return text"
   ],
   "id": "2f3eb39386eea6e6",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:07:06.179675Z",
     "start_time": "2025-10-13T08:07:04.878344Z"
    }
   },
   "cell_type": "code",
   "source": "my_extend_text(\"xx\")",
   "id": "3c660e0f433513ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xxuwusaurus'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 有状态的RNN",
   "id": "9e06daa29a61f7ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "刚才的模型无法学习长度超100个字符的模式，使用有状态的RNN可以学习更长序列。\n",
    "\n",
    "到目前为止，只使用了无状态RNN：在每次训练迭代中，模型从一个全是零的隐藏状态开始，然后在每个时间步更新这个状态，在最后一个时间步之后，将其丢弃，因为不再需要它了。\n",
    "\n",
    "如果指示RNN在处理完一个训练批次后保留该最终状态，并将其用作下一个训练批次的初始状态，那么模型可以学习长期模式，尽管只通过短序列进行反向传播。这就是所谓的有状态RNN。\n",
    "\n",
    "首先需要注意的是，只有在批次中的每个输入序列都从相应的上一批次序列结束的位置开始时才能建立有状态RNN。因此，构建有状态RNN时，首先需要使用非重叠顺序输入序列（而不是用于训练无状态RNN的随机重叠序列）。当创建tf.data.Dataset时，需要在调用window()方法时使用shift=length（而不是shift=1）。此外，不能调用shuffle()方法。\n",
    "\n",
    "在为有状态RNN准备数据集时，批处理要比无状态RNN时更困难。如果调用batch(32)，那么32个连续的窗口将被放入同一个批次中，接下来的批次将无法继续从这些窗口的最后一个位置开始。第一个批次将包含窗口1到32，第二个批次将包含窗口33到64，因此如果考虑每个批次的第一个窗口（即窗口1和33），便会发现它们不是连续的。这个问题最简单的解决方案就是只使用批量大小1。下面的to_dataset_for_stateful_rnn()自定义实用函数使用这种策略来为有状态RNN准备数据集：\n",
    "\n",
    "![为有状态RNN准备连续序列片段的数据集](./images/RNN/p10.png)"
   ],
   "id": "24f6d17019e9f4e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T02:49:03.292297Z",
     "start_time": "2026-01-22T02:49:03.266450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length+1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(length+1)).batch(1)\n",
    "\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ],
   "id": "8c154518ccd8ce51",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T02:49:35.946376Z",
     "start_time": "2026-01-22T02:49:35.463592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "length = 100  # length决定循环神经网络能学习的最长模式"
   ],
   "id": "fd1b13f932832e8a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T02:49:36.772540Z",
     "start_time": "2026-01-22T02:49:36.673283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000], length)\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
   ],
   "id": "ce27f8cbb33301eb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在创建有状态RNN。当创建每个循环层时，需要将stateful参数设置True，因为有状态RNN需要知道批量大小（因为它将为批处理中的每个输入序列保留状态）。因此，必须在第一层设置batch_input_shape参数。请注意，可以不指定第二维度，因为输入序列可以具有任意长度",
   "id": "93b7f2d0f6721a27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T02:49:38.007064Z",
     "start_time": "2026-01-22T02:49:37.810827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16, batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])"
   ],
   "id": "5e5112b3f85553bd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T02:49:38.461829Z",
     "start_time": "2026-01-22T02:49:38.446213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 每个轮次结束时，需要在返回到文本开头之前重置状态\n",
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ],
   "id": "793edb9f4685aa75",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:13:26.607642Z",
     "start_time": "2026-01-22T02:49:39.117113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True\n",
    ")\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(stateful_train_set, validation_data=stateful_valid_set, epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
   ],
   "id": "80f44cafb4ee31e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   9996/Unknown - 144s 14ms/step - loss: 1.8666 - accuracy: 0.4508INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 147s 15ms/step - loss: 1.8665 - accuracy: 0.4508 - val_loss: 1.7065 - val_accuracy: 0.4911\n",
      "Epoch 2/10\n",
      "9995/9999 [============================>.] - ETA: 0s - loss: 1.5619 - accuracy: 0.5286INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 139s 14ms/step - loss: 1.5618 - accuracy: 0.5287 - val_loss: 1.6223 - val_accuracy: 0.5136\n",
      "Epoch 3/10\n",
      "9996/9999 [============================>.] - ETA: 0s - loss: 1.4829 - accuracy: 0.5491INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 140s 14ms/step - loss: 1.4828 - accuracy: 0.5492 - val_loss: 1.5828 - val_accuracy: 0.5251\n",
      "Epoch 4/10\n",
      "9998/9999 [============================>.] - ETA: 0s - loss: 1.4440 - accuracy: 0.5594INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 144s 14ms/step - loss: 1.4440 - accuracy: 0.5594 - val_loss: 1.5596 - val_accuracy: 0.5325\n",
      "Epoch 5/10\n",
      "9998/9999 [============================>.] - ETA: 0s - loss: 1.4202 - accuracy: 0.5652INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 143s 14ms/step - loss: 1.4201 - accuracy: 0.5652 - val_loss: 1.5483 - val_accuracy: 0.5365\n",
      "Epoch 6/10\n",
      "9998/9999 [============================>.] - ETA: 0s - loss: 1.4037 - accuracy: 0.5694INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 142s 14ms/step - loss: 1.4037 - accuracy: 0.5694 - val_loss: 1.5435 - val_accuracy: 0.5397\n",
      "Epoch 7/10\n",
      "9999/9999 [==============================] - ETA: 0s - loss: 1.3918 - accuracy: 0.5723INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 144s 14ms/step - loss: 1.3918 - accuracy: 0.5723 - val_loss: 1.5354 - val_accuracy: 0.5413\n",
      "Epoch 8/10\n",
      "9997/9999 [============================>.] - ETA: 0s - loss: 1.3825 - accuracy: 0.5746INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 144s 14ms/step - loss: 1.3825 - accuracy: 0.5746 - val_loss: 1.5321 - val_accuracy: 0.5439\n",
      "Epoch 9/10\n",
      "9999/9999 [==============================] - 142s 14ms/step - loss: 1.3754 - accuracy: 0.5766 - val_loss: 1.5354 - val_accuracy: 0.5434\n",
      "Epoch 10/10\n",
      "9999/9999 [==============================] - 141s 14ms/step - loss: 1.3704 - accuracy: 0.5778 - val_loss: 1.5290 - val_accuracy: 0.5436\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在训练后，只能使用与训练期间相同大小的批次进行预测。为避免此限制，请创建相同的无状态模型，并将有状态模型的权重复制到该模型\n",
    "\n",
    "有趣的是，尽管char-RNN模型只是被训练来预测下一个字符，但是这个看似简单的任务实际上要求它学习一些更高级的任务。例如，查找“Great movie，I really”的下一个字符时，了解该句子是正面的是有帮助的，因此接下来的字符可能是“l”［表示“喜欢”(loved)］而不是“h”［表示“讨厌”(hated)］\n",
    "\n",
    "OpenAI在一篇论文中描述了他们如何在大型数据集上训练大型char-RNN模型，并发现其中的一个神经元作为出色的情感分析分类器：尽管该模型在没有任何标签的情况下进行了训练，但“情感神经元”在情感分析基准测试中达到了最先进的性能。这预示并激发了NLP中无监督预训练的应用。\n",
    "\n",
    "虽然批处理更难，但也不是不可能的。例如，可以将莎士比亚的文本分成32个长度相等的文本，为每个文本创建一个连续的输入序列数据集，最后使用tf.data.Dataset.zip(datasets).map(lambda*windows：tf.stack(windows))创建适当的连续批次，批次中的第n个输入序列恰好从上一个批次中第n个输入序列结束的地方开始"
   ],
   "id": "861d00388c668d67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:15:00.893584Z",
     "start_time": "2026-01-22T03:15:00.877977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ],
   "id": "51cc05598579c6d5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:15:04.456422Z",
     "start_time": "2026-01-22T03:15:01.422214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stateless_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "stateless_model.build(tf.TensorShape([None, None]))\n",
    "stateless_model.set_weights(model.get_weights())\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    stateless_model\n",
    "])\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"to be or not to be\", temperature=0.01))"
   ],
   "id": "fbf0f3528ef8bb57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "to be or not to be a shall be a shall be a shall be a shall be a sha\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:15:13.323704Z",
     "start_time": "2026-01-22T03:15:13.217468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extra code – shows one way to prepare a batched dataset for a stateful RNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def to_non_overlapping_windows(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    return ds.flat_map(lambda window: window.batch(length + 1))\n",
    "\n",
    "def to_batched_dataset_for_stateful_rnn(sequence, length, batch_size=32):\n",
    "    parts = np.array_split(sequence, batch_size)\n",
    "    datasets = tuple(to_non_overlapping_windows(part, length) for part in parts)\n",
    "    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "list(to_batched_dataset_for_stateful_rnn(tf.range(20), length=3, batch_size=2))"
   ],
   "id": "d2b2c78db13e7d38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 0,  1,  2],\n",
       "         [10, 11, 12]])>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 1,  2,  3],\n",
       "         [11, 12, 13]])>),\n",
       " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 3,  4,  5],\n",
       "         [13, 14, 15]])>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 4,  5,  6],\n",
       "         [14, 15, 16]])>),\n",
       " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 6,  7,  8],\n",
       "         [16, 17, 18]])>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 7,  8,  9],\n",
       "         [17, 18, 19]])>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 情感分析",
   "id": "cb678ad5f24584ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "生成文本的任务既有趣又有教学意义，但在真实项目中，NLP最常用的应用之一是文本分类——特别是情感分析。如果MNIST数据集上的图像分类是计算机视觉的“Hello World！”，那么IMDb电影评论数据集上的情感分析就是自然语言处理的“HelloWorld！”。IMDb数据集包括50000条英文电影评论（25000条用于训练，25000条用于测试），这些评论是从著名的互联网电影数据库 (https://imdb.com) 中提取的，每条评论还有一个简单的二元目标值，用以指示每条评论是负面的(0)还是正面的(1)。就像MNIST一样，IMDb电影评论数据集受欢迎是有原因的：它足够简单，可以在合理的时间内在CPU上处理。",
   "id": "b81587ae3ba3e6ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:37:39.053702Z",
     "start_time": "2026-01-22T03:36:43.905118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "tf.random.set_seed(42)\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ],
   "id": "deb82c7f0f23f572",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\Administrator\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Administrator\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "332a0277ae62421eb50af99278c03723"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b064aa16cdf945cba6d07c87efed0bf1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "924248916db34e8481a6e4ac616fbec6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13610134b1b94ac488fdc4ee94dace58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Shuffling C:\\Users\\Administrator\\tensorflow_datasets\\imdb_reviews\\plain_text\\incomplete.LSDMPT_1.0.0\\imdb_revi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f891d063a334c3db9c853f8673dd1ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3595dca2b00f4a6aa2dbf687eed24856"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Shuffling C:\\Users\\Administrator\\tensorflow_datasets\\imdb_reviews\\plain_text\\incomplete.LSDMPT_1.0.0\\imdb_revi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "896c4a32890d4ab093d0aecc30372ad6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05c0ccd7e0f84412af58743f5463940d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Shuffling C:\\Users\\Administrator\\tensorflow_datasets\\imdb_reviews\\plain_text\\incomplete.LSDMPT_1.0.0\\imdb_revi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13c465fa83624f189182e6fb10b13b15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Administrator\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:37:39.146051Z",
     "start_time": "2026-01-22T03:37:39.098267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for review, label in raw_train_set.take(4):\n",
    "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
    "    print(\"Label:\", label.numpy())"
   ],
   "id": "2ef138190cb15704",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n",
      "Label: 0\n",
      "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n",
      "Label: 1\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了建立这个任务的模型，我们需要对文本进行预处理，但这一次我们将把文本拆分成单词而不是字符。为此，我们再次使用tf.keras.layers.TextVectorization层。请注意，它使用空格来确定单词边界，这在某些语言中不太合适。例如，汉语写作单词之间不使用空格，即使在英语中，空格也并不总是分词的最佳方式\n",
    "\n",
    "幸运的是，有解决这些问题的方法。在2016年的一篇论文中探讨了几种子词级别的文本分词和重组方法。这样，即使模型遇到了它以前从未见过的生僻词，它仍然可以合理地猜测它的含义。例如，即使模型在训练期间从未见过单词smartest，如果它学到了单词smart并且还学到了后缀est的意思是“最”，它仍然可以推断出smartest的含义。作者评估的技术之一是字节对编码(Byte Pair Encoding，BPE)。BPE的工作原理是将整个训练集拆分为单个字符（包括空格），然后反复合并最常见的相邻字符对，直到词汇达到所需的大小。（从最基础的字符级词表开始，逐步地将出现频率最高的字符对（pair）合并，形成越来越长的子词序列。）"
   ],
   "id": "c702577cc3ec9f1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": " ",
   "id": "fc60e25280f230cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
