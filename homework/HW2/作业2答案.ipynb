{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "数据沿用上课使用的数据，没有的可以下载   地址：https://github.com/yt-one/ml-teach/tree/main/datasets\n",
    "\n",
    "编程作业：\n",
    "\n",
    "0. 以下作业 要把周五课上的预处理全流程带上 （ColumnTransformer），可以在熟悉了课件里的代码后，考虑自己实现\n",
    "    实现要求：\n",
    "  a. 大多数ML算法不期望缺失值, 数字特征中的缺失值将通过用中位数替换它们来估算，。在分类特征中，缺失值将被最常见的类别替换。\n",
    "  b. 大多数ML算法只接受数字输入, 类别特征将被独热编码\n",
    "  c. 计算并添加一些比率特征：bedrooms_ratio、rooms_per_house和people_per_house。希望这些能更好地与房价中位数相关联\n",
    "  d. 添加集群相似性特征。可能比纬度和经度对模型更有用\n",
    "  e. 长尾特征被它们的对数取代，因为大多数模型更喜欢具有大致均匀分布或高斯分布的特征。\n",
    "  f. 大多数ML算法更喜欢所有特征具有大致相同的尺度, 所有数值特征都将被标准化\n",
    "\n",
    "\n",
    "1. 尝试支持向量机回归器(sklearn.svm.SVR)，用这个模型来做回归。\n",
    "     试试这个模型的超参数，例如kernel=\"linear\"，kernel=\"rbf\"，不同的kernel选择下也会有不同的超参数​。  分别用GridSearchCV和RandomizedSearchCV探索性能最优（交叉验证后的预测表现最好）的超参数\n",
    "\n",
    "     请注意，支持向量机不能扩展到大型数据集，因此应该仅在训练集的前5000个实例上训练你的模型，并且仅使用3折交叉验证，否则会要运行很久（按小时计）。\n",
    "     现在不要担心支持向量机超参数的含义，将在讲支持向量机（SVM）的时候详解\n",
    "\n",
    "\n",
    "2. 去了解sklearn里 SelectFromModel的用法，尝试在数据预处理流水线中添加一个SelectFromModel转换器来仅选择最重要的属性。 并用你想尝试的回归模型去训练数据（线性回归/决策树/随机森林）\n",
    "\n",
    "\n",
    "3. 随堂练习: 创建一个自定义转换器，在其fit()方法中训练k近邻回归器(sklearn.neighbors.KNeighborsRegressor)，并在其transform()方法中输出模型的预测。然后将此功能添加到预处理流水线，使用纬度和经度作为此转换器的输入。这将在模型中添加一个与最近地区的房价中位数相对应的特征。  添加特征后，用想尝试的回归模型去训练数据。 训练可以采用GridSearchCV和RandomizedSearchCV探索性能最优（交叉验证后的预测表现最好）的超参数\n",
    "\n",
    "\n",
    "\n",
    "4. 从头开始再次实现StandardScalerClone类（对数据做标准化），然后添加对inverse_transform()方法的支持：执行scaler.inverse_transform(scaler.fit_transform(X))应该返回一个非常接近X的数组。\n",
    "       然后添加对特征名称的支持：如果输入是DataFrame，则在fit()方法中设置feature_names_in_。该属性类型是NumPy数组，存列的名字。\n",
    "       最后，实现get_feature_names_out()方法：这个方法应该有一个可选的input_features=None参数。如果传了这个参数，这个方法应检查其长度是否匹配n_features_in_，如果有feature_names_in_属性，则应匹配feature_names_in_的长度，然后应返回input_features。\n",
    "       如果input_features为None，则该方法应返回feature_names_in_（如果有这个属性），否则返回长度为n_features_in_的   np.array([\"x0\"，\"x1\"，\"x2\", ...])。"
   ],
   "id": "46c291588215527c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:27:27.587240Z",
     "start_time": "2025-08-05T06:27:25.451190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 题目0\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out （输出特征的名字）\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),  # one-to-one: 1比1映射\n",
    "    StandardScaler())\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        # KMeans估计器相关参数：集群数量，随机种子，KMeans是一个随机算法，依赖随机性来定位集群\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
    "                              random_state=self.random_state)\n",
    "\n",
    "        # sample_weight可指定样本的相对权重, 属于KMeans算法里的超参数，训练前指定。\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self # 永远返回self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # self.kmeans_.cluster_centers_ 集群中心的位置\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
    "\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]),\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # remainder，剩下的列用什么转换器，现在就剩下housing_median_age"
   ],
   "id": "8b3169efabdead0b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:02:00.602216Z",
     "start_time": "2025-08-04T14:56:57.121066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 题目1\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import expon, loguniform,randint\n",
    "\n",
    "housing = pd.read_csv(\"../../datasets/housing/housing.csv\")\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)  # 分层采样 单次拆分\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"svr\", SVR()),\n",
    "])\n",
    "\n",
    "housing_5000 = housing.iloc[:5000]\n",
    "housing_labels_5000 = housing_labels.iloc[:5000]\n",
    "\n",
    "\n",
    "#  随即搜索超参数，可以给一个概率分布，下面就是例子\n",
    "param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n",
    "                  'svr__kernel': ['linear', 'rbf'],\n",
    "                  'svr__C': loguniform(20, 200000), # loguniform(20, 200000) 表示不是在 [20, 200000] 区间内均匀采样，而是在 log(C) 这一对数空间中均匀采样。\n",
    "                  'svr__gamma': expon(scale=1),}    # 表示从以 1 为尺度参数的指数分布中采样，概率密度函数为 (e^(-x)),这个分布在采样较小值时概率更高，可以更精细地探索低 gamma 区域，但也允许偶尔采样较大的值。\n",
    "\n",
    "svr_rnd_search = RandomizedSearchCV(\n",
    "    svr_pipeline, param_distributions=param_distribs, n_iter=50, cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42)  # 3折交叉验证，50组超参数探索\n",
    "\n",
    "svr_rnd_search.fit(housing_5000, housing_labels_5000)\n",
    "print(svr_rnd_search.best_params_)\n",
    "print(svr_rnd_search.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "#   超参数之间有依赖，下面的网格展示 不同的依赖之间怎么进行搜索\n",
    "#    kernel=linear，我只用探索C参数\n",
    "#    kernel=rbf，  我需要探索 gamma参数\n",
    "param_grid = [\n",
    "        {'svr__kernel': ['linear'],\n",
    "         'svr__C': [10., 30., 100., 300., 1000.,3000., 10000., 30000.0]},\n",
    "\n",
    "        {'svr__kernel': ['rbf'],\n",
    "         'svr__C': [1.0, 3.0, 10., 30., 100., 300.,1000.0],\n",
    "         'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "svr_grid_search = GridSearchCV(svr_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')\n",
    "svr_grid_search.fit(housing_5000, housing_labels_5000)\n",
    "\n",
    "print(svr_grid_search.best_params_)\n",
    "print(svr_grid_search.best_score_)\n"
   ],
   "id": "a6119e415f44b7eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preprocessing__geo__n_clusters': 30, 'svr__C': 157055.10989448498, 'svr__gamma': 0.26497040005002437, 'svr__kernel': 'rbf'}\n",
      "-55179.352982304925\n",
      "{'svr__C': 10000.0, 'svr__kernel': 'linear'}\n",
      "-69132.8010560461\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 题目2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "preprocessing_add_feature_selection = make_pipeline(preprocessing, SelectFromModel(estimator=RandomForestRegressor(random_state=42)))\n",
    "preprocessing_add_feature_selection.fit(housing, housing_labels)\n",
    "\n",
    "preprocessing_add_feature_selection.get_feature_names_out()\n",
    "\n",
    "\n",
    "# 验证上面的，就可以把 preprocessing_add_feature_selection + 回归模型 组成一个新的流水线了，然后去训练数据\n"
   ],
   "id": "a68562f314bfda8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:06:57.413136Z",
     "start_time": "2025-08-04T15:06:57.389091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 题目3\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "class NearestLabel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "       pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        check_array(X)\n",
    "        # k个最邻近邻居的回归器\n",
    "        self.knn_ = KNeighborsRegressor(weights=\"distance\")\n",
    "        self.knn_.fit(X, y)\n",
    "        return self # 永远返回self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        result = self.knn_.predict(X)\n",
    "        if result.ndim == 1:\n",
    "            result = result.reshape(-1, 1)\n",
    "        return result\n",
    "\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [\"Nearest Median Housing Price\"]\n"
   ],
   "id": "265b25f56d3458bb",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:27:31.601996Z",
     "start_time": "2025-08-05T06:27:31.559620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#  这个部分 仅仅在验证类的转换是否征程\n",
    "knn_reg = NearestLabel()\n",
    "geo_features = housing[[\"latitude\", \"longitude\"]]\n",
    "knn_reg.fit_transform(geo_features, housing_labels)\n",
    "\n",
    "\n",
    "\n",
    "preprocessing.transformers  # (名字, 转换器, 列名列表)\n",
    "\n",
    "\n",
    "\n",
    "# 下面的代码 就在展示如何把 自定义的转换器 加到预处理流程里\n",
    "from sklearn.base import clone\n",
    "#\n",
    "transformers = [(name, clone(transformer), columns)\n",
    "                for name, transformer, columns in preprocessing.transformers]\n",
    "geo_index = [name for name, _, _ in transformers].index(\"geo\")  # \"geo\"这个名字在列表内出现的索引\n",
    "\n",
    "transformers[geo_index] = (\"geo\", knn_reg, [\"latitude\", \"longitude\"])\n",
    "#\n",
    "new_geo_preprocessing = ColumnTransformer(transformers)\n",
    "#\n",
    "new_geo_pipeline = Pipeline([\n",
    "    ('preprocessing', new_geo_preprocessing),\n",
    "\n",
    "    #  下面的回归模型，直接用了随机搜索的最佳超参数\n",
    "    ('svr', SVR(C=svr_rnd_search.best_params_[\"svr__C\"],\n",
    "                gamma=svr_rnd_search.best_params_[\"svr__gamma\"],\n",
    "                kernel=svr_rnd_search.best_params_[\"svr__kernel\"])),\n",
    "])\n",
    "#\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "new_geo_pipeline.fit(housing_5000, housing_labels_5000)  # 选完超参数，开始训练\n",
    "my_predictions = new_geo_pipeline.predict(X_test)  # 在测试集上验证 训练的参数\n",
    "\n",
    "my_rmse = mean_squared_error(y_test, my_predictions, squared=False)\n",
    "print(my_rmse)\n",
    "# mean_squared_error(housing_labels_5000, new_geo_pipeline.predict(housing_5000), squared=False)"
   ],
   "id": "e81ec54066d2aa3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bedrooms',\n",
       "  Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                  ('functiontransformer',\n",
       "                   FunctionTransformer(feature_names_out=<function ratio_name at 0x00000247DD312B00>,\n",
       "                                       func=<function column_ratio at 0x00000247D97804C0>)),\n",
       "                  ('standardscaler', StandardScaler())]),\n",
       "  ['total_bedrooms', 'total_rooms']),\n",
       " ('rooms_per_house',\n",
       "  Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                  ('functiontransformer',\n",
       "                   FunctionTransformer(feature_names_out=<function ratio_name at 0x00000247DD312B00>,\n",
       "                                       func=<function column_ratio at 0x00000247D97804C0>)),\n",
       "                  ('standardscaler', StandardScaler())]),\n",
       "  ['total_rooms', 'households']),\n",
       " ('people_per_house',\n",
       "  Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                  ('functiontransformer',\n",
       "                   FunctionTransformer(feature_names_out=<function ratio_name at 0x00000247DD312B00>,\n",
       "                                       func=<function column_ratio at 0x00000247D97804C0>)),\n",
       "                  ('standardscaler', StandardScaler())]),\n",
       "  ['population', 'households']),\n",
       " ('log',\n",
       "  Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                  ('functiontransformer',\n",
       "                   FunctionTransformer(feature_names_out='one-to-one',\n",
       "                                       func=<ufunc 'log'>)),\n",
       "                  ('standardscaler', StandardScaler())]),\n",
       "  ['total_bedrooms',\n",
       "   'total_rooms',\n",
       "   'population',\n",
       "   'households',\n",
       "   'median_income']),\n",
       " ('geo', ClusterSimilarity(random_state=42), ['latitude', 'longitude']),\n",
       " ('cat',\n",
       "  Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='most_frequent')),\n",
       "                  ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "  <sklearn.compose._column_transformer.make_column_selector at 0x247dd4cdba0>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:07:01.191574Z",
     "start_time": "2025-08-04T15:07:01.167973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 练习题5：\n",
    "# fit()方法中设置feature_names_in_。该属性类型是NumPy数组，存列的名字。\n",
    "#        最后，实现get_feature_names_out()方法：这个方法应该有一个可选的input_features=None参数。如果传了这个参数，这个方法应检查其长度是否匹配n_features_in_，如果有feature_names_in_属性，则应匹配feature_names_in_的长度，然后应返回input_features。\n",
    "#        如果input_features为None，则该方法应返回feature_names_in_（如果有这个属性），否则返回长度为n_features_in_的   np.array([\"x0\"，\"x1\"，\"x2\", ...])。\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # 没有 *args, **kwargs, sklearn的规范\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # 即使不用y，也需要它\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_in_ = X.columns.values\n",
    "\n",
    "        X = check_array(X)  # 检查X是不是数组\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # 所有估计器会把输入特征的数量存下来\n",
    "        return self  # 永远返回 self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # 检查是否适配过数据 （检查是否有那些下划线结尾的参数)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]  # 检查训练时的输入特征数量 和 要transofrm的时候的输入特征数量是不是一样的\n",
    "        if self.with_mean:  # 为什么有这个参数？ 可以控制 稀疏矩阵的标准化，希望稀疏矩阵标准化后 还是稀疏的\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_\n",
    "\n",
    "    def inverse_transform(self, X_transformed):\n",
    "        check_is_fitted(self)\n",
    "        X_transformed = check_array(X_transformed)\n",
    "        assert self.n_features_in_ == X_transformed.shape[1]\n",
    "\n",
    "        X = X_transformed * self.scale_\n",
    "        if self.with_mean:\n",
    "            X = X + self.mean_\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features:\n",
    "            assert len(input_features) == len(self.feature_names_in_) if hasattr(self, \"feature_names_in_\") else len(input_features) == self.n_features_in_\n",
    "            return input_features\n",
    "        return self.feature_names_in_ if hasattr(self, \"feature_names_in_\") else np.array([f\"x{i}\" for i in range(self.n_features_in_)])\n",
    "\n"
   ],
   "id": "8ec6c2c97cb3e910",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:07:08.225417Z",
     "start_time": "2025-08-04T15:07:08.187874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "X = imputer.fit_transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)\n",
    "\n",
    "ssc = StandardScalerClone(with_mean=False)\n",
    "\n",
    "new_housing = ssc.fit_transform(housing_tr)\n",
    "np.ptp(new_housing, axis=0)\n",
    "np.ptp(X, axis=0)\n",
    "\n",
    "new_X = ssc.inverse_transform(new_housing)\n",
    "np.allclose(X, new_X)\n",
    "\n",
    "ssc.fit(housing_tr)\n",
    "ssc.get_feature_names_out()"
   ],
   "id": "a46c27c7ecde4e2c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2844c9b42e33c49a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
