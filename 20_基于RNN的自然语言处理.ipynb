{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用字符RNN生成莎士比亚文本",
   "id": "b6905ec0806fdce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在2015年的一篇著名博客文章 \"https://karpathy.github.io/2015/05/21/rnn-effectiveness/\" 中，Andrej Karpathy展示了如何训练循环神经网络来预测句子中的下一个字符。这个char-RNN可以用来生成小说文本，每次一个字符。",
   "id": "bdf100365ed041e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-13T05:56:50.760255Z",
     "start_time": "2025-10-13T05:56:44.839218Z"
    }
   },
   "source": [
    "# tf.keras.utils.get_file()函数下载莎士比亚的所有作品\n",
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:57:28.865632Z",
     "start_time": "2025-10-13T05:57:28.857788Z"
    }
   },
   "cell_type": "code",
   "source": "print(shakespeare_text[:80])",
   "id": "5a0427af95a99073",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:27.569769Z",
     "start_time": "2025-10-13T06:24:26.907345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用tf.keras.layers.TextVectorization层对此文本进行编码。我们将设置split=\"character\"，基于字符进行编码，而不是默认的基于单词进行编码，并使用standardize=\"lower\"将文本转换为小写（这将简化任务）：\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "\n",
    "\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "encoded\n"
   ],
   "id": "a051c0bf83bb06c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:28.765374Z",
     "start_time": "2025-10-13T06:24:28.754856Z"
    }
   },
   "cell_type": "code",
   "source": "text_vec_layer.get_vocabulary()",
   "id": "fc62a9636f6ee7f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " ' ',\n",
       " 'e',\n",
       " 't',\n",
       " 'o',\n",
       " 'a',\n",
       " 'i',\n",
       " 'h',\n",
       " 's',\n",
       " 'r',\n",
       " 'n',\n",
       " '\\n',\n",
       " 'l',\n",
       " 'd',\n",
       " 'u',\n",
       " 'm',\n",
       " 'y',\n",
       " 'w',\n",
       " ',',\n",
       " 'c',\n",
       " 'f',\n",
       " 'g',\n",
       " 'b',\n",
       " 'p',\n",
       " ':',\n",
       " 'k',\n",
       " 'v',\n",
       " '.',\n",
       " \"'\",\n",
       " ';',\n",
       " '?',\n",
       " '!',\n",
       " '-',\n",
       " 'j',\n",
       " 'q',\n",
       " 'x',\n",
       " 'z',\n",
       " '3',\n",
       " '&',\n",
       " '$']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:32.555436Z",
     "start_time": "2025-10-13T06:24:32.545488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 每个字符现在都映射到一个整数，从2开始。TextVectorization层将值0保留为填充词元(token)，并将值1保留为未知字符。\n",
    "# 现在不需要这两个词元，因此从字符ID数中减去2，并计算不同字符的总数和总字符数：\n",
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(n_tokens)"
   ],
   "id": "8e7d446d727dc90e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将这个非常长的序列转换成一个窗口数据集，然后使用它来训练一个序列到序列的循环神经网络。目标序列将类似于输入序列，但是会向“未来”移动一个时间步。例如，数据集中的样本可能是一个由表示文本“to be or not to b”（不包括最后一个“e”）的字符ID组成的序列，相应的目标序列是一个由表示文本“o be or not to be”（包括最后一个“e”，但不包括开头的“t”）的字符ID组成的序列",
   "id": "6f9c13d7635eb999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:23:47.007242Z",
     "start_time": "2025-10-13T06:23:46.995673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将字符ID序列转换成输入/目标窗口对的数据集\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ],
   "id": "70ed9e8982001bf0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 它将序列作为输入（即编码文本），并创建一个包含所需长度的所有窗口的数据集。\n",
    "- 它将长度增加1，因为我们需要将下一个字符放在目标序列中。\n",
    "- 然后，它对窗口进行乱序处理（可选），将它们分批，将它们拆分为输入/输出对，并激活预取功能。\n",
    "\n",
    "长度为11的窗口和批量大小3。每个窗口的起始索引都显示在旁边：\n",
    "\n",
    "![准备乱序窗口的数据集](./images/RNN/p7.png)"
   ],
   "id": "205152dc0a2c913e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:39.614518Z",
     "start_time": "2025-10-13T06:24:39.509151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 大约90%的文本进行训练，5%用于验证，5%用于测试\n",
    "length = 100  # length决定循环神经网络能学习的最长模式\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000: 1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ],
   "id": "414ec5b323fe4c3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:24:40.485396Z",
     "start_time": "2025-10-13T06:24:40.365750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for data in valid_set.take(1):\n",
    "    print(data)"
   ],
   "id": "2198babd695e6b80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 5,  7,  0, ...,  6,  1,  0],\n",
      "       [ 7,  0,  8, ...,  1,  0, 18],\n",
      "       [ 0,  8,  1, ...,  0, 18,  6],\n",
      "       ...,\n",
      "       [ 4,  2,  0, ...,  3, 26, 10],\n",
      "       [ 2,  0,  7, ..., 26, 10, 10],\n",
      "       [ 0,  7,  6, ..., 10, 10,  2]], dtype=int64)>, <tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 7,  0,  8, ...,  1,  0, 18],\n",
      "       [ 0,  8,  1, ...,  0, 18,  6],\n",
      "       [ 8,  1,  4, ..., 18,  6,  3],\n",
      "       ...,\n",
      "       [ 2,  0,  7, ..., 26, 10, 10],\n",
      "       [ 0,  7,  6, ..., 10, 10,  2],\n",
      "       [ 7,  6,  1, ..., 10,  2,  8]], dtype=int64)>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 构建和训练char-RNN模型",
   "id": "ce01571b7082bc92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 使用Embedding层作为第一层，以编码字符ID。Embedding层的输入维数是不同字符ID的数量，输出维数是可以调整的超参数——现在将其设置为16。Embedding层的输入将是形状为［批量大小，窗口长度］的二维张量，Embedding层的输出将是形状为［批量大小，窗口长度，嵌入大小］的三维张量。\n",
    "- 为输出层使用Dense层：它必须具有39个单元(n_tokens)，因为文本中有39个不同的字符，希望在每个时间步输出每个可能字符的概率。每个时间步39个输出概率的总和应该为1，因此对Dense层的输出应用softmax激活函数。\n",
    "- 最后，使用\"sparse_categorical_crossentropy\"损失和Nadam优化器编译此模型，并使用ModelCheckpoint回调函数在训练过程中保存最佳模型（以验证精度为标准）进行多个轮次的训练。"
   ],
   "id": "bbeb76c3ffa2e1ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:11:43.648220Z",
     "start_time": "2025-10-13T07:11:43.068519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 不用简单RNN,用带有长短记忆的GRU\n",
    "#\n",
    "# [[ 5,  7,  0, ...,  6,  1,  0]]  1 * 100\n",
    "# [[ v5  v7  v0  .... v6  v1  v0]] 1 * 100 * 16\n",
    "\n",
    "# n_tokens:39\n",
    "# embedding： 39 * n 列的矩阵：  5行：  [w0, w1, ... wn-1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
   ],
   "id": "ddb8c8daf6234f32",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:14:49.242137Z",
     "start_time": "2025-10-13T07:14:48.979220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练后搭建最终模型\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X-2), # 不使用填充词元(0)  和 未知词元 (1)\n",
    "    model\n",
    "])"
   ],
   "id": "4a9cd3d1b9707e25",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:19:35.453464Z",
     "start_time": "2025-10-13T07:19:35.333035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to be\"])[0, -1]\n",
    "y_proba\n",
    "y_pred = tf.argmax(y_proba)  # 选择概率最高的字母ID\n",
    "\n",
    "print(text_vec_layer.get_vocabulary())\n",
    "\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ],
   "id": "660fc80a64c43c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "['', '[UNK]', ' ', 'e', 't', 'o', 'a', 'i', 'h', 's', 'r', 'n', '\\n', 'l', 'd', 'u', 'm', 'y', 'w', ',', 'c', 'f', 'g', 'b', 'p', ':', 'k', 'v', '.', \"'\", ';', '?', '!', '-', 'j', 'q', 'x', 'z', '3', '&', '$']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 生成莎士比亚文本",
   "id": "97a339785e97f31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "要使用char-RNN模型生成新文本，可以将一些文本输入模型，让模型预测最有可能的下一个字母，将其添加到文本的末尾，然后将扩展后的文本输入模型来猜测下一个字母，以此类推。这叫作贪婪解码。但是在实践中，这往往导致相同的单词一遍又一遍地重复。\n",
    "\n",
    "相反，可以使用TensorFlow的tf.random.categorical()函数随机采样下一个字符，采样概率等于估计概率。这将生成更多样化和有趣的文本。categorical()函数在给定类别对数概率(logits)的情况下，对随机类别指数进行采样。"
   ],
   "id": "c5d1bcc028d8d3e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:36:14.884959Z",
     "start_time": "2025-10-13T07:36:14.873932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n = 2000\n",
    "tf.random.categorical(log_probas, num_samples=n)\n",
    "\n"
   ],
   "id": "89b01494486ce3e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2000), dtype=int64, numpy=array([[0, 1, 0, ..., 1, 0, 2]], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了更好地控制生成文本的多样性，可以将logits（对数概率）除以一个称为“温度”的数字，这个数字可以根据需求进行调整。“温度”接近零将更偏向于高概率字符，而较高“温度”则会使所有字符获得相同的概率。通常在生成相对严谨和精确的文本（例如数学公式）时，较低的“温度”更为适用，而在生成更多样化且有创意的文本时，则适合用较高的“温度”。",
   "id": "aefc5c6f8577915f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:36:16.894325Z",
     "start_time": "2025-10-13T07:36:16.882005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "b358a7b630203dce",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:36:17.641702Z",
     "start_time": "2025-10-13T07:36:17.628640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ],
   "id": "fbd43a4da8c0dd45",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:36:23.437635Z",
     "start_time": "2025-10-13T07:36:19.758035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"To be or not to be\", temperature=1))\n"
   ],
   "id": "786e75ebe35d87e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "To be or not to bedf3zwvcik :ua!&q. :phgr&;ubltcpzhp:'rv:cq3z!$ pau:\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:47:50.974144Z",
     "start_time": "2025-10-10T13:47:48.297565Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=1))",
   "id": "ccb5b2f788da0fc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "To be or not to be good and wrong; but come. though which the fine.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:48:15.816518Z",
     "start_time": "2025-10-10T13:48:13.225736Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=100))",
   "id": "d36fa2f1a2ba4d56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "To be or not to bef ,mt'&ozfpady-$\n",
      "wh!nse?pws3ert--vgerdjw?c-y-ewxnj\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模拟生成莎士比亚文本的流程 生成名字",
   "id": "c69b7c36365a237d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:38:55.207827Z",
     "start_time": "2025-10-13T07:38:55.195726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"datasets/dino.txt\", \"r\") as f:\n",
    "    dino_names = f.read()\n",
    "    dino_names = dino_names.lower()"
   ],
   "id": "8c9db58ee8770353",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:39:27.978605Z",
     "start_time": "2025-10-13T07:39:27.812047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([dino_names])\n",
    "encoded = text_vec_layer([dino_names])[0]\n",
    "encoded"
   ],
   "id": "82a72b78a106c110",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19909,), dtype=int64, numpy=array([ 2,  2, 15, ...,  4,  4, 12], dtype=int64)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:39:41.911362Z",
     "start_time": "2025-10-13T07:39:41.900342Z"
    }
   },
   "cell_type": "code",
   "source": "text_vec_layer.get_vocabulary()",
   "id": "1febe554f24a9100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'a',\n",
       " 's',\n",
       " 'u',\n",
       " 'o',\n",
       " 'r',\n",
       " '\\n',\n",
       " 'n',\n",
       " 'i',\n",
       " 'e',\n",
       " 't',\n",
       " 'l',\n",
       " 'p',\n",
       " 'h',\n",
       " 'c',\n",
       " 'g',\n",
       " 'd',\n",
       " 'm',\n",
       " 'y',\n",
       " 'b',\n",
       " 'k',\n",
       " 'v',\n",
       " 'x',\n",
       " 'z',\n",
       " 'j',\n",
       " 'w',\n",
       " 'f',\n",
       " 'q']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:42:01.180282Z",
     "start_time": "2025-10-13T07:42:01.162135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "n_tokens"
   ],
   "id": "340430d9880913e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T07:43:02.164675Z",
     "start_time": "2025-10-13T07:43:02.153141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "end_of_name_encode = text_vec_layer([\"\\n\"])[0,0].numpy() - 2\n",
    "end_of_name_encode"
   ],
   "id": "8019902ca41e0a37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:02:38.365281Z",
     "start_time": "2025-10-13T08:02:38.259717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把 Tensor 转成 numpy 数组\n",
    "encoded_np = encoded.numpy()\n",
    "\n",
    "# 存放 (X, Y) 对\n",
    "names_X = []  # [第一个名字不包括换行符， 第二个名字不包括换行符， ...  最后一个名字不包括换行符]\n",
    "names_Y = []  # [第一个名字有换行符，但无第一个字符， .....    , ...  最后一个名字有换行符，但无第一个字符]\n",
    "\n",
    "# 临时缓冲区\n",
    "current_name = []\n",
    "\n",
    "for token in encoded_np:\n",
    "    current_name.append(token)\n",
    "    if token == end_of_name_encode:\n",
    "        if len(current_name) > 1:  # 至少两个字符才能形成 X/Y\n",
    "            X = current_name[:-1]\n",
    "            Y = current_name[1:]\n",
    "            names_X.append(X)\n",
    "            names_Y.append(Y)\n",
    "        current_name = []\n",
    "\n",
    "\n",
    "current_name.append(end_of_name_encode)\n",
    "if len(current_name) > 1:\n",
    "    X = current_name[:-1]\n",
    "    Y = current_name[1:]\n",
    "    names_X.append(X)\n",
    "    names_Y.append(Y)\n",
    "    current_name = []\n",
    "\n",
    "names_X[:5], names_Y[:5]\n",
    "# 转成 TensorFlow 张量\n",
    "X_dataset = tf.ragged.constant(names_X)\n",
    "X_dataset\n",
    "Y_dataset = tf.ragged.constant(names_Y)\n",
    "Y_dataset\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_dataset, Y_dataset)).shuffle(1000).batch(8)\n",
    "print(\"共提取名字数:\", len(names_X))\n"
   ],
   "id": "f5b11ea33082d4a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共提取名字数: 1536\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:02:57.620113Z",
     "start_time": "2025-10-13T08:02:38.883547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# history = model.fit(X_dataset, Y_dataset, batch_size=1, epochs=10)\n",
    "model.fit(train_set, epochs=20)"
   ],
   "id": "e96aa6e901b201d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "192/192 [==============================] - 3s 4ms/step - loss: 2.4416 - accuracy: 0.3078\n",
      "Epoch 2/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.9805 - accuracy: 0.4220\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.8669 - accuracy: 0.4487\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.7963 - accuracy: 0.4662\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.7508 - accuracy: 0.4783\n",
      "Epoch 6/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.7099 - accuracy: 0.4866\n",
      "Epoch 7/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.6760 - accuracy: 0.4983\n",
      "Epoch 8/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.6460 - accuracy: 0.5024\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.6203 - accuracy: 0.5163\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5969 - accuracy: 0.5203\n",
      "Epoch 11/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5747 - accuracy: 0.5281\n",
      "Epoch 12/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5509 - accuracy: 0.5349\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5320 - accuracy: 0.5420\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.5098 - accuracy: 0.5459\n",
      "Epoch 15/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4904 - accuracy: 0.5522\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4695 - accuracy: 0.5584\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4525 - accuracy: 0.5642\n",
      "Epoch 18/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4326 - accuracy: 0.5684\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.4157 - accuracy: 0.5749\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 1.3951 - accuracy: 0.5816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20321972560>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:03:37.075421Z",
     "start_time": "2025-10-13T08:03:36.969948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dino_name_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X -2),\n",
    "    model\n",
    "])"
   ],
   "id": "c4e238724c5a6c7",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:03:46.082733Z",
     "start_time": "2025-10-13T08:03:46.070616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_next_char(model, text, temperature=1):\n",
    "    y_proba = model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "521b1157784271da",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:05:21.427051Z",
     "start_time": "2025-10-13T08:05:21.415881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        char_gen_next = my_next_char(dino_name_model, text, temperature)\n",
    "        if char_gen_next == \"\\n\":\n",
    "            break\n",
    "        text += my_next_char(dino_name_model, text, temperature)\n",
    "    return text"
   ],
   "id": "2f3eb39386eea6e6",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T08:07:06.179675Z",
     "start_time": "2025-10-13T08:07:04.878344Z"
    }
   },
   "cell_type": "code",
   "source": "my_extend_text(\"xx\")",
   "id": "3c660e0f433513ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xxuwusaurus'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46d9c3eed421a220"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
