{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用字符RNN生成莎士比亚文本",
   "id": "b6905ec0806fdce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在2015年的一篇著名博客文章 \"https://karpathy.github.io/2015/05/21/rnn-effectiveness/\" 中，Andrej Karpathy展示了如何训练循环神经网络来预测句子中的下一个字符。这个char-RNN可以用来生成小说文本，每次一个字符。",
   "id": "bdf100365ed041e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-09T23:30:20.834259Z",
     "start_time": "2025-10-09T23:30:20.808593Z"
    }
   },
   "source": [
    "# tf.keras.utils.get_file()函数下载莎士比亚的所有作品\n",
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T23:30:21.101268Z",
     "start_time": "2025-10-09T23:30:21.085641Z"
    }
   },
   "cell_type": "code",
   "source": "print(shakespeare_text[:80])",
   "id": "5a0427af95a99073",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T23:30:22.037850Z",
     "start_time": "2025-10-09T23:30:21.584895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用tf.keras.layers.TextVectorization层对此文本进行编码。我们将设置split=\"character\"，基于字符进行编码，而不是默认的基于单词进行编码，并使用standardize=\"lower\"将文本转换为小写（这将简化任务）：\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "encoded"
   ],
   "id": "a051c0bf83bb06c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T23:30:22.928434Z",
     "start_time": "2025-10-09T23:30:22.912784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 每个字符现在都映射到一个整数，从2开始。TextVectorization层将值0保留为填充词元(token)，并将值1保留为未知字符。\n",
    "# 现在不需要这两个词元，因此从字符ID数中减去2，并计算不同字符的总数和总字符数：\n",
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)"
   ],
   "id": "8e7d446d727dc90e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将这个非常长的序列转换成一个窗口数据集，然后使用它来训练一个序列到序列的循环神经网络。目标序列将类似于输入序列，但是会向“未来”移动一个时间步。例如，数据集中的样本可能是一个由表示文本“to be or not to b”（不包括最后一个“e”）的字符ID组成的序列，相应的目标序列是一个由表示文本“o be or not to be”（包括最后一个“e”，但不包括开头的“t”）的字符ID组成的序列",
   "id": "6f9c13d7635eb999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T23:30:23.768841Z",
     "start_time": "2025-10-09T23:30:23.757698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将字符ID序列转换成输入/目标窗口对的数据集\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ],
   "id": "70ed9e8982001bf0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 它将序列作为输入（即编码文本），并创建一个包含所需长度的所有窗口的数据集。\n",
    "- 它将长度增加1，因为我们需要将下一个字符放在目标序列中。\n",
    "- 然后，它对窗口进行乱序处理（可选），将它们分批，将它们拆分为输入/输出对，并激活预取功能。\n",
    "\n",
    "长度为11的窗口和批量大小3。每个窗口的起始索引都显示在旁边：\n",
    "\n",
    "![准备乱序窗口的数据集](./images/RNN/p7.png)"
   ],
   "id": "205152dc0a2c913e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T13:26:15.698228Z",
     "start_time": "2025-10-12T13:26:15.682615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 大约90%的文本进行训练，5%用于验证，5%用于测试\n",
    "length = 100  # length决定循环神经网络能学习的最长模式\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000: 1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ],
   "id": "414ec5b323fe4c3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T23:15:17.344648Z",
     "start_time": "2025-10-09T23:15:17.270650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for data in test_set.take(1):\n",
    "    print(data)"
   ],
   "id": "2198babd695e6b80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 2, 21,  5, ...,  2,  4,  8],\n",
      "       [21,  5, 10, ...,  4,  8,  7],\n",
      "       [ 5, 10,  2, ...,  8,  7,  9],\n",
      "       ...,\n",
      "       [10, 27,  6, ..., 10, 11,  7],\n",
      "       [27,  6, 11, ..., 11,  7, 11],\n",
      "       [ 6, 11,  4, ...,  7, 11, 22]], dtype=int64)>, <tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[21,  5, 10, ...,  4,  8,  7],\n",
      "       [ 5, 10,  2, ...,  8,  7,  9],\n",
      "       [10,  2, 17, ...,  7,  9, 19],\n",
      "       ...,\n",
      "       [27,  6, 11, ..., 11,  7, 11],\n",
      "       [ 6, 11,  4, ...,  7, 11, 22],\n",
      "       [11,  4,  2, ..., 11, 22, 19]], dtype=int64)>)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 构建和训练char-RNN模型",
   "id": "ce01571b7082bc92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 使用Embedding层作为第一层，以编码字符ID。Embedding层的输入维数是不同字符ID的数量，输出维数是可以调整的超参数——现在将其设置为16。Embedding层的输入将是形状为［批量大小，窗口长度］的二维张量，Embedding层的输出将是形状为［批量大小，窗口长度，嵌入大小］的三维张量。\n",
    "- 为输出层使用Dense层：它必须具有39个单元(n_tokens)，因为文本中有39个不同的字符，希望在每个时间步输出每个可能字符的概率。每个时间步39个输出概率的总和应该为1，因此对Dense层的输出应用softmax激活函数。\n",
    "- 最后，使用\"sparse_categorical_crossentropy\"损失和Nadam优化器编译此模型，并使用ModelCheckpoint回调函数在训练过程中保存最佳模型（以验证精度为标准）进行多个轮次的训练。"
   ],
   "id": "bbeb76c3ffa2e1ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T02:58:40.366178Z",
     "start_time": "2025-10-09T23:30:27.792326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 不用简单RNN,用带有长短记忆的GRU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
   ],
   "id": "ddb8c8daf6234f32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  31247/Unknown - 1123s 36ms/step - loss: 1.4060 - accuracy: 0.5702INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1144s 36ms/step - loss: 1.4060 - accuracy: 0.5702 - val_loss: 1.6281 - val_accuracy: 0.5289\n",
      "Epoch 2/10\n",
      "31247/31247 [==============================] - ETA: 0s - loss: 1.3079 - accuracy: 0.5929INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1137s 36ms/step - loss: 1.3079 - accuracy: 0.5929 - val_loss: 1.5923 - val_accuracy: 0.5355\n",
      "Epoch 3/10\n",
      "31246/31247 [============================>.] - ETA: 0s - loss: 1.2904 - accuracy: 0.5967INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1226s 39ms/step - loss: 1.2904 - accuracy: 0.5967 - val_loss: 1.5878 - val_accuracy: 0.5368\n",
      "Epoch 4/10\n",
      "31246/31247 [============================>.] - ETA: 0s - loss: 1.2815 - accuracy: 0.5983INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1171s 37ms/step - loss: 1.2815 - accuracy: 0.5983 - val_loss: 1.5817 - val_accuracy: 0.5393\n",
      "Epoch 5/10\n",
      "31247/31247 [==============================] - 1177s 37ms/step - loss: 1.2750 - accuracy: 0.5997 - val_loss: 1.5781 - val_accuracy: 0.5387\n",
      "Epoch 6/10\n",
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2703 - accuracy: 0.6006INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1197s 38ms/step - loss: 1.2703 - accuracy: 0.6006 - val_loss: 1.5725 - val_accuracy: 0.5409\n",
      "Epoch 7/10\n",
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2663 - accuracy: 0.6016INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1454s 46ms/step - loss: 1.2663 - accuracy: 0.6016 - val_loss: 1.5715 - val_accuracy: 0.5421\n",
      "Epoch 8/10\n",
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2639 - accuracy: 0.6021INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1374s 44ms/step - loss: 1.2639 - accuracy: 0.6021 - val_loss: 1.5679 - val_accuracy: 0.5437\n",
      "Epoch 9/10\n",
      "31247/31247 [==============================] - 1306s 42ms/step - loss: 1.2621 - accuracy: 0.6025 - val_loss: 1.5736 - val_accuracy: 0.5433\n",
      "Epoch 10/10\n",
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2598 - accuracy: 0.6029INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1306s 42ms/step - loss: 1.2598 - accuracy: 0.6029 - val_loss: 1.5723 - val_accuracy: 0.5445\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:59:54.232307Z",
     "start_time": "2025-10-10T12:59:54.063149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练后搭建最终模型\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X-2), # 不使用填充词元(0)  和 未知词元 (1)\n",
    "    model\n",
    "])"
   ],
   "id": "4a9cd3d1b9707e25",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:11:49.121147Z",
     "start_time": "2025-10-10T13:11:48.401257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # 选择概率最高的字母ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ],
   "id": "660fc80a64c43c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 645ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 生成莎士比亚文本",
   "id": "97a339785e97f31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "要使用char-RNN模型生成新文本，可以将一些文本输入模型，让模型预测最有可能的下一个字母，将其添加到文本的末尾，然后将扩展后的文本输入模型来猜测下一个字母，以此类推。这叫作贪婪解码。但是在实践中，这往往导致相同的单词一遍又一遍地重复。\n",
    "\n",
    "相反，可以使用TensorFlow的tf.random.categorical()函数随机采样下一个字符，采样概率等于估计概率。这将生成更多样化和有趣的文本。categorical()函数在给定类别对数概率(logits)的情况下，对随机类别指数进行采样。"
   ],
   "id": "c5d1bcc028d8d3e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:12:55.691823Z",
     "start_time": "2025-10-10T13:12:55.644945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)\n"
   ],
   "id": "89b01494486ce3e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]], dtype=int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了更好地控制生成文本的多样性，可以将logits（对数概率）除以一个称为“温度”的数字，这个数字可以根据需求进行调整。“温度”接近零将更偏向于高概率字符，而较高“温度”则会使所有字符获得相同的概率。通常在生成相对严谨和精确的文本（例如数学公式）时，较低的“温度”更为适用，而在生成更多样化且有创意的文本时，则适合用较高的“温度”。",
   "id": "aefc5c6f8577915f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:15:15.334476Z",
     "start_time": "2025-10-10T13:15:15.320608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "b358a7b630203dce",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:16:24.189534Z",
     "start_time": "2025-10-10T13:16:24.173909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ],
   "id": "fbd43a4da8c0dd45",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:17:38.585016Z",
     "start_time": "2025-10-10T13:17:35.681434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"To be or not to be\", temperature=0.01))\n"
   ],
   "id": "786e75ebe35d87e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "To be or not to be the rest of the statute of the statute of his par\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:47:50.974144Z",
     "start_time": "2025-10-10T13:47:48.297565Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=1))",
   "id": "ccb5b2f788da0fc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "To be or not to be good and wrong; but come. though which the fine.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:48:15.816518Z",
     "start_time": "2025-10-10T13:48:13.225736Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=100))",
   "id": "d36fa2f1a2ba4d56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "To be or not to bef ,mt'&ozfpady-$\n",
      "wh!nse?pws3ert--vgerdjw?c-y-ewxnj\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模拟生成莎士比亚文本的流程 生成名字",
   "id": "c69b7c36365a237d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:23:24.525632Z",
     "start_time": "2025-10-10T14:23:24.513634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"datasets/dino.txt\", \"r\") as f:\n",
    "    dino_names = f.read()\n",
    "    dino_names = dino_names.lower()"
   ],
   "id": "8c9db58ee8770353",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:23:41.636630Z",
     "start_time": "2025-10-10T14:23:41.449976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([dino_names])\n",
    "encoded = text_vec_layer([dino_names])[0]\n",
    "encoded"
   ],
   "id": "82a72b78a106c110",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19909,), dtype=int64, numpy=array([ 2,  2, 15, ...,  4,  4, 12], dtype=int64)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:24:23.215893Z",
     "start_time": "2025-10-10T14:24:23.201894Z"
    }
   },
   "cell_type": "code",
   "source": "text_vec_layer.get_vocabulary()",
   "id": "1febe554f24a9100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'a',\n",
       " 's',\n",
       " 'u',\n",
       " 'o',\n",
       " 'r',\n",
       " '\\n',\n",
       " 'n',\n",
       " 'i',\n",
       " 'e',\n",
       " 't',\n",
       " 'l',\n",
       " 'p',\n",
       " 'h',\n",
       " 'c',\n",
       " 'g',\n",
       " 'd',\n",
       " 'm',\n",
       " 'y',\n",
       " 'b',\n",
       " 'k',\n",
       " 'v',\n",
       " 'x',\n",
       " 'z',\n",
       " 'j',\n",
       " 'w',\n",
       " 'f',\n",
       " 'q']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:27:28.018600Z",
     "start_time": "2025-10-10T14:27:28.006601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "n_tokens"
   ],
   "id": "340430d9880913e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:42:53.593421Z",
     "start_time": "2025-10-10T14:42:53.580422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "end_of_name_encode = text_vec_layer([\"\\n\"])[0,0].numpy() - 2\n",
    "end_of_name_encode"
   ],
   "id": "8019902ca41e0a37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:18:15.844135Z",
     "start_time": "2025-10-10T15:18:15.786136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把 Tensor 转成 numpy 数组\n",
    "encoded_np = encoded.numpy()\n",
    "\n",
    "# 存放 (X, Y) 对\n",
    "names_X = []\n",
    "names_Y = []\n",
    "\n",
    "# 临时缓冲区\n",
    "current_name = []\n",
    "\n",
    "for token in encoded_np:\n",
    "    current_name.append(token)\n",
    "    if token == end_of_name_encode:\n",
    "        if len(current_name) > 1:  # 至少两个字符才能形成 X/Y\n",
    "            X = current_name[:-1]\n",
    "            Y = current_name[1:]\n",
    "            names_X.append(X)\n",
    "            names_Y.append(Y)\n",
    "        current_name = []\n",
    "\n",
    "\n",
    "if len(current_name) > 1:\n",
    "    X = current_name[:-1]\n",
    "    Y = current_name[1:]\n",
    "    names_X.append(X)\n",
    "    names_Y.append(Y)\n",
    "    current_name = []\n",
    "\n",
    "# 转成 TensorFlow 张量\n",
    "X_dataset = tf.ragged.constant(names_X)\n",
    "Y_dataset = tf.ragged.constant(names_Y)\n",
    "\n",
    "print(\"共提取名字数:\", len(names_X))\n",
    "print(\"X_dataset 形状:\", X_dataset.shape)\n",
    "print(\"Y_dataset 形状:\", Y_dataset.shape)\n",
    "\n",
    "Y_dataset"
   ],
   "id": "f5b11ea33082d4a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共提取名字数: 1536\n",
      "X_dataset 形状: (1536, None)\n",
      "Y_dataset 形状: (1536, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[0, 13, 12, 8, 6, 3, 1, 0, 2, 4, 2, 1, 5], [0, 4, 15, 3, 6, 17, 21, 5],\n",
       " [18, 15, 0, 10, 10, 0, 12, 1, 0, 2, 4, 2, 1, 5], ...,\n",
       " [2, 3, 17, 2, 6, 10, 3, 6, 14, 5], [2, 11, 0, 17, 1, 0, 2, 4, 2, 1, 5],\n",
       " [2, 2, 10]]>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:19:17.107144Z",
     "start_time": "2025-10-10T15:18:21.842498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_dataset, Y_dataset, batch_size=1, epochs=10)"
   ],
   "id": "e96aa6e901b201d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1536/1536 [==============================] - 7s 3ms/step - loss: 2.0521 - accuracy: 0.3971\n",
      "Epoch 2/10\n",
      "1536/1536 [==============================] - 5s 3ms/step - loss: 1.7431 - accuracy: 0.4802\n",
      "Epoch 3/10\n",
      "1536/1536 [==============================] - 5s 3ms/step - loss: 1.6397 - accuracy: 0.5152\n",
      "Epoch 4/10\n",
      "1536/1536 [==============================] - 5s 3ms/step - loss: 1.5672 - accuracy: 0.5377\n",
      "Epoch 5/10\n",
      "1536/1536 [==============================] - 5s 3ms/step - loss: 1.5101 - accuracy: 0.5524\n",
      "Epoch 6/10\n",
      "1536/1536 [==============================] - 5s 4ms/step - loss: 1.4546 - accuracy: 0.5663\n",
      "Epoch 7/10\n",
      "1536/1536 [==============================] - 5s 4ms/step - loss: 1.3978 - accuracy: 0.5814\n",
      "Epoch 8/10\n",
      "1536/1536 [==============================] - 5s 4ms/step - loss: 1.3432 - accuracy: 0.5918\n",
      "Epoch 9/10\n",
      "1536/1536 [==============================] - 5s 3ms/step - loss: 1.2854 - accuracy: 0.6109\n",
      "Epoch 10/10\n",
      "1536/1536 [==============================] - 5s 3ms/step - loss: 1.2264 - accuracy: 0.6284\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:19:18.618291Z",
     "start_time": "2025-10-10T15:19:18.478552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dino_name_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X -2),\n",
    "    model\n",
    "])"
   ],
   "id": "c4e238724c5a6c7",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:19:19.661354Z",
     "start_time": "2025-10-10T15:19:19.648357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_next_char(model, text, temperature=1):\n",
    "    y_proba = model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "521b1157784271da",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:19:20.064029Z",
     "start_time": "2025-10-10T15:19:20.050667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        char_gen_next = my_next_char(dino_name_model, text, temperature)\n",
    "        if char_gen_next == \"\\n\":\n",
    "            break\n",
    "        text += my_next_char(dino_name_model, text, temperature)\n",
    "    return text"
   ],
   "id": "2f3eb39386eea6e6",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:20:01.208063Z",
     "start_time": "2025-10-10T15:20:00.601790Z"
    }
   },
   "cell_type": "code",
   "source": "my_extend_text(\"man\")",
   "id": "3c660e0f433513ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mansodon'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 119
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
